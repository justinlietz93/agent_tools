<?xml version="1.0" ?>
<agents>
  <metadata>
    <global_stats>
      <total_files>139</total_files>
      <total_size_bytes>460120</total_size_bytes>
      <total_loc>12976</total_loc>
    </global_stats>
    <chunk_stats>
      <files_in_chunk>139</files_in_chunk>
      <size_in_chunk_bytes>460120</size_in_chunk_bytes>
      <loc_in_chunk>12976</loc_in_chunk>
    </chunk_stats>
  </metadata>
  <ascii_map><![CDATA[
src/
├── __init__.py
│   (LOC: 1, Size: 91 B)
├── cli/
│   └── app.py
│       (LOC: 556, Size: 21.5 KB)
├── indexing/
│   ├── embedder_base.py
│   │   (LOC: 1, Size: 96 B)
│   └── qdrant/
├── integrations/
│   └── openai_agents/
│       └── bridge.py
│           (LOC: 189, Size: 5.9 KB)
├── memory/
│   ├── graph/
│   ├── memory_base.py
│   │   (LOC: 1, Size: 102 B)
│   └── vector/
├── networking/
├── prompts/
├── providers/
│   ├── README.md
│   │   (LOC: 211, Size: 10.6 KB)
│   ├── __init__.py
│   │   (LOC: 216, Size: 6.6 KB)
│   ├── anthropic/
│   │   ├── anthropic-models.json
│   │   │   (LOC: 0, Size: 0 B)
│   │   └── get_anthropic_models.py
│   │       (LOC: 117, Size: 3.4 KB)
│   ├── base/
│   │   ├── __init__.py
│   │   │   (LOC: 62, Size: 1.4 KB)
│   │   ├── factory.py
│   │   │   (LOC: 74, Size: 2.5 KB)
│   │   ├── get_models_base.py
│   │   │   (LOC: 236, Size: 8.2 KB)
│   │   ├── interfaces.py
│   │   │   (LOC: 92, Size: 3.1 KB)
│   │   ├── models.py
│   │   │   (LOC: 199, Size: 6.1 KB)
│   │   └── repositories/
│   │       ├── __init__.py
│   │       │   (LOC: 17, Size: 424 B)
│   │       ├── keys.py
│   │       │   (LOC: 129, Size: 4.1 KB)
│   │       └── model_registry.py
│   │           (LOC: 355, Size: 13.6 KB)
│   ├── decorators.py
│   │   (LOC: 173, Size: 5.9 KB)
│   ├── deepseek/
│   │   ├── deepseek-models.json
│   │   │   (LOC: 0, Size: 0 B)
│   │   └── get_deepseek_models.py
│   │       (LOC: 119, Size: 3.5 KB)
│   ├── exceptions.py
│   │   (LOC: 49, Size: 1.4 KB)
│   ├── gemini/
│   │   ├── gemini-models.json
│   │   │   (LOC: 0, Size: 0 B)
│   │   └── get_gemini_models.py
│   │       (LOC: 111, Size: 3.3 KB)
│   ├── ollama/
│   │   ├── get_ollama_models.py
│   │   │   (LOC: 107, Size: 3.2 KB)
│   │   └── ollama-models.json
│   │       (LOC: 0, Size: 0 B)
│   ├── openai/
│   │   ├── __init__.py
│   │   │   (LOC: 13, Size: 229 B)
│   │   ├── client.py
│   │   │   (LOC: 171, Size: 6.8 KB)
│   │   ├── get_openai_models.py
│   │   │   (LOC: 163, Size: 5.8 KB)
│   │   └── openai-models.json
│   │       (LOC: 1089, Size: 25.2 KB)
│   └── xai/
│       ├── get_xai_models.py
│       │   (LOC: 0, Size: 0 B)
│       └── xai-models.json
│           (LOC: 0, Size: 0 B)
├── settings/
│   ├── __init__.py
│   │   (LOC: 26, Size: 967 B)
│   ├── interfaces.py
│   │   (LOC: 43, Size: 1.1 KB)
│   └── sqlite_repository.py
│       (LOC: 110, Size: 3.6 KB)
├── tools/
│   ├── __init__.py
│   │   (LOC: 7, Size: 177 B)
│   ├── advanced_file_tool.py
│   │   (LOC: 262, Size: 9.8 KB)
│   ├── code_runner_tool.py
│   │   (LOC: 326, Size: 12.1 KB)
│   ├── computer_tool.py
│   │   (LOC: 552, Size: 21.6 KB)
│   ├── config.py
│   │   (LOC: 86, Size: 3.2 KB)
│   ├── deepseek_wrapper.py
│   │   (LOC: 11, Size: 351 B)
│   ├── file_tool.py
│   │   (LOC: 358, Size: 14.2 KB)
│   ├── mcp_tools/
│   │   └── mcp_base_tool.py
│   │       (LOC: 1, Size: 30 B)
│   ├── package_manager_tool.py
│   │   (LOC: 210, Size: 7.5 KB)
│   ├── requests_tool.py
│   │   (LOC: 120, Size: 3.8 KB)
│   ├── shell_tool.py
│   │   (LOC: 124, Size: 3.9 KB)
│   ├── tool_base.py
│   │   (LOC: 125, Size: 3.8 KB)
│   ├── tool_manager.py
│   │   (LOC: 120, Size: 3.5 KB)
│   ├── voidkit_tools/
│   │   ├── README.md
│   │   │   (LOC: 0, Size: 0 B)
│   │   ├── __init__.py
│   │   │   (LOC: 5, Size: 256 B)
│   │   ├── calculate_descriptive_stats.py
│   │   │   (LOC: 163, Size: 5.8 KB)
│   │   ├── causal_inference/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 76 B)
│   │   │   └── causal_inference.py
│   │   │       (LOC: 102, Size: 3.3 KB)
│   │   ├── clustering/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 2, Size: 146 B)
│   │   │   ├── adaptive_clustering.py
│   │   │   │   (LOC: 46, Size: 1.4 KB)
│   │   │   └── spectral_clustering.py
│   │   │       (LOC: 71, Size: 2.4 KB)
│   │   ├── dynamical_systems/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 3, Size: 149 B)
│   │   │   ├── analyze_stability.py
│   │   │   │   (LOC: 53, Size: 1.6 KB)
│   │   │   ├── calculate_jacobian.py
│   │   │   │   (LOC: 42, Size: 1.3 KB)
│   │   │   └── find_fixed_points.py
│   │   │       (LOC: 36, Size: 1.2 KB)
│   │   ├── evolutionary/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 2, Size: 96 B)
│   │   │   ├── apply_mutation.py
│   │   │   │   (LOC: 40, Size: 1.2 KB)
│   │   │   └── apply_recombination.py
│   │   │       (LOC: 41, Size: 1.2 KB)
│   │   ├── fractal_analysis/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 2, Size: 131 B)
│   │   │   ├── calculate_fractal_dimension.py
│   │   │   │   (LOC: 53, Size: 1.7 KB)
│   │   │   └── fractal_spike_train.py
│   │   │       (LOC: 49, Size: 1.4 KB)
│   │   ├── fractional_calculus/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 49 B)
│   │   │   └── caputo_derivative.py
│   │   │       (LOC: 47, Size: 1.3 KB)
│   │   ├── graph/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 4, Size: 294 B)
│   │   │   ├── advanced_graph_analysis.py
│   │   │   │   (LOC: 48, Size: 1.3 KB)
│   │   │   ├── coarse_grain_graph.py
│   │   │   │   (LOC: 48, Size: 1.5 KB)
│   │   │   ├── graph_dynamics.py
│   │   │   │   (LOC: 84, Size: 2.2 KB)
│   │   │   └── graph_theory_toolkit.py
│   │   │       (LOC: 87, Size: 3.2 KB)
│   │   ├── iit/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 63 B)
│   │   │   └── calculate_simplified_phi.py
│   │   │       (LOC: 58, Size: 1.7 KB)
│   │   ├── info_theory/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 2, Size: 164 B)
│   │   │   ├── information_bottleneck.py
│   │   │   │   (LOC: 44, Size: 1.3 KB)
│   │   │   └── information_theory.py
│   │   │       (LOC: 92, Size: 2.7 KB)
│   │   ├── linear_system_solver.py
│   │   │   (LOC: 99, Size: 3.6 KB)
│   │   ├── neuro/
│   │   │   ├── README.md
│   │   │   │   (LOC: 5, Size: 274 B)
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 3, Size: 155 B)
│   │   │   ├── advanced_sie.py
│   │   │   │   (LOC: 79, Size: 2.1 KB)
│   │   │   ├── apply_stc.py
│   │   │   │   (LOC: 64, Size: 2.1 KB)
│   │   │   └── apply_stdp.py
│   │   │       (LOC: 304, Size: 13.2 KB)
│   │   ├── numerical_integrate.py
│   │   │   (LOC: 110, Size: 3.9 KB)
│   │   ├── numerical_ode_solver.py
│   │   │   (LOC: 234, Size: 9.0 KB)
│   │   ├── optimization/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 57 B)
│   │   │   └── bayesian_optimization.py
│   │   │       (LOC: 83, Size: 3.1 KB)
│   │   ├── ot/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 75 B)
│   │   │   └── calculate_wasserstein_distance.py
│   │   │       (LOC: 38, Size: 1.2 KB)
│   │   ├── pathway_analysis/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 103 B)
│   │   │   └── dynamic_persistence.py
│   │   │       (LOC: 58, Size: 1.8 KB)
│   │   ├── rmt/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 63 B)
│   │   │   └── plot_eigenvalue_spectrum.py
│   │   │       (LOC: 47, Size: 1.5 KB)
│   │   ├── sde/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 35 B)
│   │   │   └── sde_solver.py
│   │   │       (LOC: 56, Size: 1.9 KB)
│   │   ├── semantic/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 69 B)
│   │   │   └── calculate_semantic_coverage.py
│   │   │       (LOC: 41, Size: 1.4 KB)
│   │   ├── soc_analysis/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 2, Size: 108 B)
│   │   │   ├── detect_neuronal_avalanches.py
│   │   │   │   (LOC: 65, Size: 2.1 KB)
│   │   │   └── fit_power_law.py
│   │   │       (LOC: 46, Size: 1.5 KB)
│   │   ├── spatial/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 47 B)
│   │   │   └── spatial_hash_grid.py
│   │   │       (LOC: 36, Size: 1.3 KB)
│   │   ├── stochastic/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 55 B)
│   │   │   └── gillespie_simulation.py
│   │   │       (LOC: 68, Size: 2.1 KB)
│   │   ├── structural_plasticity/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 6, Size: 269 B)
│   │   │   ├── advanced_triggers.py
│   │   │   │   (LOC: 45, Size: 1.3 KB)
│   │   │   ├── apply_structural_plasticity.py
│   │   │   │   (LOC: 52, Size: 1.9 KB)
│   │   │   ├── calculate_bdnf_proxy.py
│   │   │   │   (LOC: 57, Size: 2.0 KB)
│   │   │   └── detect_bursts.py
│   │   │       (LOC: 60, Size: 2.1 KB)
│   │   ├── symbolic/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 5, Size: 307 B)
│   │   │   ├── calculus.py
│   │   │   │   (LOC: 47, Size: 1001 B)
│   │   │   ├── define_symbols.py
│   │   │   │   (LOC: 54, Size: 1.2 KB)
│   │   │   ├── logic.py
│   │   │   │   (LOC: 40, Size: 919 B)
│   │   │   ├── manipulate_expression.py
│   │   │   │   (LOC: 46, Size: 1.4 KB)
│   │   │   └── solve_equation.py
│   │   │       (LOC: 31, Size: 866 B)
│   │   ├── symbolic_differentiation.py
│   │   │   (LOC: 132, Size: 5.1 KB)
│   │   ├── tda/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 3, Size: 187 B)
│   │   │   ├── calculate_tda_metrics.py
│   │   │   │   (LOC: 49, Size: 1.7 KB)
│   │   │   ├── compute_persistent_homology.py
│   │   │   │   (LOC: 47, Size: 1.8 KB)
│   │   │   └── construct_vietoris_rips.py
│   │   │       (LOC: 78, Size: 2.9 KB)
│   │   ├── thermodynamics/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 74 B)
│   │   │   └── free_energy.py
│   │   │       (LOC: 87, Size: 2.3 KB)
│   │   ├── time_series/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 1, Size: 104 B)
│   │   │   └── time_series_analysis.py
│   │   │       (LOC: 88, Size: 2.4 KB)
│   │   └── void_dynamics/
│   │       ├── FUM_Void_Debt_Modulation.py
│   │       │   (LOC: 136, Size: 5.8 KB)
│   │       ├── FUM_Void_Equations.py
│   │       │   (LOC: 120, Size: 4.7 KB)
│   │       ├── __init__.py
│   │       │   (LOC: 5, Size: 219 B)
│   │       ├── apply_revgsp.py
│   │       │   (LOC: 626, Size: 26.0 KB)
│   │       ├── diagnostics_formulas.py
│   │       │   (LOC: 49, Size: 1.9 KB)
│   │       ├── revgsp_formulas.py
│   │       │   (LOC: 72, Size: 2.8 KB)
│   │       ├── sie_formulas.py
│   │       │   (LOC: 83, Size: 3.2 KB)
│   │       └── tda_formulas.py
│   │           (LOC: 328, Size: 12.4 KB)
│   ├── web_browser_tool.py
│   │   (LOC: 130, Size: 4.3 KB)
│   └── web_search_tool.py
│       (LOC: 151, Size: 5.2 KB)
└── wrappers/
    ├── __init__.py
    │   (LOC: 5, Size: 273 B)
    ├── base.py
    │   (LOC: 112, Size: 4.0 KB)
    ├── deepseek_wrapper.py
    │   (LOC: 37, Size: 1.1 KB)
    ├── ollama_wrapper.py
    │   (LOC: 329, Size: 12.7 KB)
    └── openai_compatible.py
        (LOC: 259, Size: 10.4 KB)]]></ascii_map>
  <files>
    <file>
      <path>__init__.py</path>
      <content><![CDATA[# Make 'agent_tools.src' a package for module execution (python -m agent_tools.src.cli.app)]]></content>
    </file>
    <file>
      <path>cli/app.py</path>
      <content><![CDATA["""
Interactive CLI for Agent Tools with OpenAI-compatible providers (DeepSeek, Ollama, Custom).

Features:
- Provider selection (DeepSeek, Ollama, Custom OpenAI-compatible)
- Local Ollama support out of the box (http://localhost:11434/v1)
- Color themes (dark/light) and rich panels for output
- Tool listing and live execution via wrappers
- Smooth prompt experience using prompt_toolkit
- Model selection and listing via providers registry for supported providers

Commands:
  /help      Show help
  /tools     List registered tools
  /config    Show current provider configuration
  /models    List available models for current provider (use "/models refresh" to refresh)
  /model     Select or set the current model ("/model <id>" or interactive)
  /provider  Switch provider (restarts wrapper)
  /theme     Toggle theme (dark/light)
  /clear     Clear the screen
  /exit      Exit

Run:
  python -m src.cli.app
  or
  python src/cli/app.py
"""

from __future__ import annotations

import os
import sys
import json
from getpass import getpass
from typing import Optional, Tuple, List

# Ensure project root is importable when run directly
CURRENT_DIR = os.path.dirname(__file__)
ROOT_DIR = os.path.abspath(os.path.join(CURRENT_DIR, "..", ".."))
if ROOT_DIR not in sys.path:
    sys.path.insert(0, ROOT_DIR)

from prompt_toolkit import PromptSession
from prompt_toolkit.history import InMemoryHistory
from prompt_toolkit.patch_stdout import patch_stdout
from prompt_toolkit.completion import WordCompleter

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.theme import Theme
from rich.box import ROUNDED

from src.wrappers.deepseek_wrapper import DeepseekToolWrapper
from src.wrappers.ollama_wrapper import OllamaWrapper
from src.wrappers.openai_compatible import OpenAICompatibleWrapper
from src.tools.tool_manager import ToolManager
from src.providers.base.repositories.model_registry import ModelRegistryRepository
from src.settings import get_settings_repo


def make_console(theme_name: str, use_color: bool = True) -> Console:
    """Create a Rich console with the selected theme and color policy."""
    if theme_name == "light":
        theme = Theme(
            {
                "primary": "black",
                "accent": "dark_green",
                "warning": "dark_orange",
                "error": "red",
                "success": "green",
                "muted": "grey42",
                "box_title": "bold black",
            }
        )
    else:
        # dark
        theme = Theme(
            {
                "primary": "white",
                "accent": "cyan",
                "warning": "yellow",
                "error": "bold red",
                "success": "green",
                "muted": "grey70",
                "box_title": "bold cyan",
            }
        )
    # no_color=True disables ANSI codes for environments that don't support them
    return Console(
        theme=theme,
        no_color=not use_color,
        color_system=("standard" if use_color else None),
        force_terminal=use_color,
        markup=False,
    )


def select_provider(console: Console, session: PromptSession) -> str:
    """Prompt to select a provider."""
    console.print(
        Panel(
            "Select Provider\n\n"
            "1) DeepSeek (hosted)\n"
            "2) Ollama (local)\n"
            "3) OpenAI (hosted)\n"
            "4) Custom (OpenAI-compatible)\n",
            title="Provider",
            box=ROUNDED
        )
    )
    completer = WordCompleter(["1", "2", "3", "4"], ignore_case=True)
    while True:
        choice = session.prompt("Choose [1-4]: ", completer=completer).strip()
        if choice in {"1", "2", "3", "4"}:
            return {"1": "deepseek", "2": "ollama", "3": "openai", "4": "custom"}[choice]
        console.print("[warning]Invalid option. Enter 1, 2, 3, or 4.[/warning]")


def input_custom_config(console: Console, session: PromptSession) -> Tuple[str, str, str]:
    """Gather custom OpenAI-compatible config: base_url, model, api_key."""
    console.print(
        Panel(
            "[box_title]Custom Provider[/box_title]\nProvide OpenAI-compatible connection details.",
            title="Custom Config",
            box=ROUNDED,
            border_style="accent",
        )
    )
    base_url = session.prompt("Base URL (e.g., http://host:port/v1): ").strip() or "http://localhost:11434/v1"
    model = session.prompt("Model (e.g., llama3.1, deepseek-reasoner): ").strip() or "llama3.1"
    api_key = session.prompt("API Key (leave blank if not required): ").strip()
    return base_url, model, api_key


def instantiate_wrapper(provider: str, console: Console, session: PromptSession, repo=None) -> OpenAICompatibleWrapper:
    """Create the appropriate wrapper instance based on selection."""
    repo = repo or get_settings_repo()

    if provider == "deepseek":
        wrapper = DeepseekToolWrapper()
        print("Using DeepSeek provider")
        # Restore saved model for this provider if present
        saved = repo.get_pref(f"model.{provider}")
        if saved:
            wrapper.model = saved
            print(f"Restored model to '{saved}'")
        return wrapper

    elif provider == "ollama":
        wrapper = OllamaWrapper()
        print("Using Ollama local provider (http://localhost:11434/v1)")
        saved = repo.get_pref(f"model.{provider}")
        if saved:
            wrapper.model = saved
            print(f"Restored model to '{saved}'")
        return wrapper

    elif provider == "openai":
        # Prefer env; then DB; else prompt (no placeholder fallback)
        key = os.getenv("OPENAI_API_KEY") or (repo.get_api_key("openai") or "")
        if not key:
            try:
                key = getpass("Enter OPENAI_API_KEY (input hidden, press Enter to cancel): ").strip()
            except Exception:
                key = ""
        if not key:
            print("OpenAI API key not provided. Returning to provider selection.")
            # Re-enter provider selection flow
            new_provider = select_provider(console, session)
            return instantiate_wrapper(new_provider, console, session, repo=repo)

        # Persist and set env for process-wide usage
        try:
            repo.set_api_key("openai", key)
        except Exception:
            pass
        os.environ["OPENAI_API_KEY"] = key

        wrapper = OpenAICompatibleWrapper(
            api_key=key,
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
        )
        print("Using OpenAI provider https://api.openai.com/v1")
        saved = repo.get_pref(f"model.{provider}")
        if saved:
            wrapper.model = saved
            print(f"Restored model to '{saved}'")
        return wrapper

    # custom (OpenAI-compatible)
    base_url, model, api_key = input_custom_config(console, session)
    wrapper = OpenAICompatibleWrapper(api_key=api_key or None, base_url=base_url, model=model)
    print(f"Using Custom provider {base_url} model={model}")
    saved = repo.get_pref(f"model.{provider}")
    if saved:
        wrapper.model = saved
        print(f"Restored model to '{saved}'")
    return wrapper


def list_tools(console: Console, manager: ToolManager) -> None:
    """Render a table of registered tools."""
    table = Table(title="Registered Tools", box=ROUNDED)
    table.add_column("Name", no_wrap=True)
    table.add_column("Description")
    table.add_column("Required Params")

    for info in manager.list_tools():
        req = ", ".join(info["input_schema"].get("required", []))
        table.add_row(info["name"], info["description"], req or "-")

    console.print(table)


def show_config(console: Console, wrapper: OpenAICompatibleWrapper, provider_name: str) -> None:
    """Display current provider configuration."""
    content = (
        f"Provider: {provider_name}\n"
        f"Base URL: {getattr(wrapper, 'base_url', '')}\n"
        f"Model: {getattr(wrapper, 'model', '')}\n"
    )
    console.print(Panel(content, title="Configuration", box=ROUNDED))


def show_help(console: Console) -> None:
    """Print help panel."""
    console.print(
        Panel(
            "Commands\n"
            "/help      Show help\n"
            "/tools     List registered tools\n"
            "/config    Show provider configuration\n"
            "/models    List available models (use '/models refresh' to refresh)\n"
            "/model     Select or set the current model (e.g., '/model llama3.1')\n"
            "/call      Execute a tool directly (e.g., /call file {\"operation\":\"read\",\"path\":\"/tmp/a.txt\"})\n"
            "/auth      Set API key for a provider (e.g., /auth openai sk-...) or prompt if omitted\n"
            "/prefs     Show persisted preferences and API key providers\n"
            "/provider  Switch provider\n"
            "/theme     Toggle theme (dark/light)\n"
            "/colors    Toggle color output (on/off)\n"
            "/clear     Clear the screen\n"
            "/exit      Exit\n\n"
            "Type natural language instructions to let the LLM choose and run tools.",
            title="Help",
            box=ROUNDED
        )
    )


def _registry_provider_key(provider: str) -> Optional[str]:
    """
    Map CLI provider identifier to registry provider key.
    Returns None when no registry is supported (e.g., 'custom').
    """
    p = (provider or "").strip().lower()
    if p in {"deepseek", "ollama", "openai", "xai", "gemini", "openrouter", "anthropic"}:
        return p
    return None


def _render_models_table(console: Console, models: List[object]) -> None:
    table = Table(title="Available Models", box=ROUNDED)
    table.add_column("ID")
    table.add_column("Name")
    table.add_column("Family")
    table.add_column("Updated")

    for m in models or []:
        mid = getattr(m, "id", None) or "-"
        name = getattr(m, "name", None) or "-"
        fam = getattr(m, "family", None) or "-"
        upd = getattr(m, "updated_at", None) or "-"
        table.add_row(str(mid), str(name), str(fam), str(upd))

    console.print(table)


def show_models(console: Console, provider: str, refresh: bool = False) -> None:
    """List models via ModelRegistryRepository for the current provider."""
    key = _registry_provider_key(provider)
    if not key:
        console.print("[warning]Model registry not supported for this provider (custom connection).[/warning]")
        return
    try:
        repo = ModelRegistryRepository()
        snap = repo.list_models(key, refresh=refresh)
        if not snap.models:
            console.print("[warning]No models found in registry. Try '/models refresh' if applicable.[/warning]")
            return
        _render_models_table(console, snap.models)
    except Exception as e:
        console.print(Panel(str(e), title="Error", box=ROUNDED))


def select_model_interactive(console: Console, session: PromptSession, wrapper: OpenAICompatibleWrapper, provider: str) -> None:
    """Prompt user to choose a model id and set it on the wrapper."""
    key = _registry_provider_key(provider)
    model_choices: List[str] = []

    if key:
        try:
            repo = ModelRegistryRepository()
            snap = repo.list_models(key, refresh=False)
            model_choices = [str(getattr(m, "id", "") or getattr(m, "name", "")) for m in snap.models if getattr(m, "id", None) or getattr(m, "name", None)]
        except Exception:
            model_choices = []

    if model_choices:
        completer = WordCompleter(model_choices, ignore_case=True, match_middle=True)
        sel = session.prompt("Model ID (tab to complete): ", completer=completer).strip()
    else:
        # Fallback: free text entry
        sel = session.prompt("Model ID: ").strip()

    if not sel:
        console.print("[warning]Model unchanged.[/warning]")
        return

    # Assign directly (OpenAICompatibleWrapper reads self.model)
    wrapper.model = sel
    print(f"Model set to '{sel}'")


def run() -> None:
    """Main interactive loop."""
    repo = get_settings_repo()

    # Theme and color policy (ENV overrides DB, then defaults)
    theme = (os.getenv("CLI_THEME") or (repo.get_pref("cli_theme") or "dark")).lower()
    # Conservative default: disable colors unless explicitly enabled
    use_color_env = os.getenv("CLI_COLOR")
    if use_color_env is not None:
        use_color = use_color_env.lower() in ("1", "true", "yes", "on")
    else:
        use_color = (repo.get_pref("cli_color") or "").lower() in ("1", "true", "yes", "on")
    console = make_console("light" if theme in ("light", "white") else "dark", use_color=use_color)
    session = PromptSession(history=InMemoryHistory())

    console.print(
        Panel(
            "Agent Tools CLI\n"
            "OpenAI-compatible with tool-use and local Ollama support.",
            title="Welcome",
            box=ROUNDED
        )
    )

    provider = select_provider(console, session)
    wrapper = instantiate_wrapper(provider, console, session, repo=repo)
    try:
        repo.set_pref("last_provider", provider)
    except Exception:
        pass
    # Restore model if set (also done in instantiate_wrapper but safe)
    saved_model = repo.get_pref(f"model.{provider}")
    if saved_model:
        wrapper.model = saved_model

    # Register default tools
    manager = ToolManager(register_defaults=True)
    for tool in manager.tools.values():
        wrapper.register_tool(tool)

    show_help(console)

    completer = WordCompleter(
        ["/help", "/tools", "/config", "/models", "/model", "/call", "/auth", "/prefs", "/provider", "/theme", "/colors", "/clear", "/exit"],
        ignore_case=True,
        match_middle=True,
    )

    with patch_stdout():
        while True:
            try:
                user_input = session.prompt("> ", completer=completer)
            except (KeyboardInterrupt, EOFError):
                console.print("\n[warning]Exiting...[/warning]")
                break

            cmd = user_input.strip()
            if not cmd:
                continue

            # Built-in commands
            if cmd == "/help":
                show_help(console)
                continue
            if cmd == "/tools":
                list_tools(console, manager)
                continue
            if cmd == "/config":
                show_config(console, wrapper, provider_name=provider)
                continue
            if cmd.startswith("/models"):
                refresh = any(tok.lower().startswith("ref") for tok in cmd.split()[1:])
                show_models(console, provider, refresh=refresh)
                continue
            if cmd.startswith("/model"):
                parts = cmd.split(maxsplit=1)
                if len(parts) == 2 and parts[1].strip():
                    wrapper.model = parts[1].strip()
                    print(f"Model set to '{wrapper.model}'")
                    try:
                        repo.set_pref(f"model.{provider}", wrapper.model)
                    except Exception:
                        pass
                else:
                    select_model_interactive(console, session, wrapper, provider)
                    try:
                        if getattr(wrapper, "model", None):
                            repo.set_pref(f"model.{provider}", wrapper.model)
                    except Exception:
                        pass
                continue
            if cmd.startswith("/call"):
                # Direct tool execution: /call <tool_name> <json_params>
                parts = cmd.split(maxsplit=2)
                if len(parts) < 3:
                    print("Usage: /call <tool_name> {\"param\":\"value\", ...}")
                    continue
                tool_name = parts[1].strip()
                params_raw = parts[2].strip()
                try:
                    params = json.loads(params_raw)
                except Exception as e:
                    print(f"Invalid JSON: {e}")
                    continue
                try:
                    tool = manager.get_tool(tool_name)
                except KeyError as e:
                    print(str(e))
                    continue
                try:
                    # Prefer run(input_dict) signature
                    result = tool.run(params)  # type: ignore[arg-type]
                except TypeError:
                    # Fallback for tools using run(tool_call_id, **kwargs)
                    try:
                        result = tool.run("cli", **(params if isinstance(params, dict) else {}))  # type: ignore[call-arg]
                    except Exception as e:
                        print(f"Error executing tool: {e}")
                        continue
                try:
                    rendered = result if isinstance(result, str) else json.dumps(result, ensure_ascii=False, indent=2)
                except Exception:
                    rendered = str(result)
                console.print(Panel(rendered, title=f"Tool Result: {tool_name}", box=ROUNDED))
                continue
            if cmd.startswith("/auth"):
                # /auth <provider> [key]
                parts = cmd.split(maxsplit=2)
                if len(parts) < 2:
                    print("Usage: /auth <provider> [key]")
                    continue
                auth_provider = parts[1].strip().lower()
                input_key = parts[2].strip() if len(parts) == 3 else None
                if not input_key:
                    try:
                        input_key = getpass(f"Enter API key for {auth_provider} (hidden): ").strip()
                    except Exception:
                        input_key = ""
                try:
                    repo.set_api_key(auth_provider, input_key or None)
                    # update process env for common providers
                    env_map = {
                        "openai": "OPENAI_API_KEY",
                        "anthropic": "ANTHROPIC_API_KEY",
                        "deepseek": "DEEPSEEK_API_KEY",
                        "gemini": "GEMINI_API_KEY",
                        "xai": "XAI_API_KEY",
                        "openrouter": "OPENROUTER_API_KEY",
                    }
                    env_name = env_map.get(auth_provider)
                    if env_name:
                        if input_key:
                            os.environ[env_name] = input_key
                        elif env_name in os.environ:
                            del os.environ[env_name]
                    print(f"API key {'updated' if input_key else 'cleared'} for {auth_provider}.")
                    # If updating the current provider, re-instantiate wrapper to pick changes
                    if auth_provider == provider:
                        wrapper = instantiate_wrapper(provider, console, session, repo=repo)
                        for tool in manager.tools.values():
                            wrapper.register_tool(tool)
                except Exception as e:
                    print(f"Auth update failed: {e}")
                continue
            if cmd == "/prefs":
                try:
                    data = {"prefs": repo.all_prefs(), "api_keys": list(repo.all_api_keys().keys())}
                    console.print(Panel(json.dumps(data, ensure_ascii=False, indent=2), title="Preferences", box=ROUNDED))
                except Exception as e:
                    console.print(Panel(f"Failed to load prefs: {e}", title="Preferences", box=ROUNDED))
                continue
            if cmd == "/provider":
                provider = select_provider(console, session)
                wrapper = instantiate_wrapper(provider, console, session, repo=repo)
                # Re-register tools on the new wrapper
                for tool in manager.tools.values():
                    wrapper.register_tool(tool)
                try:
                    repo.set_pref("last_provider", provider)
                except Exception:
                    pass
                print("Provider switched.")
                continue
            if cmd == "/theme":
                theme = "light" if theme == "dark" else "dark"
                console = make_console(theme, use_color=use_color)
                try:
                    repo.set_pref("cli_theme", theme)
                except Exception:
                    pass
                print(f"Theme switched to {theme}")
                continue
            if cmd.startswith("/colors"):
                parts = cmd.split(maxsplit=1)
                if len(parts) == 2 and parts[1].lower() in ("on", "off"):
                    use_color = parts[1].lower() == "on"
                else:
                    use_color = not use_color
                console = make_console(theme, use_color=use_color)
                status = "enabled" if use_color else "disabled"
                try:
                    repo.set_pref("cli_color", "on" if use_color else "off")
                except Exception:
                    pass
                print(f"Colors {status}")
                continue
            if cmd == "/clear":
                console.clear()
                continue
            if cmd == "/exit":
                console.print("[warning]Goodbye.[/warning]")
                break

            # Execute via wrapper
            console.print(Panel("Thinking...", box=ROUNDED))
            try:
                result = wrapper.execute(cmd)
                console.print(
                    Panel(
                        result,
                        title="LLM Response",
                        box=ROUNDED,
                        border_style="accent",
                    )
                )
            except Exception as e:
                console.print(Panel(f"[error]{str(e)}[/error]", title="Error", box=ROUNDED, border_style="error"))


if __name__ == "__main__":
    run()]]></content>
    </file>
    <file>
      <path>indexing/embedder_base.py</path>
      <content><![CDATA[# Calls a Qdrant vector database to embed and store documents for semantic search and retrieval.]]></content>
    </file>
    <file>
      <path>integrations/openai_agents/bridge.py</path>
      <content><![CDATA["""
OpenAI Agents SDK Bridge Module.

Provides integration with OpenAI Agents Python SDK for tool orchestration.
Handles safe imports, router tool creation, and agent execution.

File length constraint: <500 LOC
"""

from __future__ import annotations

import json
from typing import Any, Dict, Optional

# Safe imports with fallbacks
try:
    from agents import Agent, Runner, function_tool
    SDK_AVAILABLE = True
except ImportError:
    SDK_AVAILABLE = False
    Agent = None  # type: ignore
    Runner = None  # type: ignore
    function_tool = None  # type: ignore


def create_router_tool(tools_registry: Dict[str, Any]) -> Any:
    """
    Create a single router function tool that delegates to ToolManager.

    Args:
        tools_registry: Dict mapping tool names to tool info (from wrapper.tools)

    Returns:
        Function tool if SDK available, None otherwise
    """
    if not SDK_AVAILABLE or not function_tool:
        return None

    @function_tool(name="tool_router")
    def router_tool(tool_name: str, input: Dict[str, Any]) -> str:
        """
        Route tool execution to registered tools.

        Args:
            tool_name: Name of the tool to execute
            input: Input parameters as dict

        Returns:
            Tool execution result as string
        """
        if tool_name not in tools_registry:
            return f"Error: Tool '{tool_name}' not found."

        tool_info = tools_registry[tool_name]
        tool = tool_info["tool"]

        try:
            # Try calling with input_dict first (preferred)
            if hasattr(tool, 'run') and callable(tool.run):
                # Check signature - if it takes exactly 2 args (self, input_dict), use that
                import inspect
                sig = inspect.signature(tool.run)
                params = list(sig.parameters.values())
                if len(params) == 2 and params[1].name != 'tool_call_id':
                    # Assume run(input_dict)
                    result = tool.run(input)
                else:
                    # Use fallback: run("sdk", **input)
                    result = tool.run("sdk", **input)
            else:
                return f"Error: Tool '{tool_name}' has no callable run method."

            # Serialize result if dict
            if isinstance(result, dict):
                return json.dumps(result)
            return str(result)

        except Exception as e:
            return f"Error executing tool '{tool_name}': {str(e)}"

    return router_tool


def run_openai_agents(
    model: str,
    system_prompt: str,
    user_input: str,
    tools_registry: Dict[str, Any],
    api_key: Optional[str],
    base_url: Optional[str]
) -> Dict[str, Any]:
    """
    Execute user input using OpenAI Agents SDK.

    Args:
        model: Model name
        system_prompt: System instructions
        user_input: User query
        tools_registry: Tool registry dict
        api_key: API key
        base_url: Base URL

    Returns:
        Dict with reasoning, tool_call, tool_result, response, raw_items
    """
    if not SDK_AVAILABLE or not Agent or not Runner:
        raise ImportError("OpenAI Agents SDK not available")

    # Create router tool
    router_tool = create_router_tool(tools_registry)
    if not router_tool:
        raise RuntimeError("Failed to create router tool")

    # Create agent with router tool
    agent = Agent(
        name="ToolOrchestrator",
        instructions=system_prompt,
        model=model,
        tools=[router_tool],
        # Use OpenAI client config if provided
        model_settings={
            "api_key": api_key,
            "base_url": base_url,
        } if api_key or base_url else None
    )

    # Run synchronously (since wrapper is sync)
    import asyncio
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If event loop is already running, use run_until_complete
            result = loop.run_until_complete(Runner.run(agent, user_input))
        else:
            # Create new event loop
            result = asyncio.run(Runner.run(agent, user_input))
    except RuntimeError:
        # Fallback for environments without event loop
        import nest_asyncio
        nest_asyncio.apply()
        result = asyncio.run(Runner.run(agent, user_input))

    # Extract components from RunResult
    reasoning = ""
    tool_call = None
    tool_result = None
    response = None

    # Parse final_output and items
    final_output = result.final_output or ""

    # Look for tool calls in new_items
    for item in result.new_items:
        if hasattr(item, 'type'):
            if item.type == 'tool_call':
                # Extract tool call info
                tool_call = {
                    "tool": getattr(item, 'tool_name', ''),
                    "input_schema": getattr(item, 'arguments', {})
                }
                # Tool result might be in subsequent items
                if hasattr(item, 'output'):
                    tool_result = item.output
            elif item.type == 'text' and not response:
                response = getattr(item, 'content', '')

    # If no explicit tool call found, check if final_output contains tool info
    if not tool_call and "tool_router" in final_output:
        # Parse from text (fallback)
        if "tool_name" in final_output and "input" in final_output:
            try:
                # Simple parsing - in practice, SDK should provide structured data
                tool_call = {"tool": "unknown", "input_schema": {}}
                tool_result = final_output
            except:
                pass

    # If no tool call, use final_output as response
    if not tool_call:
        response = final_output
        reasoning = ""  # SDK handles reasoning internally

    return {
        "reasoning": reasoning,
        "tool_call": tool_call,
        "tool_result": tool_result,
        "response": response,
        "raw_items": [str(item) for item in result.new_items]
    }]]></content>
    </file>
    <file>
      <path>memory/memory_base.py</path>
      <content><![CDATA[# Memory base class for storing and retrieving conversation history, intermediate states, and context.]]></content>
    </file>
    <file>
      <path>providers/README.md</path>
      <content><![CDATA[Providers layer (Hybrid Clean Architecture)

Overview
- Goal: Normalize provider interactions behind small, typed contracts while keeping all provider-specific logic contained under src/providers/.
- Key benefits:
  - Provider-agnostic DTOs and interfaces
  - Central factory for adapter creation
  - Model registry repository to read/write per-provider model listings (JSON)
  - Pluggable “get models” fetchers per provider
  - Non-invasive: adapters wrap existing clients where possible

Core contracts and types
- Interfaces:
  - LLMProvider (core provider contract)
  - SupportsJSONOutput, SupportsResponsesAPI (capability flags)
  - ModelListingProvider (expose list_models)
  - HasDefaultModel (optional default model)
  See [interfaces.py](Cogito/src/providers/base/interfaces.py).

- DTOs (Provider-agnostic models):
  - Message, ChatRequest, ChatResponse, ProviderMetadata
  - ModelInfo, ModelRegistrySnapshot
  - References:
    - [Message](Cogito/src/providers/base/models.py:52)
    - [ChatRequest](Cogito/src/providers/base/models.py:104)
    - [ChatResponse](Cogito/src/providers/base/models.py:139)
    - [ProviderMetadata](Cogito/src/providers/base/models.py:86)
    - [ModelInfo](Cogito/src/providers/base/models.py:160)
    - [ModelRegistrySnapshot](Cogito/src/providers/base/models.py:181)
  See [models.py](Cogito/src/providers/base/models.py).

- Repositories:
  - Model registry I/O and refresh orchestration:
    - [ModelRegistryRepository](Cogito/src/providers/base/repositories/model_registry.py:64)
      - Key method: list_models(provider, refresh=False)
      - Reads canonical JSON files under src/providers/{provider}/{provider}-models.json
      - Optional refresh invokes provider “get models” script
      - Resilient JSON read for empty/whitespace files:
        [ModelRegistryRepository._read_provider_json()](Cogito/src/providers/base/repositories/model_registry.py:253)
  - Keys (API key resolution):
    - [KeysRepository.get_api_key()](Cogito/src/providers/base/repositories/keys.py:62)
    - Resolution order: env (OPENAI_API_KEY, etc.) → config.yaml → None

- Factory:
  - [ProviderFactory.create()](Cogito/src/providers/base/factory.py:47)
    - Maps canonical provider name (e.g., "openai") to adapter class by lazy import
    - Raises UnknownProviderError for unregistered/failed adapters

Provider adapters
- OpenAI
  - Adapter: [OpenAIProvider](Cogito/src/providers/openai/client.py:44)
    - Exposes:
      - provider_name: "openai"
      - default_model(): str
      - supports_json_output(): bool
      - uses_responses_api(model: str): bool (o3/o1 families)
      - list_models(refresh: bool): ModelRegistrySnapshot
      - chat(request: ChatRequest): ChatResponse
    - Internals:
      - Wraps existing call_openai_with_retry() with normalized ChatRequest/ChatResponse
      - model default derived from get_openai_config() with fallback (o3-mini)
      - Structured outputs: request.response_format == "json_object"

Model registry
- JSON location (per provider):
  - src/providers/{provider}/{provider}-models.json
  - Examples:
    - OpenAI: src/providers/openai/openai-models.json
    - Anthropic: src/providers/anthropic/anthropic-models.json
- Repository: [ModelRegistryRepository](Cogito/src/providers/base/repositories/model_registry.py:64)
  - list_models(provider, refresh=False)
    - refresh=True will attempt a provider get-models runner:
      src.providers.{provider}.get_{provider}_models
    - Accepts multiple entrypoint names: run(), get_models(), fetch_models(), update_models(), refresh_models(), main()
  - Resilient read:
    - [ModelRegistryRepository._read_provider_json()](Cogito/src/providers/base/repositories/model_registry.py:253)
    - Empty/whitespace-only files are treated as empty registry with a warning (stderr)

Provider “get models” fetchers (API-backed)
- OpenAI:
  - Module: [get_openai_models.py](Cogito/src/providers/openai/get_openai_models.py)
    - Entry point: [run()](Cogito/src/providers/openai/get_openai_models.py:49)
    - Behavior:
      - If OPENAI_API_KEY is present, fetch via OpenAI SDK:
        client = OpenAI(api_key=key)
        items = client.models.list()
      - Normalize and persist via save_provider_models()
      - Return list[dict] for ModelRegistryRepository convenience
    - Offline mode:
      - Falls back to load_cached_models(provider) and returns cached entries
      - Does not write empty registries (no false positives)
  - Key resolution:
    - [KeysRepository.get_api_key("openai")](Cogito/src/providers/base/repositories/keys.py:62)
    - Uses env (OPENAI_API_KEY) or config.yaml api.openai.api_key

Usage

1) Installing dependencies
- Base:
  - pip install -r requirements.txt
- Dev (pytest, etc.):
  - pip install -r requirements-dev.txt

2) API keys
- Preferred: .env at project root (Cogito/.env)
  - Example:
    OPENAI_API_KEY=sk-...
- Alternative: config.yaml
  - api:
      openai:
        api_key: "sk-..."
- Key resolution order:
  - Env → config.yaml → None. See [KeysRepository](Cogito/src/providers/base/repositories/keys.py:41).

3) Populate model registries (OpenAI example)
- Ensure key is loaded into the process environment (source .env).
- Run fetcher (no code changes required):
  - python -m src.providers.openai.get_openai_models
  - Expected: "[openai] loaded <N> models" and JSON persisted at:
    src/providers/openai/openai-models.json

4) Instantiate providers via factory
- Example:
  - from src.providers.base import ProviderFactory, ChatRequest, Message
  - provider = [ProviderFactory.create()](Cogito/src/providers/base/factory.py:47)("openai")
  - req = ChatRequest(
      model=provider.default_model() or "o3-mini",
      messages=[
        Message(role="system", content="You are a concise assistant."),
        Message(role="user", content="Say 'ok' once.")
      ],
      max_tokens=32,
      response_format="text",
    )
  - resp = provider.chat(req)
  - print(resp.text)

5) Listing models
- Example (OpenAI):
  - provider = ProviderFactory.create("openai")
  - snap = [OpenAIProvider.list_models()](Cogito/src/providers/openai/client.py:72)(refresh=False)
  - print(len(snap.models))
- To refresh from API:
  - snap = provider.list_models(refresh=True)  # requires OPENAI_API_KEY

Testing

Location and policy
- All tests live under Cogito/tests/... (no test scripts in src/ per architecture rules).

Unit smoke test (factory + OpenAI adapter)
- File: [tests/providers/test_provider_factory_smoke.py](Cogito/tests/providers/test_provider_factory_smoke.py)
- What it covers:
  - ProviderFactory creates OpenAI adapter
  - Adapter exposes provider_name and default_model
  - list_models(refresh=True) populates JSON when OPENAI_API_KEY is set; otherwise the test is skipped (no-network)
- Run:
  - python -m pytest -q tests/providers/test_provider_factory_smoke.py

Design notes and guardrails
- Non-invasive adapter design:
  - OpenAIProvider chat() wraps existing call_openai_with_retry()
  - No changes to legacy OpenAI client semantics
- JSON I/O resilience (important for offline/dev):
  - [ModelRegistryRepository._read_provider_json()](Cogito/src/providers/base/repositories/model_registry.py:253) gracefully handles empty/whitespace files by returning {} with a warning
  - Fetchers do not write empty registries in offline/cache mode
- Extensibility:
  - Register new providers by adding entries to ProviderFactory._PROVIDERS and implementing a client.py adapter plus get_{provider}_models.py
  - Keep provider-specific SDK imports in provider modules only
- Observability:
  - ProviderMetadata attached to ChatResponse supports audits and test logging

Metadata completeness and limitations
- context_length
  - Policy: Keep null when not provided by the API. Do not fabricate limits.
  - Enrichment flow: [get_openai_models.run()](Cogito/src/providers/openai/get_openai_models.py:49) fetches list and best-effort details (client.models.retrieve) and passes through numeric fields (e.g., input_token_limit/context_window) when present. Normalization in [normalize_items()](Cogito/src/providers/base/get_models_base.py:110) maps these into context_length if and only if explicitly available.
- capabilities
  - Derived from SDK “modalities” when present and from conservative id heuristics in [normalize_items()](Cogito/src/providers/base/get_models_base.py:110) (e.g., mark reasoning/responses_api for o1/o3 families; vision for gpt-4o/omni; embeddings for text-embedding ids; JSON structured output flagged by default).
- updated_at
  - Prefer explicit updated_at from the SDK. If absent, infer ISO date from the model id via [_infer_updated_at_from_id()](Cogito/src/providers/base/get_models_base.py:71). As a last resort, convert a numeric “created” epoch (UTC) to YYYY-MM-DD.
- provenance
  - The snapshot writer [save_provider_models()](Cogito/src/providers/base/get_models_base.py:145) persists fetched_via and metadata.source. The OpenAI fetcher records “api” and “openai_sdk_enriched” to indicate SDK-originated, enriched listings.

Note: If future endpoints expose explicit token limits per model id, the normalization path will automatically populate context_length without policy changes.

Extending to new providers (checklist)
- Create src/providers/{provider}/client.py implementing LLMProvider (+ capabilities)
- Add get_{provider}_models.py fetcher module with run() entrypoint
- Add factory mapping in [ProviderFactory](Cogito/src/providers/base/factory.py:32)
- Provide {provider}-models.json seed or rely on fetcher to create it
- Add unit tests under Cogito/tests/providers/

FAQ

Q: How do I run a minimal end-to-end provider call without wiring the orchestrator?
- Use ProviderFactory + ChatRequest in a small script or a unit test as shown above. No orchestrator changes are required.

Q: What if my JSON registry file is empty and I get errors?
- The repository now tolerates empty/whitespace files and returns an empty snapshot with a warning, but live API refresh is recommended to populate real models:
  python -m src.providers.openai.get_openai_models

Q: Where should tests go?
- Only under Cogito/tests/. Test-like scripts in src/ are not allowed by architecture rules.

References
- Factory: [ProviderFactory.create()](Cogito/src/providers/base/factory.py:47)
- OpenAI Adapter: [OpenAIProvider](Cogito/src/providers/openai/client.py:44)
- DTOs: [ChatRequest](Cogito/src/providers/base/models.py:104), [ChatResponse](Cogito/src/providers/base/models.py:139), [Message](Cogito/src/providers/base/models.py:52)
- Registry: [ModelRegistryRepository](Cogito/src/providers/base/repositories/model_registry.py:64)
- Key resolution: [KeysRepository.get_api_key()](Cogito/src/providers/base/repositories/keys.py:62)
- OpenAI fetcher: [get_openai_models.run()](Cogito/src/providers/openai/get_openai_models.py:49)]]></content>
    </file>
    <file>
      <path>providers/__init__.py</path>
      <content><![CDATA["""
Aggregator and exports for provider package.
"""

import logging
import json
import re
from typing import Dict, Any, Tuple, Union

# Re-export exception types
from .exceptions import (
    ProviderError,
    ApiCallError,
    ApiResponseError,
    JsonParsingError,
    JsonProcessingError,
)

__all__ = [
    "ProviderError",
    "ApiCallError",
    "ApiResponseError",
    "JsonParsingError",
    "JsonProcessingError",
    "call_with_retry",
    "anthropic_client",
    "deepseek_client",
    "gemini_client",
    "openai_client",
    "openrouter_client",
    "ollama_client",
    "xai_client",
    "model_config",
    "decorators",
]

logger = logging.getLogger(__name__)


def _safe_format(template: str, context: Dict[str, Any]) -> str:
    """Best-effort placeholder replacement without raising on missing keys."""
    try:
        formatted = template
        for k, v in context.items():
            formatted = formatted.replace(f"{{{k}}}", str(v))
        return formatted
    except Exception as e:
        logger.debug(f"Safe format failed: {e}")
        return template


def _clean_json_markers(s: str) -> str:
    """Strip common code fences from LLM JSON replies."""
    s = s.strip()
    if s.startswith("```json"):
        s = s[7:]
    elif s.startswith("```"):
        s = s[3:]
    if s.endswith("```"):
        s = s[:-3]
    return s.strip()


def _attempt_json_repair(s: str) -> str:
    """
    Attempt to repair common JSON formatting issues:
    - Trim to the first JSON object/array envelope
    - Remove code fences and trailing commas
    - Balance braces/brackets and quotes
    """
    # Trim to JSON envelope
    first_obj = s.find("{")
    first_arr = s.find("[")
    idx_candidates = [i for i in [first_obj, first_arr] if i != -1]
    if idx_candidates:
        start = min(idx_candidates)
        s = s[start:]

    # Clean code fences if any slipped through
    s = _clean_json_markers(s)

    # Remove trailing commas before a closing brace/bracket
    s = re.sub(r",\s*(\}|\])", r"\1", s)

    # Balance braces/brackets
    open_braces = s.count("{")
    close_braces = s.count("}")
    if open_braces > close_braces:
        s += "}" * (open_braces - close_braces)

    open_brackets = s.count("[")
    close_brackets = s.count("]")
    if open_brackets > close_brackets:
        s += "]" * (open_brackets - close_brackets)

    # Balance quotes in a naive but effective way
    s_wo_escaped = re.sub(r'\\"', "", s)
    if s_wo_escaped.count('"') % 2 == 1:
        s += '"'

    return s


def call_with_retry(
    prompt_template: str,
    context: Dict[str, Any],
    config: Dict[str, Any],
    is_structured: bool = True,
) -> Tuple[Union[str, Dict[str, Any]], str]:
    """
    Provider-agnostic call with retry that delegates to the configured provider.
    Returns a tuple of (response, model_used_label).
    """
    api_cfg = (config or {}).get("api", {}) or {}
    primary = api_cfg.get("primary_provider") or api_cfg.get("provider") or "gemini"
    provider = str(primary).lower()

    if provider == "gemini":
        from .gemini_client import call_gemini_with_retry
        resp, model = call_gemini_with_retry(
            prompt_template=prompt_template,
            context=context,
            config=config,
            is_structured=is_structured,
        )
        return resp, model

    if provider == "openai":
        from .openai_client import call_openai_with_retry
        resp, model = call_openai_with_retry(
            prompt_template=prompt_template,
            context=context,
            config=config,
            is_structured=is_structured,
        )
        return resp, model

    if provider == "openrouter":
        from .openrouter_client import run_openrouter_client
        from .model_config import get_openrouter_config

        formatted = _safe_format(prompt_template, context)
        if is_structured:
            formatted += "\n\nRespond strictly in valid JSON format. Do not include code fences."

        sys_msg = api_cfg.get("openrouter", {}).get(
            "system_message",
            "You are a helpful research assistant.",
        )
        messages = [
            {"role": "system", "content": sys_msg},
            {"role": "user", "content": formatted},
        ]

        # Execute via OpenRouter
        try:
            text = run_openrouter_client(messages=messages)
        except Exception as e:
            raise ApiCallError(f"OpenRouter call failed: {e}") from e

        model_name = get_openrouter_config().get("model", "unknown")
        label = f"openrouter:{model_name}"

        if is_structured:
            cleaned = _clean_json_markers(text)
            try:
                data = json.loads(cleaned)
                return data, label
            except json.JSONDecodeError as e1:
                # Attempt a best-effort repair for truncated or slightly malformed JSON
                repaired = _attempt_json_repair(cleaned)
                try:
                    data = json.loads(repaired)
                    logger.info("Repaired malformed JSON from OpenRouter response")
                    return data, label
                except Exception as e2:
                    raise JsonParsingError(
                        f"OpenRouter returned non-JSON when JSON expected: {e1}. Raw: {text[:800]}"
                    ) from e2
        else:
            return text, label

    if provider == "ollama":
        from .ollama_client import run_ollama_client

        formatted = _safe_format(prompt_template, context)
        # Do not inject any system prompt here; upstream components supply system prompts.
        messages = [
            {"role": "user", "content": formatted},
        ]

        try:
            text = run_ollama_client(messages=messages)
        except Exception as e:
            raise ApiCallError(f"Ollama call failed: {e}") from e

        label = "ollama"
        if is_structured:
            cleaned = _clean_json_markers(text)
            try:
                data = json.loads(cleaned)
                return data, label
            except json.JSONDecodeError as e1:
                repaired = _attempt_json_repair(cleaned)
                try:
                    data = json.loads(repaired)
                    logger.info("Repaired malformed JSON from Ollama response")
                    return data, label
                except Exception as e2:
                    raise JsonParsingError(
                        f"Ollama returned non-JSON when JSON expected: {e1}. Raw: {text[:800]}"
                    ) from e2
        else:
            return text, label

    raise ApiCallError(f"Unknown or unsupported provider: {provider}")]]></content>
    </file>
    <file>
      <path>providers/anthropic/anthropic-models.json</path>
      <content/>
    </file>
    <file>
      <path>providers/anthropic/get_anthropic_models.py</path>
      <content><![CDATA["""
Anthropic: get models

Behavior
- Attempts to fetch model listings via Anthropic SDK.
- Persists to JSON at: src/providers/anthropic/anthropic-models.json
- If API key or SDK is unavailable, falls back to cached JSON (no network).

Entry points recognized by the ModelRegistryRepository:
- run()  (preferred)
- get_models()/fetch_models()/update_models()/refresh_models() also provided for convenience
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional

try:
    import anthropic  # anthropic>=0.49.0 recommended
except Exception:
    anthropic = None  # type: ignore

from src.providers.base.get_models_base import save_provider_models, load_cached_models
from src.providers.base.repositories.keys import KeysRepository


PROVIDER = "anthropic"


def _fetch_via_sdk(api_key: str) -> List[Any]:
    """
    Fetch model listings using Anthropic SDK. Returns raw items (SDK objects or dicts).
    """
    if anthropic is None:
        raise RuntimeError("anthropic SDK not available")

    # Prefer modern initialization
    client = getattr(anthropic, "Anthropic", None)
    if client is None:
        raise RuntimeError("Anthropic.Anthropic class not found in SDK")

    client = client(api_key=api_key)  # type: ignore[call-arg]

    # Modern clients expose models.list()
    models_obj = getattr(getattr(client, "models", None), "list", None)
    if callable(models_obj):
        resp = models_obj()
        data = getattr(resp, "data", None)
        if data is None and isinstance(resp, dict):
            data = resp.get("data", [])
        return list(data or [])

    # Fallback: try attribute access commonly used in older SDKs
    list_fn = getattr(client, "models", None)
    if callable(list_fn):
        try:
            resp = list_fn()  # type: ignore[call-arg]
            data = getattr(resp, "data", None)
            if data is None and isinstance(resp, dict):
                data = resp.get("data", [])
            return list(data or [])
        except Exception:
            pass

    raise RuntimeError("Anthropic SDK does not expose a models listing in this version")


def _resolve_key() -> Optional[str]:
    return KeysRepository().get_api_key(PROVIDER)


def run() -> List[Dict[str, Any]]:
    """
    Preferred entrypoint. Attempts online refresh; falls back to cached snapshot.

    Returns a list of dicts (models) for convenience; ModelRegistryRepository can
    also parse and persist this return value.
    """
    key = _resolve_key()
    if key:
        try:
            items = _fetch_via_sdk(key)
            save_provider_models(PROVIDER, items, fetched_via="api", metadata={"source": "anthropic_sdk"})
            out: List[Dict[str, Any]] = []
            for it in items:
                mid = getattr(it, "id", None) or getattr(it, "name", None) or str(it)
                name = getattr(it, "name", None) or mid
                out.append({"id": mid, "name": name})
            return out
        except Exception:
            # Fall through to cached
            pass

    snap = load_cached_models(PROVIDER)
    return [m.to_dict() for m in snap.models]


# Aliases for repository compatibility
def get_models() -> List[Dict[str, Any]]:
    return run()


def fetch_models() -> List[Dict[str, Any]]:
    return run()


def update_models() -> List[Dict[str, Any]]:
    return run()


def refresh_models() -> List[Dict[str, Any]]:
    return run()


if __name__ == "__main__":
    models = run()
    print(f"[anthropic] loaded {len(models)} models")]]></content>
    </file>
    <file>
      <path>providers/base/__init__.py</path>
      <content><![CDATA["""
Providers Base Package

Exports provider-agnostic contracts, DTOs, repositories, and the provider factory
for use within the providers layer (and, later, by orchestrators).

Conforms to the hybrid clean architecture scaffolding:
- Interfaces: normalized provider boundaries
- Models (DTOs): serialization-friendly request/response objects
- Repositories: model registry and key resolution
- Factory: lazy creation of provider adapters by canonical name
"""

from .models import (
    Role,
    ContentPartType,
    ContentPart,
    Message,
    ProviderMetadata,
    ChatRequest,
    ChatResponse,
    ModelInfo,
    ModelRegistrySnapshot,
)

from .interfaces import (
    LLMProvider,
    SupportsJSONOutput,
    SupportsResponsesAPI,
    ModelListingProvider,
    HasDefaultModel,
)

from .repositories.model_registry import ModelRegistryRepository
from .repositories.keys import KeysRepository, KeyResolution
from .factory import ProviderFactory, UnknownProviderError

__all__ = [
    # Models
    "Role",
    "ContentPartType",
    "ContentPart",
    "Message",
    "ProviderMetadata",
    "ChatRequest",
    "ChatResponse",
    "ModelInfo",
    "ModelRegistrySnapshot",
    # Interfaces
    "LLMProvider",
    "SupportsJSONOutput",
    "SupportsResponsesAPI",
    "ModelListingProvider",
    "HasDefaultModel",
    # Repositories
    "ModelRegistryRepository",
    "KeysRepository",
    "KeyResolution",
    # Factory
    "ProviderFactory",
    "UnknownProviderError",
]]]></content>
    </file>
    <file>
      <path>providers/base/factory.py</path>
      <content><![CDATA["""
Provider Factory

Purpose
- Centralized, provider-agnostic creation of LLMProvider adapters.
- Lazy-imports provider adapters to avoid heavy dependencies at import time.
- No side effects: strictly returns instances or raises a clear error.

Contracts
- Returns instances implementing [LLMProvider](Cogito/src/providers/base/interfaces.py:1).

Scope
- Providers supported in this scaffolding: openai (others to be added later).
"""

from __future__ import annotations

from typing import Optional, Dict, Type, Any

from .interfaces import LLMProvider


class UnknownProviderError(Exception):
    pass


class ProviderFactory:
    """
    Create provider adapters based on a canonical name (e.g., 'openai').
    """

    # Map canonical provider names to import paths and class names
    _PROVIDERS: Dict[str, Dict[str, str]] = {
        "openai": {
            "module": "src.providers.openai.client",
            "class": "OpenAIProvider",
        },
        # Future entries:
        # "anthropic": {"module": "src.providers.anthropic.client", "class": "AnthropicProvider"},
        # "deepseek": {"module": "src.providers.deepseek.client", "class": "DeepseekProvider"},
        # "gemini": {"module": "src.providers.gemini.client", "class": "GeminiProvider"},
        # "xai": {"module": "src.providers.xai.client", "class": "XAIProvider"},
        # "openrouter": {"module": "src.providers.openrouter.client", "class": "OpenRouterProvider"},
        # "ollama": {"module": "src.providers.ollama.client", "class": "OllamaProvider"},
    }

    @classmethod
    def create(cls, provider: str, **kwargs: Any) -> LLMProvider:
        """
        Create a provider adapter instance.

        Args:
            provider: Canonical provider name (e.g., 'openai')
            **kwargs: Adapter-specific constructor kwargs (optional)

        Returns:
            Instance implementing LLMProvider

        Raises:
            UnknownProviderError: if provider is not registered or cannot be imported.
        """
        name = (provider or "").lower().strip()
        spec = cls._PROVIDERS.get(name)
        if not spec:
            raise UnknownProviderError(f"Unknown provider '{provider}'")

        module_path, class_name = spec["module"], spec["class"]

        try:
            mod = __import__(module_path, fromlist=[class_name])
            klass: Type[LLMProvider] = getattr(mod, class_name)
            return klass(**kwargs)  # type: ignore[call-arg]
        except Exception as e:
            raise UnknownProviderError(f"Failed to initialize provider '{provider}': {e}") from e]]></content>
    </file>
    <file>
      <path>providers/base/get_models_base.py</path>
      <content><![CDATA["""
Utilities for provider model registry fetchers.

This module provides helpers shared by provider-specific "get models" scripts:
- Normalization of raw API results into ModelInfo DTOs
- Saving snapshots to canonical JSON files under src/providers/{provider}/{provider}-models.json
- Loading cached snapshots when online refresh fails

Intended usage (inside provider script):
    from src.providers.base.get_models_base import save_provider_models, load_cached_models
    items = fetch_from_api()  # list of dicts/SDK objects
    save_provider_models("openai", items, fetched_via="api", metadata={"source": "openai_api"})
"""

from __future__ import annotations

from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional
import re

from .models import ModelInfo, ModelRegistrySnapshot
from .repositories.model_registry import ModelRegistryRepository


def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def _as_dict(obj: Any) -> Dict[str, Any]:
    """
    Best-effort conversion of SDK objects to dicts by probing common attributes.
    """
    if isinstance(obj, dict):
        return obj
    d: Dict[str, Any] = {}
    # Probe common attributes found on SDK objects; tolerate absence.
    for attr in (
        "id",
        "name",
        "slug",
        "model",
        "display_name",
        "family",
        "series",
        "context_length",
        "max_context",
        # Additional candidates that may exist on some SDK objects
        "created",              # epoch seconds
        "modalities",           # e.g., ["text","vision","audio"]
        "input_token_limit",    # sometimes exposed as per-model token limit
        "max_output_tokens",    # optional output cap (not mapped directly)
        "capabilities",         # provider-defined capabilities if present
    ):
        v = getattr(obj, attr, None)
        if v is not None:
            d[attr] = v
    # OpenAI often nests under .data; ignore here
    return d


def _norm_name_id(d: Dict[str, Any]) -> (str, str):
    """
    Determine (id, name) from a flexible dict.
    """
    sid = str(d.get("id") or d.get("model") or d.get("name") or d.get("slug") or "unknown")
    name = str(d.get("name") or d.get("display_name") or d.get("id") or d.get("model") or sid)
    return sid, name


def _infer_updated_at_from_id(model_id: str) -> Optional[str]:
    """
    Infer an ISO-like YYYY-MM-DD date from the model id when present, e.g.:
    'gpt-4o-mini-search-preview-2025-03-11' -> '2025-03-11'
    """
    m = re.search(r"\b(20\d{2}-\d{2}-\d{2})\b", model_id)
    return m.group(1) if m else None


def _infer_family_from_id(model_id: str, provider: str) -> Optional[str]:
    """
    Infer a reasonable 'family' string from the model id for known providers.

    For OpenAI, prefer stable prefixes:
    - 'gpt-4o-mini-...' -> 'gpt-4o-mini'
    - 'gpt-4o-...'      -> 'gpt-4o'
    - 'o3-...'          -> 'o3'
    - 'o1-...'          -> 'o1'
    Fallback: first token before '-' or ':'.
    """
    lower = (model_id or "").lower()

    if provider == "openai":
        if "gpt-4o-mini" in lower:
            return "gpt-4o-mini"
        if "gpt-4o" in lower:
            return "gpt-4o"
        if lower.startswith("o3"):
            return "o3"
        if lower.startswith("o1"):
            return "o1"
        if lower.startswith("gpt-4"):
            return "gpt-4"

    # generic fallback: first token by dash/colon
    first = re.split(r"[-:]", lower)[0]
    return first or None


def normalize_items(provider: str, items: Iterable[Any]) -> List[ModelInfo]:
    """
    Convert arbitrary list of dictionaries/SDK objects into a list of ModelInfo.
    """
    out: List[ModelInfo] = []
    for it in items or []:
        if isinstance(it, ModelInfo):
            out.append(it)
            continue

        d = _as_dict(it)
        sid, name = _norm_name_id(d)

        # Family: prefer explicit field; otherwise infer from id
        fam = d.get("family") or d.get("series")
        if not isinstance(fam, str) or not fam:
            fam = _infer_family_from_id(sid, provider)

        # Context length: prefer explicit numeric fields from SDK; avoid fabricating unknowns
        ctx = (
            d.get("context_length")
            or d.get("max_context")
            or d.get("ctx")
            or d.get("input_token_limit")
            or d.get("context_window")
        )
        try:
            ctx_int = int(ctx) if ctx is not None else None
        except Exception:
            ctx_int = None

        # Capabilities: normalize to dict and enrich from modalities/id patterns when missing
        caps = d.get("capabilities")
        if caps is None:
            caps = {}
        elif not isinstance(caps, dict):
            caps = {"raw_capabilities": caps}

        # Map 'modalities' (if provided) to boolean flags inside capabilities
        mods = d.get("modalities")
        if isinstance(mods, (list, tuple)):
            for m in mods:
                mstr = str(m).lower()
                caps[mstr] = True
                # Normalize common synonyms
                if mstr in ("image", "vision"):
                    caps["vision"] = True

        # Baseline capability inference from model id patterns (provider-specific)
        def _infer_caps(provider: str, model_id: str) -> Dict[str, Any]:
            lower = (model_id or "").lower()
            inferred: Dict[str, Any] = {}
            if provider == "openai":
                # Reasoning + Responses API families
                if lower.startswith("o1") or lower.startswith("o3"):
                    inferred["reasoning"] = True
                    inferred["responses_api"] = True
                # Vision-capable families
                if "gpt-4o" in lower or "omni" in lower or "vision" in lower:
                    inferred["vision"] = True
                # Embeddings
                if "embedding" in lower or lower.startswith("text-embedding"):
                    inferred["embedding"] = True
                # Search-related previews
                if "search" in lower:
                    inferred["search"] = True
                # JSON/structured outputs are broadly supported; mark as likely
                inferred.setdefault("json_output", True)
            return inferred

        # Merge inferred capabilities if none present so far
        if not caps:
            caps = _infer_caps(provider, sid)
        else:
            # Non-destructive merge to add obvious booleans derived from id
            caps = {**_infer_caps(provider, sid), **caps}

        # Updated at: prefer explicit; else infer date fragment from id; else convert 'created' epoch if available
        updated = d.get("updated_at") if isinstance(d.get("updated_at"), str) else None
        if not updated:
            inferred_date = _infer_updated_at_from_id(sid)
            if inferred_date:
                updated = inferred_date
        if not updated and isinstance(d.get("created"), (int, float)):
            try:
                updated = datetime.fromtimestamp(d["created"], tz=timezone.utc).date().isoformat()
            except Exception:
                pass

        out.append(
            ModelInfo(
                id=sid,
                name=name,
                provider=provider,
                family=fam if isinstance(fam, str) and fam else None,
                context_length=ctx_int,
                capabilities=caps,
                updated_at=updated,
            )
        )
    return out


def save_provider_models(provider: str, items: Iterable[Any], fetched_via: str = "api", metadata: Optional[Dict[str, Any]] = None) -> Path:
    """
    Normalize and persist a provider model registry snapshot to its JSON file.

    Returns path to the written JSON.
    """
    models = normalize_items(provider, items)
    snapshot = ModelRegistrySnapshot(
        provider=provider,
        models=models,
        fetched_via=fetched_via,
        fetched_at=_now_iso(),
        metadata=metadata or {},
    )
    repo = ModelRegistryRepository()
    return repo.save_snapshot(snapshot)


def load_cached_models(provider: str) -> ModelRegistrySnapshot:
    """
    Load the last-saved snapshot from disk without refreshing.
    """
    repo = ModelRegistryRepository()
    return repo.list_models(provider, refresh=False)]]></content>
    </file>
    <file>
      <path>providers/base/interfaces.py</path>
      <content><![CDATA["""
Provider-agnostic interfaces (ABCs/Protocols) for the providers layer.

This module defines the boundary contracts that upstream code should depend on:
- LLMProvider: minimal chat interface taking a normalized ChatRequest and returning ChatResponse
- Capability mixins (Protocols) to advertise optional features without tight coupling
- ModelListingProvider: unified way to (re)load provider model registries

Adapters for each concrete provider (OpenAI, Anthropic, etc.) should implement LLMProvider
and optionally capability mixins. Upstream does not import SDKs directly.

Related DTOs are defined in: src/providers/base/models.py
"""

from __future__ import annotations

from typing import Protocol, runtime_checkable, Optional
from .models import ChatRequest, ChatResponse, ModelRegistrySnapshot


@runtime_checkable
class LLMProvider(Protocol):
    """
    Minimal interface for Large Language Model providers.

    Implementations should map ChatRequest fields to their SDK-specific parameters,
    normalize responses to ChatResponse, and never leak SDK objects upstream.
    """

    @property
    def provider_name(self) -> str:
        """Canonical provider identifier, e.g., 'openai', 'anthropic'."""
        ...

    def chat(self, request: ChatRequest) -> ChatResponse:
        """
        Execute a single chat completion request.

        - Must not raise for common provider errors; instead encode details in ChatResponse.meta.extra
          and return a best-effort text/parts. Reserve exceptions for programmer errors.
        """
        ...


@runtime_checkable
class SupportsJSONOutput(Protocol):
    """
    Capability marker for providers that can request JSON-native responses.

    Providers implementing this may honor ChatRequest.response_format == 'json_object'
    or an equivalent representation, and normalize JSON content into a text string
    (serialized) or parts as appropriate.
    """
    def supports_json_output(self) -> bool:
        return True


@runtime_checkable
class SupportsResponsesAPI(Protocol):
    """
    Capability marker for providers that expose a 'responses' style API
    (e.g., OpenAI o1/o3-mini) separate from classic chat completions.
    """
    def uses_responses_api(self, model: str) -> bool:
        """
        Return True if the given model should be executed via the Responses API path.
        """
        ...


@runtime_checkable
class ModelListingProvider(Protocol):
    """
    Interface for providers that can materialize a model registry snapshot
    (e.g., via a 'get models' endpoint or local tooling like 'ollama list').
    """
    def list_models(self, refresh: bool = False) -> ModelRegistrySnapshot:
        """
        Return a snapshot of known models for this provider.

        - When refresh=True, implementations should attempt to re-fetch from source
          (network/API or local tool) and persist the provider's JSON registry.
        - When refresh=False, implementations may return a cached snapshot from JSON.
        """
        ...


# Optional convenience mixin for providers with a default model
@runtime_checkable
class HasDefaultModel(Protocol):
    def default_model(self) -> Optional[str]:
        return None]]></content>
    </file>
    <file>
      <path>providers/base/models.py</path>
      <content><![CDATA["""
Provider-agnostic domain models (DTOs) for the providers layer.

These dataclasses define a normalized contract for LLM requests/responses,
model registry items, and provider metadata. They are intentionally minimal
and JSON-serializable to allow easy logging, caching, and testing.

Design goals
- Pure data: no provider-specific behavior here.
- Provider adapters convert between SDK objects and these DTOs.
- Upstream layers depend only on these models and provider interfaces.

See interfaces in: src/providers/base/interfaces.py (to be added).
"""

from __future__ import annotations

from dataclasses import dataclass, field, asdict
from typing import Any, Dict, List, Literal, Optional, Union


# Message roles used across providers.
Role = Literal["system", "user", "assistant", "tool"]

# Known content part types seen across providers' structured messages.
ContentPartType = Literal[
    "text",          # Plain text content
    "json",          # JSON content as string
    "tool_call",     # Tool call metadata
    "image",         # Image content (path/URL/base64) - adapter-defined semantics
    "refusal",       # Refusal reason text
    "other"          # Catch-all (adapter may attach provider-specific type info)
]


@dataclass
class ContentPart:
    """
    A single piece of structured content from an assistant message.

    Providers may return structured "parts" instead of a flat string.
    This DTO captures the minimum cross-provider subset.
    """
    type: ContentPartType
    text: Optional[str] = None
    data: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class Message:
    """
    A chat message. Content may be a flat string or a list of ContentPart.

    Adapters should normalize provider payloads into one of:
      - content: str
      - content: List[ContentPart]
    """
    role: Role
    content: Union[str, List[ContentPart]]

    def is_structured(self) -> bool:
        return isinstance(self.content, list)

    def text_or_joined(self) -> str:
        """
        Returns a reasonable text representation of the content for logging or
        providers that require flattened text.
        """
        if isinstance(self.content, str):
            return self.content
        parts: List[str] = []
        for p in self.content:
            if p.text:
                parts.append(p.text)
            elif p.data is not None:
                # Compact JSON-ish preview
                parts.append(f"[{p.type}]")
            else:
                parts.append(f"[{p.type}]")
        return "\n".join(parts)


@dataclass
class ProviderMetadata:
    """
    Execution metadata for a provider call (for diagnostics and audits).
    """
    provider_name: str
    model_name: str
    token_param_used: Optional[str] = None  # e.g., "max_tokens", "max_completion_tokens", "max_output_tokens"
    temperature_included: Optional[bool] = None
    http_status: Optional[int] = None
    request_id: Optional[str] = None
    latency_ms: Optional[float] = None
    extra: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class ChatRequest:
    """
    Normalized request to an LLM provider.

    Upstream specifies only these fields; provider adapters map to SDK calls.
    """
    model: str
    messages: List[Message]
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    response_format: Optional[str] = None  # "text" | "json_object" | adapter-specific
    extra: Dict[str, Any] = field(default_factory=dict)  # Escape hatch for rare needs

    def to_dict(self) -> Dict[str, Any]:
        return {
            "model": self.model,
            "messages": [
                {
                    "role": m.role,
                    "content": (
                        m.content if isinstance(m.content, str)
                        else [p.to_dict() for p in m.content]
                    ),
                }
                for m in self.messages
            ],
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "response_format": self.response_format,
            "extra": self.extra,
        }


@dataclass
class ChatResponse:
    """
    Normalized response from an LLM provider.

    At least one of (text, parts) should be present.
    The raw field may contain an SDK object or dict for debugging.
    """
    text: Optional[str]
    parts: Optional[List[ContentPart]]
    raw: Optional[Any]
    meta: ProviderMetadata

    def to_dict(self) -> Dict[str, Any]:
        return {
            "text": self.text,
            "parts": [p.to_dict() for p in self.parts] if self.parts else None,
            "raw": None,  # Avoid serializing heavy raw objects by default
            "meta": self.meta.to_dict(),
        }


@dataclass
class ModelInfo:
    """
    Describes a model entry from a provider's "get models" listing.

    The JSON files in src/providers/{provider}/{provider}-models.json
    can be represented as a list[ModelInfo], potentially with provider-
    specific 'capabilities' fields preserved in 'capabilities'.
    """
    id: str
    name: str
    provider: str
    family: Optional[str] = None  # e.g., "gpt-4o", "o1", "claude-3"
    context_length: Optional[int] = None
    capabilities: Dict[str, Any] = field(default_factory=dict)
    updated_at: Optional[str] = None  # iso8601 string if sourced from an API

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class ModelRegistrySnapshot:
    """
    A snapshot of the model registry for a single provider.
    """
    provider: str
    models: List[ModelInfo]
    fetched_via: Optional[str] = None  # "api", "local", "ollama_list", etc.
    fetched_at: Optional[str] = None   # iso8601 timestamp
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "provider": self.provider,
            "models": [m.to_dict() for m in self.models],
            "fetched_via": self.fetched_via,
            "fetched_at": self.fetched_at,
            "metadata": self.metadata,
        }]]></content>
    </file>
    <file>
      <path>providers/base/repositories/__init__.py</path>
      <content><![CDATA["""
Repositories package for providers layer.

Exports:
- ModelRegistryRepository / ModelRegistryError: model listings load/refresh
- KeysRepository / KeyResolution: API key resolution
"""

from .model_registry import ModelRegistryRepository, ModelRegistryError
from .keys import KeysRepository, KeyResolution

__all__ = [
    "ModelRegistryRepository",
    "ModelRegistryError",
    "KeysRepository",
    "KeyResolution",
]]]></content>
    </file>
    <file>
      <path>providers/base/repositories/keys.py</path>
      <content><![CDATA["""
Keys Repository

Purpose
- Centralize API key and related credential resolution for providers.
- Prefer environment variables; optionally read from unified config when available.
- Keep logic contained in providers layer (no external writes).

Design
- Non-throwing accessors that return None if a key is not resolved.
- Simple, explicit env var map per provider.
- Optional config fallbacks via src.config_loader if present.

Usage
- repo = KeysRepository()
- key = repo.get_api_key("openai")
"""

from __future__ import annotations

import os
from dataclasses import dataclass
from typing import Any, Dict, Optional


# Optional unified config loader (do not fail if absent)
try:
    from src.config_loader import config_loader  # type: ignore
except Exception:
    config_loader = None  # type: ignore


@dataclass
class KeyResolution:
    provider: str
    api_key: Optional[str]
    source: str  # "env", "config", "none"
    extra: Dict[str, Any]


class KeysRepository:
    """
    Resolve provider credentials with a strict priority order:

    1) Environment variables (authoritative)
    2) Unified config.yaml (when available)
    3) None

    This repository only reads values; it does not mutate any external state.
    """

    ENV_MAP: Dict[str, str] = {
        "openai": "OPENAI_API_KEY",
        "anthropic": "ANTHROPIC_API_KEY",
        "deepseek": "DEEPSEEK_API_KEY",
        "gemini": "GEMINI_API_KEY",
        "xai": "XAI_API_KEY",
        "openrouter": "OPENROUTER_API_KEY",
        # ollama uses local runtime; no API key by default
    }

    def get_api_key(self, provider: str) -> Optional[str]:
        return self.get_resolution(provider).api_key

    def get_resolution(self, provider: str) -> KeyResolution:
        p = (provider or "").lower().strip()
        env_var = self.ENV_MAP.get(p)

        # 1) Environment
        if env_var:
            val = os.getenv(env_var)
            if val:
                return KeyResolution(provider=p, api_key=val, source="env", extra={"env_var": env_var})

        # 2) Unified config (best-effort)
        cfg_key, extra = self._from_config(p)

        if cfg_key:
            return KeyResolution(provider=p, api_key=cfg_key, source="config", extra=extra)

        # 3) Settings repository (SQLite) fallback
        try:
            # Lazy import to avoid hard dependency at import time
            from src.settings import get_settings_repo  # type: ignore
            repo = get_settings_repo()
            db_key = repo.get_api_key(p)
            if db_key:
                return KeyResolution(provider=p, api_key=db_key, source="settings_db", extra={"repo": "sqlite"})
        except Exception:
            # Silent fallback to none if settings infra not available
            pass

        # 4) None
        return KeyResolution(provider=p, api_key=None, source="none", extra=extra)

    # -------------------- internal helpers --------------------

    def _from_config(self, provider: str) -> (Optional[str], Dict[str, Any]):
        """
        Attempt to read API key equivalents from config.yaml via config_loader.
        Returns (key_or_none, meta).
        """
        meta: Dict[str, Any] = {}
        if not config_loader:
            meta["config_loader"] = "absent"
            return None, meta

        try:
            api_cfg = config_loader.get_section("api") or {}
            prov_cfg = api_cfg.get(provider) or {}
            meta["has_api_section"] = bool(api_cfg)
            meta["has_provider_section"] = bool(prov_cfg)

            # Common key fields across providers
            for field in ("api_key", "resolved_key", "key"):
                if field in prov_cfg and isinstance(prov_cfg[field], str) and prov_cfg[field]:
                    meta["field"] = field
                    return prov_cfg[field], meta

            # Some configs may store under 'token'
            token = prov_cfg.get("token")
            if isinstance(token, str) and token:
                meta["field"] = "token"
                return token, meta

            return None, meta
        except Exception as e:
            meta["error"] = str(e)
            return None, meta]]></content>
    </file>
    <file>
      <path>providers/base/repositories/model_registry.py</path>
      <content><![CDATA["""
Model Registry Repository

Purpose
- Provide a normalized way to load and refresh per-provider model listings.
- Persist model listings into versioned JSON files located under:
  src/providers/{provider}/{provider}-models.json

Design
- Read snapshot from JSON and return typed models (ModelInfo).
- Optional refresh uses a provider-specific "get models" module if present.
- Ollama: optional fallback via local 'ollama list' command when permitted.
- Does not import SDKs; provider modules own API interactions.

Notes
- JSON shape tolerated:
  - {"models": [...], "fetched_at": "...", "source": "..."} OR
  - simple list: [...]
- When unknown fields are present, we preserve them in snapshot.metadata.

Dependencies
- DTOs: src/providers/base/models.py
- Optional provider modules:
  - src/providers/openai/get_openai_models.py
  - src/providers/anthropic/get_anthropic_models.py
  - src/providers/ollama/get_ollama_models.py
  - ... similar for others (gemini, xai, openrouter)

Usage
- repo = ModelRegistryRepository()
- snap = repo.list_models("openai", refresh=True)
"""

from __future__ import annotations

import json
import os
import subprocess
import sys
from dataclasses import asdict
from datetime import datetime, timezone
from importlib import import_module
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from ..models import ModelInfo, ModelRegistrySnapshot


PROVIDER_JSON_FILENAMES: Dict[str, str] = {
    "openai": "openai-models.json",
    "anthropic": "anthropic-models.json",
    "deepseek": "deepseek-models.json",
    "gemini": "gemini-models.json",
    "xai": "xai-models.json",
    "openrouter": "openrouter-models.json",
    "ollama": "ollama-models.json",
}


class ModelRegistryError(Exception):
    pass


class ModelRegistryRepository:
    """
    Manage model registry snapshots for providers.

    This repository only reads/writes JSON on disk and optionally invokes
    provider-specific refresh scripts. It intentionally avoids importing
    any cloud SDKs directly.
    """

    def __init__(self, providers_root: Optional[Path] = None) -> None:
        # Default to this file's ../../ directory: src/providers
        self.providers_root = providers_root or Path(__file__).resolve().parents[2]

    # -------------------- Public API --------------------

    def list_models(self, provider: str, refresh: bool = False) -> ModelRegistrySnapshot:
        """
        Load provider model list. If refresh=True, attempt to update from source first.
        """
        provider = provider.lower().strip()
        if refresh:
            try:
                self._refresh_provider_models(provider)
            except Exception as e:
                # Non-fatal: attempt to read cached JSON even if refresh failed
                print(f"[ModelRegistry] Refresh failed for {provider}: {e}. Falling back to cache.", file=sys.stderr)

        # Load JSON snapshot (may be empty)
        data, src_path = self._read_provider_json(provider)
        models, meta = self._parse_models(provider, data)
        snapshot = ModelRegistrySnapshot(
            provider=provider,
            models=models,
            fetched_via=meta.get("source") or meta.get("fetched_via"),
            fetched_at=meta.get("fetched_at"),
            metadata={k: v for k, v in meta.items() if k not in {"source", "fetched_via", "fetched_at"}},
        )
        return snapshot

    def save_snapshot(self, snapshot: ModelRegistrySnapshot) -> Path:
        """
        Persist a snapshot to the provider's JSON file. Overwrites existing content.
        """
        path = self._json_path(snapshot.provider)
        path.parent.mkdir(parents=True, exist_ok=True)

        # Prefer consistent top-level shape
        payload: Dict[str, Any] = {
            "provider": snapshot.provider,
            "models": [m.to_dict() for m in snapshot.models],
            "fetched_at": snapshot.fetched_at or self._now_iso(),
            "fetched_via": snapshot.fetched_via or "local",
            "metadata": snapshot.metadata,
        }

        with path.open("w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)

        return path

    # -------------------- Refresh strategies --------------------

    def _refresh_provider_models(self, provider: str) -> None:
        """
        Try to refresh using a provider-specific module. Non-fatal on error.

        Expected module path pattern:
          src.providers.{provider}.get_{provider}_models

        Expected callable names (first found is used):
          - refresh_models()
          - update_models()
          - fetch_models()
          - get_models()
          - run()
          - main()
        """
        # Special-case minimal local listing for ollama if no module present
        if provider == "ollama":
            if self._try_provider_refresh_module(provider):
                return
            try:
                self._refresh_via_ollama_cli()
                return
            except Exception as e:
                raise ModelRegistryError(f"Ollama refresh failed: {e}") from e

        # Default path: attempt provider module
        ok = self._try_provider_refresh_module(provider)
        if not ok:
            raise ModelRegistryError(f"No refresh entry point found for provider '{provider}'")

    def _try_provider_refresh_module(self, provider: str) -> bool:
        module_name = f"src.providers.{provider}.get_{provider}_models"
        try:
            mod = import_module(module_name)
        except Exception:
            # Try package-local (when executed from within providers)
            try:
                module_name_local = f".{provider}.get_{provider}_models"
                mod = import_module(module_name_local, package="src.providers")
            except Exception:
                return False

        # Candidate function names in order
        candidates = ["refresh_models", "update_models", "fetch_models", "get_models", "run", "main"]
        for name in candidates:
            fn = getattr(mod, name, None)
            if callable(fn):
                result = fn()  # Adapters should write JSON file and may return snapshot or list
                # If adapter returned a list/dict, persist in our canonical JSON.
                self._persist_result_if_returned(provider, result)
                return True
        return False

    def _persist_result_if_returned(self, provider: str, result: Any) -> None:
        if result is None:
            return
        # Normalize common shapes
        models: List[ModelInfo] = []
        meta: Dict[str, Any] = {"fetched_via": "api", "fetched_at": self._now_iso()}

        try:
            if isinstance(result, list):
                # Assume list of dict-like models
                for item in result:
                    if isinstance(item, ModelInfo):
                        models.append(item)
                    elif isinstance(item, dict):
                        models.append(self._model_from_dict(provider, item))
                    else:
                        models.append(ModelInfo(id=str(item), name=str(item), provider=provider))
            elif isinstance(result, dict):
                # Try 'models' key
                if "models" in result and isinstance(result["models"], list):
                    for item in result["models"]:
                        if isinstance(item, ModelInfo):
                            models.append(item)
                        elif isinstance(item, dict):
                            models.append(self._model_from_dict(provider, item))
                # Copy metadata if present
                for k in ("fetched_via", "source", "fetched_at"):
                    if k in result and isinstance(result[k], str):
                        meta[k] = result[k]
            else:
                # Fallback: stringify
                models.append(ModelInfo(id=str(result), name=str(result), provider=provider))
        except Exception:
            # On parsing failure, do not overwrite existing registry
            return

        if models:
            snapshot = ModelRegistrySnapshot(provider=provider, models=models, fetched_via=meta.get("fetched_via"), fetched_at=meta.get("fetched_at"), metadata={})
            self.save_snapshot(snapshot)

    def _refresh_via_ollama_cli(self) -> None:
        """
        Use 'ollama list' to produce a simple model registry when available.

        This does not validate contracts; it only captures model names and sizes.
        """
        try:
            out = subprocess.check_output(["ollama", "list"], stderr=subprocess.STDOUT, text=True, timeout=10)
        except Exception as e:
            raise RuntimeError(f"ollama list failed: {e}")

        models: List[ModelInfo] = []
        # Expected sample output header:
        # NAME                                 ID              SIZE    MODIFIED
        lines = [ln.strip() for ln in out.splitlines() if ln.strip()]
        if lines:
            # Skip header if it looks like the column header
            data_lines = lines[1:] if "NAME" in lines[0].upper() and "SIZE" in lines[0].upper() else lines
            for ln in data_lines:
                # naive split on whitespace; first token is name
                name = ln.split()[0]
                models.append(ModelInfo(id=name, name=name, provider="ollama"))

        snapshot = ModelRegistrySnapshot(
            provider="ollama",
            models=models,
            fetched_via="ollama_list",
            fetched_at=self._now_iso(),
            metadata={"source": "ollama_cli"},
        )
        self.save_snapshot(snapshot)

    # -------------------- JSON IO --------------------

    def _read_provider_json(self, provider: str) -> Tuple[Dict[str, Any], Path]:
        path = self._json_path(provider)
        if not path.exists():
            return ({}, path)
        try:
            # Fast-path: tolerate empty or whitespace-only files
            try:
                if path.stat().st_size == 0:
                    print(f"[ModelRegistry] Empty JSON file at {path}; treating as empty registry.", file=sys.stderr)
                    return ({}, path)
                content = path.read_text(encoding="utf-8")
                if not content.strip():
                    print(f"[ModelRegistry] Whitespace-only JSON file at {path}; treating as empty registry.", file=sys.stderr)
                    return ({}, path)
            except Exception:
                # If a stat/read preview error occurs, fall through to json.load
                pass

            with path.open("r", encoding="utf-8") as f:
                data = json.load(f) or {}
            return (data, path)
        except Exception as e:
            # Be resilient during offline/unit tests: warn and return empty registry
            print(f"[ModelRegistry] Warning: failed to parse provider JSON at {path}: {e}. Treating as empty registry.", file=sys.stderr)
            return ({}, path)

    def _json_path(self, provider: str) -> Path:
        filename = PROVIDER_JSON_FILENAMES.get(provider)
        if not filename:
            # default filename if not mapped
            filename = f"{provider}-models.json"
        return self.providers_root / provider / filename

    # -------------------- Parsing helpers --------------------

    def _parse_models(self, provider: str, data: Dict[str, Any]) -> Tuple[List[ModelInfo], Dict[str, Any]]:
        """
        Accept either a dict with 'models' or a plain list.
        """
        meta: Dict[str, Any] = {}
        raw_models: List[Any] = []

        if isinstance(data, dict):
            # Copy meta-ish fields if present
            for k in ("provider", "source", "fetched_via", "fetched_at", "metadata"):
                if k in data:
                    meta[k] = data[k]
            if isinstance(data.get("models"), list):
                raw_models = data["models"]
            elif isinstance(data.get("data"), list):  # tolerate alt schema
                raw_models = data["data"]
            else:
                raw_models = []
        elif isinstance(data, list):
            raw_models = data
        else:
            raw_models = []

        models: List[ModelInfo] = []
        for item in raw_models:
            if isinstance(item, ModelInfo):
                models.append(item)
            elif isinstance(item, dict):
                models.append(self._model_from_dict(provider, item))
            else:
                sid = str(item)
                models.append(ModelInfo(id=sid, name=sid, provider=provider))

        return models, meta

    def _model_from_dict(self, provider: str, d: Dict[str, Any]) -> ModelInfo:
        # Try common fields; tolerate different schemas
        mid = str(d.get("id") or d.get("name") or d.get("slug") or d.get("model") or "unknown")
        name = str(d.get("name") or d.get("id") or d.get("slug") or d.get("model") or mid)
        fam = d.get("family") or d.get("series")
        ctx = d.get("context_length") or d.get("max_context") or d.get("ctx") or None
        caps = d.get("capabilities") or {}
        updated_at = d.get("updated_at") or d.get("fetched_at")

        # Idempotent cast
        try:
            ctx = int(ctx) if ctx is not None else None
        except Exception:
            ctx = None

        if not isinstance(caps, dict):
            caps = {"raw_capabilities": caps}

        return ModelInfo(
            id=mid,
            name=name,
            provider=provider,
            family=fam if isinstance(fam, str) else None,
            context_length=ctx,
            capabilities=caps,
            updated_at=updated_at if isinstance(updated_at, str) else None,
        )

    # -------------------- Utils --------------------

    @staticmethod
    def _now_iso() -> str:
        return datetime.now(timezone.utc).isoformat()]]></content>
    </file>
    <file>
      <path>providers/decorators.py</path>
      <content><![CDATA["""
Decorator utilities for AI providers.

This module provides useful decorators to simplify error handling, retry logic,
and other cross-cutting concerns for AI provider implementations.
"""

import time
import functools
import logging
from typing import Callable, Any, Optional, TypeVar, Dict, List, Union, Tuple

# Import exceptions
from .exceptions import (
    ApiCallError, 
    ApiResponseError, 
    MaxRetriesExceededError,
    JsonParsingError
)

logger = logging.getLogger(__name__)

T = TypeVar('T')  # Return type for generic functions

def with_retry(max_attempts: int = 3, delay_base: float = 2.0):
    """
    Decorator that adds retry logic to API calls.
    
    Args:
        max_attempts: Maximum number of retry attempts
        delay_base: Base for exponential backoff delay calculation
    
    Returns:
        Decorator function
    """
    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> T:
            last_exception = None
            
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except (ApiCallError, ApiResponseError) as e:
                    last_exception = e
                    if attempt < max_attempts - 1:
                        delay = delay_base ** attempt
                        logger.warning(f"API call failed, retrying in {delay}s: {e}")
                        time.sleep(delay)
                    else:
                        logger.error(f"API call failed after {max_attempts} retries: {e}")
            
            # If we get here, all attempts have failed
            if last_exception:
                raise MaxRetriesExceededError(f"Maximum retries exceeded: {last_exception}")
            else:
                raise ApiCallError("All API call attempts failed")
                
        return wrapper
    return decorator


def with_error_handling(func: Callable[..., T]) -> Callable[..., T]:
    """
    Decorator that handles common API errors and provides consistent logging.
    
    Args:
        func: The function to decorate
    
    Returns:
        Decorated function
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs) -> T:
        try:
            return func(*args, **kwargs)
        except ApiCallError as e:
            logger.error(f"API call error in {func.__name__}: {e}")
            raise
        except ApiResponseError as e:
            logger.error(f"API response error in {func.__name__}: {e}")
            raise
        except JsonParsingError as e:
            logger.error(f"JSON parsing error in {func.__name__}: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in {func.__name__}: {e}", exc_info=True)
            raise ApiCallError(f"Unexpected error in {func.__name__}: {e}") from e
            
    return wrapper


def with_fallback(fallback_provider: str = 'openai'):
    """
    Decorator that provides a fallback to another provider if the primary one fails.
    
    Args:
        fallback_provider: Name of fallback provider to use
    
    Returns:
        Decorator function
    """
    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> T:
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.warning(f"Primary provider {func.__module__} failed: {e}, falling back to {fallback_provider}")
                
                # Import the fallback provider and call it with the same arguments
                # We import inside the function to avoid circular imports
                if fallback_provider == 'openai':
                    from . import openai_client
                    return openai_client.run_openai_client(*args, **kwargs)
                elif fallback_provider == 'anthropic':
                    from . import anthropic_client
                    return anthropic_client.run_anthropic_client(*args, **kwargs)
                elif fallback_provider == 'deepseek':
                    from . import deepseek_client
                    return deepseek_client.run_deepseek_client(*args, **kwargs)
                elif fallback_provider == 'gemini':
                    from . import gemini_client
                    return gemini_client.run_gemini_client(*args, **kwargs)
                else:
                    logger.error(f"Unknown fallback provider: {fallback_provider}")
                    raise
                
        return wrapper
    return decorator


def cache_result(maxsize: int = 128, ttl: int = 3600):
    """
    Decorator that caches function results with a time-to-live.
    
    Args:
        maxsize: Maximum size of the cache
        ttl: Time-to-live in seconds
        
    Returns:
        Decorator function
    """
    cache = {}
    timestamps = {}
    
    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> T:
            # Create a hashable key from the arguments
            key = str((args, frozenset(sorted(kwargs.items()))))
            
            # Check if result is in cache and not expired
            current_time = time.time()
            if key in cache and current_time - timestamps[key] < ttl:
                logger.debug(f"Cache hit for {func.__name__}")
                return cache[key]
            
            # Call the function and cache the result
            result = func(*args, **kwargs)
            
            # Manage cache size with simple LRU approach
            if len(cache) >= maxsize:
                oldest_key = min(timestamps.items(), key=lambda x: x[1])[0]
                del cache[oldest_key]
                del timestamps[oldest_key]
            
            cache[key] = result
            timestamps[key] = current_time
            return result
            
        return wrapper
    return decorator
]]></content>
    </file>
    <file>
      <path>providers/deepseek/deepseek-models.json</path>
      <content/>
    </file>
    <file>
      <path>providers/deepseek/get_deepseek_models.py</path>
      <content><![CDATA["""
DeepSeek: get models

Behavior
- Attempts to fetch model listings from DeepSeek's OpenAI-compatible HTTP endpoint:
  GET {DEEPSEEK_BASE_URL or https://api.deepseek.com/v1}/models
- Persists to JSON at: src/providers/deepseek/deepseek-models.json
- If API key or HTTP client is unavailable or fails, falls back to cached JSON (no network).

Entry points recognized by the ModelRegistryRepository:
- run()  (preferred)
- get_models()/fetch_models()/update_models()/refresh_models() also provided for convenience
"""

from __future__ import annotations

import os
from typing import Any, Dict, List, Optional

try:
    import requests  # type: ignore
except Exception:
    requests = None  # type: ignore

from src.providers.base.get_models_base import save_provider_models, load_cached_models
from src.providers.base.repositories.keys import KeysRepository

PROVIDER = "deepseek"


def _resolve_key() -> Optional[str]:
    return KeysRepository().get_api_key(PROVIDER)


def _resolve_base_url() -> str:
    # Default to the documented base; allow override via env
    return os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")


def _fetch_via_http(api_key: str, base_url: str) -> List[Dict[str, Any]]:
    """
    Fetch model listings using OpenAI-compatible HTTP endpoint.

    Returns a list of dicts with at least {'id', 'name'} keys.
    """
    if requests is None:
        raise RuntimeError("requests library not available")

    url = base_url.rstrip("/") + "/models"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Accept": "application/json",
    }
    resp = requests.get(url, headers=headers, timeout=15)
    resp.raise_for_status()
    data = resp.json()

    # Accept either a plain list or {"data": [...]}
    raw = data.get("data", data) if isinstance(data, dict) else data
    items: List[Dict[str, Any]] = []
    for it in raw or []:
        if isinstance(it, dict):
            # Normalize id/name
            mid = it.get("id") or it.get("model") or it.get("name") or str(it)
            name = it.get("name") or it.get("id") or str(it)
            row = {"id": str(mid), "name": str(name)}
            # Passthrough useful fields when present
            for k in ("created", "modalities", "context_length", "max_context", "capabilities"):
                if k in it:
                    row[k] = it[k]
            items.append(row)
        else:
            items.append({"id": str(it), "name": str(it)})
    return items


def run() -> List[Dict[str, Any]]:
    """
    Preferred entrypoint. Attempts online refresh; falls back to cached snapshot.

    Returns a list of dicts (models) for convenience; ModelRegistryRepository can
    also parse and persist this return value.
    """
    key = _resolve_key()
    if key:
        try:
            base = _resolve_base_url()
            items = _fetch_via_http(key, base)
            if items:
                save_provider_models(PROVIDER, items, fetched_via="api", metadata={"source": "deepseek_http_models"})
                return items
        except Exception:
            # Fall through to cached
            pass

    snap = load_cached_models(PROVIDER)
    return [m.to_dict() for m in snap.models]


# Aliases for repository compatibility
def get_models() -> List[Dict[str, Any]]:
    return run()


def fetch_models() -> List[Dict[str, Any]]:
    return run()


def update_models() -> List[Dict[str, Any]]:
    return run()


def refresh_models() -> List[Dict[str, Any]]:
    return run()


if __name__ == "__main__":
    models = run()
    print(f"[deepseek] loaded {len(models)} models")]]></content>
    </file>
    <file>
      <path>providers/exceptions.py</path>
      <content><![CDATA["""
Custom exceptions for LLM provider interactions.
"""

class ProviderError(Exception):
    """Base class for provider-related errors."""
    pass

class ApiKeyError(ProviderError):
    """Error related to API key configuration or validity."""
    pass

class ApiCallError(ProviderError):
    """Error during the API call itself (e.g., network issue, server error)."""
    pass

class ApiResponseError(ProviderError):
    """Error related to the structure or content of the API response."""
    pass

class ApiBlockedError(ApiResponseError):
    """Error indicating the request was blocked, often due to safety filters."""
    def __init__(self, message, reason=None, ratings=None):
        super().__init__(message)
        self.reason = reason
        self.ratings = ratings

    def __str__(self):
        details = f"Reason: {self.reason}" if self.reason else "No reason provided"
        if self.ratings:
            details += f", Safety Ratings: {self.ratings}"
        return f"{super().__str__()} ({details})"


class JsonParsingError(ApiResponseError):
    """Error when failing to parse JSON from the API response."""
    pass

class JsonProcessingError(ApiResponseError):
    """Error during processing of the parsed JSON response."""
    pass

class ModelCallError(ProviderError):
    """Error during a model API call."""
    pass

class MaxRetriesExceededError(ProviderError):
    """Error when maximum number of retries is exceeded."""
    pass
]]></content>
    </file>
    <file>
      <path>providers/gemini/gemini-models.json</path>
      <content/>
    </file>
    <file>
      <path>providers/gemini/get_gemini_models.py</path>
      <content><![CDATA["""
Gemini: get models

Behavior
- Attempts to fetch model listings via Google Generative AI SDK (google-generativeai).
- Persists to JSON at: src/providers/gemini/gemini-models.json
- If API key or SDK is unavailable, falls back to cached JSON (no network).

Entry points recognized by the ModelRegistryRepository:
- run()  (preferred)
- get_models()/fetch_models()/update_models()/refresh_models() also provided for convenience
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional

try:
    import google.generativeai as genai  # pip install google-generativeai
except Exception:
    genai = None  # type: ignore

from src.providers.base.get_models_base import save_provider_models, load_cached_models
from src.providers.base.repositories.keys import KeysRepository


PROVIDER = "gemini"


def _fetch_via_sdk(api_key: str) -> List[Any]:
    """
    Fetch model listings using Google Generative AI SDK. Returns raw items (SDK objects or dicts).
    """
    if genai is None:
        raise RuntimeError("google-generativeai SDK not available")

    # Configure API key
    genai.configure(api_key=api_key)

    # list_models returns iterable of Model objects with attributes:
    # - name (e.g., 'models/gemini-1.5-pro')
    # - supported_generation_methods
    # - input_token_limit, output_token_limit (on some versions)
    models = list(genai.list_models())

    return models


def _resolve_key() -> Optional[str]:
    # GEMINI_API_KEY via KeysRepository
    return KeysRepository().get_api_key(PROVIDER)


def run() -> List[Dict[str, Any]]:
    """
    Preferred entrypoint. Attempts online refresh; falls back to cached snapshot.

    Returns a list of dicts (models) for convenience; ModelRegistryRepository can
    also parse and persist this return value.
    """
    key = _resolve_key()
    if key:
        try:
            items = _fetch_via_sdk(key)
            # Persist normalized snapshot
            save_provider_models(PROVIDER, items, fetched_via="api", metadata={"source": "google_generativeai"})
            # Return lightweight list of dicts
            out: List[Dict[str, Any]] = []
            for it in items:
                # Try to extract useful fields defensively
                name = getattr(it, "name", None) or str(it)
                # Some SDK versions expose token limits
                input_limit = getattr(it, "input_token_limit", None)
                output_limit = getattr(it, "output_token_limit", None)
                out.append(
                    {
                        "id": name,
                        "name": name,
                        "input_token_limit": input_limit,
                        "output_token_limit": output_limit,
                    }
                )
            return out
        except Exception:
            # Fall through to cached
            pass

    snap = load_cached_models(PROVIDER)
    return [m.to_dict() for m in snap.models]


# Aliases for repository compatibility
def get_models() -> List[Dict[str, Any]]:
    return run()


def fetch_models() -> List[Dict[str, Any]]:
    return run()


def update_models() -> List[Dict[str, Any]]:
    return run()


def refresh_models() -> List[Dict[str, Any]]:
    return run()


if __name__ == "__main__":
    models = run()
    print(f"[gemini] loaded {len(models)} models")]]></content>
    </file>
    <file>
      <path>providers/ollama/get_ollama_models.py</path>
      <content><![CDATA["""
Ollama: get models

Behavior
- Attempts to fetch locally installed models via the 'ollama list' CLI.
- Persists to JSON at: src/providers/ollama/ollama-models.json
- If CLI is unavailable or fails, falls back to cached JSON (no network).

Entry points recognized by the ModelRegistryRepository:
- run()  (preferred)
- get_models()/fetch_models()/update_models()/refresh_models() also provided for convenience
"""

from __future__ import annotations

import json
import subprocess
from typing import Any, Dict, List

from src.providers.base.get_models_base import save_provider_models, load_cached_models

PROVIDER = "ollama"


def _fetch_via_cli() -> List[Dict[str, Any]]:
    """
    Fetch model listings using 'ollama list' command.
    Tries JSON output first; falls back to parsing table output.
    Returns a list of dicts with at least {'id', 'name'} keys.
    """
    # Try JSON output first (supported on modern ollama)
    try:
        out = subprocess.check_output(
            ["ollama", "list", "--json"], stderr=subprocess.STDOUT, text=True, timeout=10
        )
        data = json.loads(out)
        # Accept either a plain list or {"models": [...]}
        raw = data.get("models", data) if isinstance(data, dict) else data
        items: List[Dict[str, Any]] = []
        for it in raw or []:
            if isinstance(it, dict):
                name = it.get("name") or it.get("model") or str(it)
                items.append({"id": name, "name": name, **it})
            else:
                items.append({"id": str(it), "name": str(it)})
        return items
    except Exception:
        pass

    # Fallback to parsing table output
    out = subprocess.check_output(["ollama", "list"], stderr=subprocess.STDOUT, text=True, timeout=10)
    lines = [ln.strip() for ln in out.splitlines() if ln.strip()]
    if not lines:
        return []
    # Skip header if present
    data_lines = lines[1:] if "NAME" in lines[0].upper() and "SIZE" in lines[0].upper() else lines
    items: List[Dict[str, Any]] = []
    for ln in data_lines:
        # naive split: first token is model name
        parts = ln.split()
        if not parts:
            continue
        name = parts[0]
        items.append({"id": name, "name": name})
    return items


def run() -> List[Dict[str, Any]]:
    """
    Preferred entrypoint. Attempts local refresh; falls back to cached snapshot.

    Returns a list of dicts (models) for convenience; ModelRegistryRepository can
    also parse and persist this return value.
    """
    try:
        items = _fetch_via_cli()
        if items:
            save_provider_models(PROVIDER, items, fetched_via="ollama_list", metadata={"source": "ollama_cli"})
            return items
    except Exception:
        # Fall through to cached
        pass

    snap = load_cached_models(PROVIDER)
    return [m.to_dict() for m in snap.models]


# Aliases for repository compatibility
def get_models() -> List[Dict[str, Any]]:
    return run()


def fetch_models() -> List[Dict[str, Any]]:
    return run()


def update_models() -> List[Dict[str, Any]]:
    return run()


def refresh_models() -> List[Dict[str, Any]]:
    return run()


if __name__ == "__main__":
    models = run()
    print(f"[ollama] loaded {len(models)} models")]]></content>
    </file>
    <file>
      <path>providers/ollama/ollama-models.json</path>
      <content/>
    </file>
    <file>
      <path>providers/openai/__init__.py</path>
      <content><![CDATA["""
OpenAI provider package.

Exports:
- OpenAIProvider: Adapter implementing LLMProvider for OpenAI

See:
- [client.py](Cogito/src/providers/openai/client.py)
"""

from .client import OpenAIProvider

__all__ = ["OpenAIProvider"]]]></content>
    </file>
    <file>
      <path>providers/openai/client.py</path>
      <content><![CDATA["""
OpenAIProvider adapter

Implements the provider-agnostic interfaces defined in
[interfaces.py](Cogito/src/providers/base/interfaces.py) using the existing
OpenAI client implementation at [openai_client.py](Cogito/src/providers/openai_client.py).

This adapter:
- Accepts a normalized [ChatRequest](Cogito/src/providers/base/models.py:1)
- Invokes [call_openai_with_retry()](Cogito/src/providers/openai_client.py:51)
- Returns a normalized [ChatResponse](Cogito/src/providers/base/models.py:1)
- Exposes model registry listing via [ModelRegistryRepository](Cogito/src/providers/base/repositories/model_registry.py:1)

Notes
- Non-invasive: no changes to existing openai_client behavior.
- Stays contained within providers/ as requested.
"""

from __future__ import annotations

import json
import time
from typing import Optional, List

from src.providers.base.interfaces import (
    LLMProvider,
    SupportsJSONOutput,
    SupportsResponsesAPI,
    ModelListingProvider,
    HasDefaultModel,
)
from src.providers.base.models import (
    ChatRequest,
    ChatResponse,
    ProviderMetadata,
    Message,
    ContentPart,
)
from src.providers.base.repositories.model_registry import ModelRegistryRepository
from src.providers.model_config import get_openai_config
from src.providers.openai_client import call_openai_with_retry


class OpenAIProvider(LLMProvider, SupportsJSONOutput, SupportsResponsesAPI, ModelListingProvider, HasDefaultModel):
    """
    Adapter for OpenAI which conforms to provider-agnostic contracts.

    Construction:
      provider = OpenAIProvider()  # uses defaults from [get_openai_config()](Cogito/src/providers/model_config.py:69)
      response = provider.chat(ChatRequest(...))
    """

    def __init__(self, default_model: Optional[str] = None, registry: Optional[ModelRegistryRepository] = None) -> None:
        cfg = get_openai_config()
        self._default_model = default_model or cfg.get("model") or "o3-mini"
        self._registry = registry or ModelRegistryRepository()

    @property
    def provider_name(self) -> str:
        return "openai"

    def default_model(self) -> Optional[str]:
        return self._default_model

    def supports_json_output(self) -> bool:
        return True

    def uses_responses_api(self, model: str) -> bool:
        lower = (model or "").lower()
        return ("o1" in lower) or ("o3-mini" in lower)

    def list_models(self, refresh: bool = False):
        return self._registry.list_models("openai", refresh=refresh)

    # -------------------- Core Chat --------------------

    def chat(self, request: ChatRequest) -> ChatResponse:
        """
        Execute a single chat request via existing call_openai_with_retry, normalizing the result.
        """
        # Resolve model with request override
        model = (request.model or self._default_model)

        # Extract a single system message (first wins) and concatenate user content
        system_message: Optional[str] = None
        user_segments: List[str] = []

        for m in request.messages:
            if not isinstance(m, Message):
                continue
            if m.role == "system" and system_message is None:
                system_message = m.text_or_joined()
            elif m.role == "user":
                user_segments.append(m.text_or_joined())
            # assistant/tool roles are ignored for now in this minimal adapter; could be included as context

        user_content = "\n".join(s for s in user_segments if s)

        # Build config payload as expected by call_openai_with_retry
        api_config = {
            "api": {
                "openai": {
                    "model": model,
                    # request.max_tokens maps to the appropriate OpenAI param internally
                    **({"max_tokens": request.max_tokens} if request.max_tokens is not None else {}),
                    # temperature is omitted downstream if the chosen model family disallows it
                    **({"temperature": request.temperature} if request.temperature is not None else {}),
                    **({"system_message": system_message} if system_message else {}),
                }
            }
        }

        # Structured output?
        is_structured = (request.response_format == "json_object")

        # Invoke provider and measure latency
        t0 = time.perf_counter()
        try:
            resp, model_used = call_openai_with_retry(
                prompt_template="{content}",
                context={"content": user_content},
                config=api_config,
                is_structured=is_structured,
                # Also pass max_tokens explicitly to cooperate with responses.create path
                max_tokens=request.max_tokens if request.max_tokens is not None else None,
            )
            latency_ms = (time.perf_counter() - t0) * 1000.0
        except Exception as e:
            # Normalize catastrophic failures into a ChatResponse with error metadata
            meta = ProviderMetadata(
                provider_name=self.provider_name,
                model_name=model,
                http_status=None,
                request_id=None,
                latency_ms=None,
                extra={"error": str(e), "phase": "call_openai_with_retry"},
            )
            return ChatResponse(text=None, parts=None, raw=None, meta=meta)

        # Build metadata (limited visibility since underlying client hides HTTP details)
        meta = ProviderMetadata(
            provider_name=self.provider_name,
            model_name=str(model_used),
            token_param_used=None,          # Not directly exposed by call_openai_with_retry
            temperature_included=None,      # Not directly exposed by call_openai_with_retry
            http_status=None,               # Not returned by the high-level client
            request_id=None,                # Not returned by the high-level client
            latency_ms=latency_ms,
            extra={
                "is_structured": is_structured,
                "used_responses_api": self.uses_responses_api(str(model_used)),
                "response_format": request.response_format,
            },
        )

        # Normalize output into text/parts
        if isinstance(resp, dict):
            # Structured JSON returned; represent as a single JSON part and provide a text serialization
            try:
                text = json.dumps(resp, ensure_ascii=False)
            except Exception:
                text = str(resp)
            parts = [ContentPart(type="json", text=text, data=None)]
            return ChatResponse(text=text, parts=parts, raw=None, meta=meta)

        # Fallback: plain text or unknown object
        if isinstance(resp, str):
            return ChatResponse(text=resp, parts=None, raw=None, meta=meta)

        # Last resort: string-coerce unknown types
        return ChatResponse(text=str(resp), parts=None, raw=None, meta=meta)]]></content>
    </file>
    <file>
      <path>providers/openai/get_openai_models.py</path>
      <content><![CDATA["""
OpenAI: get models

Behavior
- Attempts to fetch model listings via OpenAI SDK.
- Persists to JSON at: src/providers/openai/openai-models.json
- If API key or SDK is unavailable, falls back to cached JSON (no network).

Entry points recognized by the ModelRegistryRepository:
- run()  (preferred)
- get_models()/fetch_models()/update_models()/refresh_models() also provided for convenience
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional

try:
    from openai import OpenAI  # openai>=1.0.0
except Exception:
    OpenAI = None  # type: ignore

from src.providers.base.get_models_base import save_provider_models, load_cached_models
from src.providers.base.repositories.keys import KeysRepository


PROVIDER = "openai"


def _fetch_via_sdk(api_key: str) -> List[Any]:
    """
    Fetch model listings using OpenAI SDK. Returns raw items (SDK objects or dicts).
    """
    if not OpenAI:
        raise RuntimeError("openai SDK not available")
    client = OpenAI(api_key=api_key)
    resp = client.models.list()
    # Support both object and dict
    data = getattr(resp, "data", None)
    if data is None and isinstance(resp, dict):
        data = resp.get("data", [])
    return list(data or [])


def _resolve_key() -> Optional[str]:
    return KeysRepository().get_api_key(PROVIDER)


def run() -> List[Dict[str, Any]]:
    """
    Preferred entrypoint. Attempts online refresh; falls back to cached snapshot.

    Returns a list of dicts (models) for convenience; ModelRegistryRepository can
    also parse and persist this return value.
    """
    key = _resolve_key()
    if key:
        try:
            # Initial list call
            items = _fetch_via_sdk(key)

            # Enrich with per-model metadata where available
            # Note: client.models.retrieve(id) often includes fields like `created`,
            # `modalities`, and token limits; we map these through to normalization.
            client = OpenAI(api_key=key)

            enriched: List[Dict[str, Any]] = []
            for it in items:
                mid = getattr(it, "id", None) or getattr(it, "model", None) or getattr(it, "name", None) or str(it)
                name = getattr(it, "name", None) or getattr(it, "id", None) or str(it)

                # Best-effort details fetch; tolerate failures
                det = None
                try:
                    det = client.models.retrieve(str(mid))
                except Exception:
                    det = None  # keep minimal

                # Gather possible fields from either list item or retrieved detail
                def g(obj: Any, attr: str):
                    return getattr(obj, attr, None) if obj is not None else None

                modalities = g(det, "modalities") or g(it, "modalities")
                input_token_limit = g(det, "input_token_limit") or g(it, "input_token_limit") or g(det, "context_window")
                created = g(det, "created") or g(it, "created")
                context_length = g(det, "context_length") or g(it, "context_length") or g(it, "max_context")

                # Build capabilities using modalities and id heuristics
                caps: Dict[str, Any] = {}
                if isinstance(modalities, (list, tuple)):
                    for m in modalities:
                        mstr = str(m).lower()
                        caps[mstr] = True
                        if mstr in ("image", "vision"):
                            caps["vision"] = True

                lower = str(mid).lower()
                if lower.startswith(("o1", "o3")):
                    caps["reasoning"] = True
                    caps["responses_api"] = True
                if "gpt-4o" in lower or "omni" in lower or "vision" in lower:
                    caps["vision"] = True
                if "embedding" in lower or lower.startswith("text-embedding"):
                    caps["embedding"] = True
                if "search" in lower:
                    caps["search"] = True
                # JSON structured outputs generally supported by major chat families
                caps.setdefault("json_output", True)

                row: Dict[str, Any] = {"id": mid, "name": name}

                # Prefer numeric context length from explicit fields
                if context_length is None and input_token_limit is not None:
                    context_length = input_token_limit
                if context_length is not None:
                    try:
                        row["context_length"] = int(context_length)
                    except Exception:
                        pass

                if isinstance(modalities, (list, tuple)):
                    row["modalities"] = list(modalities)

                if caps:
                    row["capabilities"] = caps

                if isinstance(created, (int, float)):
                    # Normalizer will convert created epoch to updated_at date
                    row["created"] = int(created)

                enriched.append(row)

            # Persist enriched snapshot; base normalizer will handle family/updated_at inference
            save_provider_models(PROVIDER, enriched, fetched_via="api", metadata={"source": "openai_sdk_enriched"})
            return [{"id": it["id"], "name": it["name"]} for it in enriched]
        except Exception:
            # Fall through to cached
            pass

    snap = load_cached_models(PROVIDER)
    return [m.to_dict() for m in snap.models]


# Aliases for repository compatibility
def get_models() -> List[Dict[str, Any]]:
    return run()


def fetch_models() -> List[Dict[str, Any]]:
    return run()


def update_models() -> List[Dict[str, Any]]:
    return run()


def refresh_models() -> List[Dict[str, Any]]:
    return run()


if __name__ == "__main__":
    models = run()
    print(f"[openai] loaded {len(models)} models")]]></content>
    </file>
    <file>
      <path>providers/openai/openai-models.json</path>
      <content><![CDATA[{
  "provider": "openai",
  "models": [
    {
      "id": "gpt-4-0613",
      "name": "gpt-4-0613",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-06-12"
    },
    {
      "id": "gpt-4",
      "name": "gpt-4",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-06-27"
    },
    {
      "id": "gpt-3.5-turbo",
      "name": "gpt-3.5-turbo",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-02-28"
    },
    {
      "id": "gpt-audio",
      "name": "gpt-audio",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-28"
    },
    {
      "id": "gpt-5-nano",
      "name": "gpt-5-nano",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-05"
    },
    {
      "id": "gpt-audio-2025-08-28",
      "name": "gpt-audio-2025-08-28",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-28"
    },
    {
      "id": "gpt-realtime",
      "name": "gpt-realtime",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-27"
    },
    {
      "id": "gpt-realtime-2025-08-28",
      "name": "gpt-realtime-2025-08-28",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-28"
    },
    {
      "id": "davinci-002",
      "name": "davinci-002",
      "provider": "openai",
      "family": "davinci",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-08-21"
    },
    {
      "id": "babbage-002",
      "name": "babbage-002",
      "provider": "openai",
      "family": "babbage",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-08-21"
    },
    {
      "id": "gpt-3.5-turbo-instruct",
      "name": "gpt-3.5-turbo-instruct",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-08-24"
    },
    {
      "id": "gpt-3.5-turbo-instruct-0914",
      "name": "gpt-3.5-turbo-instruct-0914",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-09-07"
    },
    {
      "id": "dall-e-3",
      "name": "dall-e-3",
      "provider": "openai",
      "family": "dall",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-10-31"
    },
    {
      "id": "dall-e-2",
      "name": "dall-e-2",
      "provider": "openai",
      "family": "dall",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-11-01"
    },
    {
      "id": "gpt-4-1106-preview",
      "name": "gpt-4-1106-preview",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-11-02"
    },
    {
      "id": "gpt-3.5-turbo-1106",
      "name": "gpt-3.5-turbo-1106",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-11-02"
    },
    {
      "id": "tts-1-hd",
      "name": "tts-1-hd",
      "provider": "openai",
      "family": "tts",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-11-03"
    },
    {
      "id": "tts-1-1106",
      "name": "tts-1-1106",
      "provider": "openai",
      "family": "tts",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-11-03"
    },
    {
      "id": "tts-1-hd-1106",
      "name": "tts-1-hd-1106",
      "provider": "openai",
      "family": "tts",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-11-03"
    },
    {
      "id": "text-embedding-3-small",
      "name": "text-embedding-3-small",
      "provider": "openai",
      "family": "text",
      "context_length": null,
      "capabilities": {
        "embedding": true,
        "json_output": true
      },
      "updated_at": "2024-01-22"
    },
    {
      "id": "text-embedding-3-large",
      "name": "text-embedding-3-large",
      "provider": "openai",
      "family": "text",
      "context_length": null,
      "capabilities": {
        "embedding": true,
        "json_output": true
      },
      "updated_at": "2024-01-22"
    },
    {
      "id": "gpt-4-0125-preview",
      "name": "gpt-4-0125-preview",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2024-01-23"
    },
    {
      "id": "gpt-4-turbo-preview",
      "name": "gpt-4-turbo-preview",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2024-01-23"
    },
    {
      "id": "gpt-3.5-turbo-0125",
      "name": "gpt-3.5-turbo-0125",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2024-01-23"
    },
    {
      "id": "gpt-4-turbo",
      "name": "gpt-4-turbo",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2024-04-05"
    },
    {
      "id": "gpt-4-turbo-2024-04-09",
      "name": "gpt-4-turbo-2024-04-09",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2024-04-09"
    },
    {
      "id": "gpt-4o",
      "name": "gpt-4o",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-05-10"
    },
    {
      "id": "gpt-4o-2024-05-13",
      "name": "gpt-4o-2024-05-13",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-05-13"
    },
    {
      "id": "gpt-4o-mini-2024-07-18",
      "name": "gpt-4o-mini-2024-07-18",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-07-18"
    },
    {
      "id": "gpt-4o-mini",
      "name": "gpt-4o-mini",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-07-16"
    },
    {
      "id": "gpt-4o-2024-08-06",
      "name": "gpt-4o-2024-08-06",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-08-06"
    },
    {
      "id": "chatgpt-4o-latest",
      "name": "chatgpt-4o-latest",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-08-13"
    },
    {
      "id": "o1-mini-2024-09-12",
      "name": "o1-mini-2024-09-12",
      "provider": "openai",
      "family": "o1",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2024-09-12"
    },
    {
      "id": "o1-mini",
      "name": "o1-mini",
      "provider": "openai",
      "family": "o1",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2024-09-06"
    },
    {
      "id": "gpt-4o-realtime-preview-2024-10-01",
      "name": "gpt-4o-realtime-preview-2024-10-01",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-10-01"
    },
    {
      "id": "gpt-4o-audio-preview-2024-10-01",
      "name": "gpt-4o-audio-preview-2024-10-01",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-10-01"
    },
    {
      "id": "gpt-4o-audio-preview",
      "name": "gpt-4o-audio-preview",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-09-27"
    },
    {
      "id": "gpt-4o-realtime-preview",
      "name": "gpt-4o-realtime-preview",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-09-30"
    },
    {
      "id": "omni-moderation-latest",
      "name": "omni-moderation-latest",
      "provider": "openai",
      "family": "omni",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-11-15"
    },
    {
      "id": "omni-moderation-2024-09-26",
      "name": "omni-moderation-2024-09-26",
      "provider": "openai",
      "family": "omni",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-09-26"
    },
    {
      "id": "gpt-4o-realtime-preview-2024-12-17",
      "name": "gpt-4o-realtime-preview-2024-12-17",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-12-17"
    },
    {
      "id": "gpt-4o-audio-preview-2024-12-17",
      "name": "gpt-4o-audio-preview-2024-12-17",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-12-17"
    },
    {
      "id": "gpt-4o-mini-realtime-preview-2024-12-17",
      "name": "gpt-4o-mini-realtime-preview-2024-12-17",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-12-17"
    },
    {
      "id": "gpt-4o-mini-audio-preview-2024-12-17",
      "name": "gpt-4o-mini-audio-preview-2024-12-17",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-12-17"
    },
    {
      "id": "o1-2024-12-17",
      "name": "o1-2024-12-17",
      "provider": "openai",
      "family": "o1",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2024-12-17"
    },
    {
      "id": "o1",
      "name": "o1",
      "provider": "openai",
      "family": "o1",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2024-12-16"
    },
    {
      "id": "gpt-4o-mini-realtime-preview",
      "name": "gpt-4o-mini-realtime-preview",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-12-16"
    },
    {
      "id": "gpt-4o-mini-audio-preview",
      "name": "gpt-4o-mini-audio-preview",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-12-16"
    },
    {
      "id": "computer-use-preview",
      "name": "computer-use-preview",
      "provider": "openai",
      "family": "computer",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2024-12-20"
    },
    {
      "id": "o3-mini",
      "name": "o3-mini",
      "provider": "openai",
      "family": "o3",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2025-01-17"
    },
    {
      "id": "o3-mini-2025-01-31",
      "name": "o3-mini-2025-01-31",
      "provider": "openai",
      "family": "o3",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2025-01-31"
    },
    {
      "id": "gpt-4o-2024-11-20",
      "name": "gpt-4o-2024-11-20",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2024-11-20"
    },
    {
      "id": "computer-use-preview-2025-03-11",
      "name": "computer-use-preview-2025-03-11",
      "provider": "openai",
      "family": "computer",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-03-11"
    },
    {
      "id": "gpt-4o-search-preview-2025-03-11",
      "name": "gpt-4o-search-preview-2025-03-11",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "search": true,
        "json_output": true
      },
      "updated_at": "2025-03-11"
    },
    {
      "id": "gpt-4o-search-preview",
      "name": "gpt-4o-search-preview",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "search": true,
        "json_output": true
      },
      "updated_at": "2025-03-07"
    },
    {
      "id": "gpt-4o-mini-search-preview-2025-03-11",
      "name": "gpt-4o-mini-search-preview-2025-03-11",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "search": true,
        "json_output": true
      },
      "updated_at": "2025-03-11"
    },
    {
      "id": "gpt-4o-mini-search-preview",
      "name": "gpt-4o-mini-search-preview",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "search": true,
        "json_output": true
      },
      "updated_at": "2025-03-07"
    },
    {
      "id": "gpt-4o-transcribe",
      "name": "gpt-4o-transcribe",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2025-03-15"
    },
    {
      "id": "gpt-4o-mini-transcribe",
      "name": "gpt-4o-mini-transcribe",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2025-03-15"
    },
    {
      "id": "o1-pro-2025-03-19",
      "name": "o1-pro-2025-03-19",
      "provider": "openai",
      "family": "o1",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2025-03-19"
    },
    {
      "id": "o1-pro",
      "name": "o1-pro",
      "provider": "openai",
      "family": "o1",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2025-03-17"
    },
    {
      "id": "gpt-4o-mini-tts",
      "name": "gpt-4o-mini-tts",
      "provider": "openai",
      "family": "gpt-4o-mini",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2025-03-19"
    },
    {
      "id": "o3-2025-04-16",
      "name": "o3-2025-04-16",
      "provider": "openai",
      "family": "o3",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2025-04-16"
    },
    {
      "id": "o4-mini-2025-04-16",
      "name": "o4-mini-2025-04-16",
      "provider": "openai",
      "family": "o4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-04-16"
    },
    {
      "id": "o3",
      "name": "o3",
      "provider": "openai",
      "family": "o3",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2025-04-09"
    },
    {
      "id": "o4-mini",
      "name": "o4-mini",
      "provider": "openai",
      "family": "o4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-04-09"
    },
    {
      "id": "gpt-4.1-2025-04-14",
      "name": "gpt-4.1-2025-04-14",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-04-14"
    },
    {
      "id": "gpt-4.1",
      "name": "gpt-4.1",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-04-10"
    },
    {
      "id": "gpt-4.1-mini-2025-04-14",
      "name": "gpt-4.1-mini-2025-04-14",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-04-14"
    },
    {
      "id": "gpt-4.1-mini",
      "name": "gpt-4.1-mini",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-04-10"
    },
    {
      "id": "gpt-4.1-nano-2025-04-14",
      "name": "gpt-4.1-nano-2025-04-14",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-04-14"
    },
    {
      "id": "gpt-4.1-nano",
      "name": "gpt-4.1-nano",
      "provider": "openai",
      "family": "gpt-4",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-04-10"
    },
    {
      "id": "gpt-image-1",
      "name": "gpt-image-1",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-04-24"
    },
    {
      "id": "codex-mini-latest",
      "name": "codex-mini-latest",
      "provider": "openai",
      "family": "codex",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-05-08"
    },
    {
      "id": "o3-pro",
      "name": "o3-pro",
      "provider": "openai",
      "family": "o3",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2025-05-28"
    },
    {
      "id": "gpt-4o-realtime-preview-2025-06-03",
      "name": "gpt-4o-realtime-preview-2025-06-03",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2025-06-03"
    },
    {
      "id": "gpt-4o-audio-preview-2025-06-03",
      "name": "gpt-4o-audio-preview-2025-06-03",
      "provider": "openai",
      "family": "gpt-4o",
      "context_length": null,
      "capabilities": {
        "vision": true,
        "json_output": true
      },
      "updated_at": "2025-06-03"
    },
    {
      "id": "o3-pro-2025-06-10",
      "name": "o3-pro-2025-06-10",
      "provider": "openai",
      "family": "o3",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "json_output": true
      },
      "updated_at": "2025-06-10"
    },
    {
      "id": "o4-mini-deep-research",
      "name": "o4-mini-deep-research",
      "provider": "openai",
      "family": "o4",
      "context_length": null,
      "capabilities": {
        "search": true,
        "json_output": true
      },
      "updated_at": "2025-06-11"
    },
    {
      "id": "o3-deep-research",
      "name": "o3-deep-research",
      "provider": "openai",
      "family": "o3",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "search": true,
        "json_output": true
      },
      "updated_at": "2025-06-13"
    },
    {
      "id": "o3-deep-research-2025-06-26",
      "name": "o3-deep-research-2025-06-26",
      "provider": "openai",
      "family": "o3",
      "context_length": null,
      "capabilities": {
        "reasoning": true,
        "responses_api": true,
        "search": true,
        "json_output": true
      },
      "updated_at": "2025-06-26"
    },
    {
      "id": "o4-mini-deep-research-2025-06-26",
      "name": "o4-mini-deep-research-2025-06-26",
      "provider": "openai",
      "family": "o4",
      "context_length": null,
      "capabilities": {
        "search": true,
        "json_output": true
      },
      "updated_at": "2025-06-26"
    },
    {
      "id": "gpt-5-chat-latest",
      "name": "gpt-5-chat-latest",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-01"
    },
    {
      "id": "gpt-5-2025-08-07",
      "name": "gpt-5-2025-08-07",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-07"
    },
    {
      "id": "gpt-5",
      "name": "gpt-5",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-05"
    },
    {
      "id": "gpt-5-mini-2025-08-07",
      "name": "gpt-5-mini-2025-08-07",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-07"
    },
    {
      "id": "gpt-5-mini",
      "name": "gpt-5-mini",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-05"
    },
    {
      "id": "gpt-5-nano-2025-08-07",
      "name": "gpt-5-nano-2025-08-07",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2025-08-07"
    },
    {
      "id": "gpt-3.5-turbo-16k",
      "name": "gpt-3.5-turbo-16k",
      "provider": "openai",
      "family": "gpt",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-05-10"
    },
    {
      "id": "tts-1",
      "name": "tts-1",
      "provider": "openai",
      "family": "tts",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-04-19"
    },
    {
      "id": "whisper-1",
      "name": "whisper-1",
      "provider": "openai",
      "family": "whisper",
      "context_length": null,
      "capabilities": {
        "json_output": true
      },
      "updated_at": "2023-02-27"
    },
    {
      "id": "text-embedding-ada-002",
      "name": "text-embedding-ada-002",
      "provider": "openai",
      "family": "text",
      "context_length": null,
      "capabilities": {
        "embedding": true,
        "json_output": true
      },
      "updated_at": "2022-12-16"
    }
  ],
  "fetched_at": "2025-09-01T02:01:55.745157+00:00",
  "fetched_via": "api",
  "metadata": {
    "source": "openai_sdk_enriched"
  }
}]]></content>
    </file>
    <file>
      <path>providers/xai/get_xai_models.py</path>
      <content/>
    </file>
    <file>
      <path>providers/xai/xai-models.json</path>
      <content/>
    </file>
    <file>
      <path>settings/__init__.py</path>
      <content><![CDATA["""
Settings repository factory (composition root helper)

Presentation code should depend on SettingsRepository (interfaces.py).
This module provides a small factory that lazily instantiates the chosen
infrastructure backend (SQLite by default) and returns a singleton instance.

This keeps presentation depending on a stable package boundary while allowing
the infrastructure implementation to live behind this factory.
"""

from __future__ import annotations

from typing import Optional
from .interfaces import SettingsRepository  # re-exported contract

_repo_singleton: Optional[SettingsRepository] = None


def get_settings_repo(db_path: Optional[str] = None) -> SettingsRepository:
    global _repo_singleton
    if _repo_singleton is None:
        # Lazy import to avoid hard-coupling at import time
        from .sqlite_repository import SqliteSettingsRepository
        _repo_singleton = SqliteSettingsRepository(db_path=db_path)
    return _repo_singleton]]></content>
    </file>
    <file>
      <path>settings/interfaces.py</path>
      <content><![CDATA["""
Settings repository abstractions (Clean Architecture)

- Presentation (CLI) must depend only on these interfaces, not on concrete DBs.
- Infrastructure (sqlite, etc.) implements these contracts.
"""

from __future__ import annotations

from typing import Protocol, Optional, Dict


class SettingsRepository(Protocol):
    """
    Repository contract for persisting user settings, API keys, and preferences.
    """

    # API keys (by provider id, e.g., "openai", "deepseek", "ollama")
    def get_api_key(self, provider: str) -> Optional[str]:
        ...

    def set_api_key(self, provider: str, key: Optional[str]) -> None:
        """
        Persist/update API key for provider. Passing None deletes the key.
        """
        ...

    # Preferences (arbitrary simple string key/value pairs)
    def get_pref(self, key: str) -> Optional[str]:
        ...

    def set_pref(self, key: str, value: Optional[str]) -> None:
        """
        Persist/update preference. Passing None deletes the preference.
        """
        ...

    # Bulk helpers
    def all_api_keys(self) -> Dict[str, str]:
        ...

    def all_prefs(self) -> Dict[str, str]:
        ...]]></content>
    </file>
    <file>
      <path>settings/sqlite_repository.py</path>
      <content><![CDATA["""
SQLite Settings Repository (Infrastructure)

- Implements SettingsRepository interface to persist:
  * API keys per provider
  * Arbitrary user preferences (theme, colors, last provider, model per provider, etc.)

- File size limit: <500 LOC (kept small)
- No dependencies on presentation; pure infrastructure
"""

from __future__ import annotations

import os
import sqlite3
from pathlib import Path
from typing import Optional, Dict

from .interfaces import SettingsRepository


class SqliteSettingsRepository(SettingsRepository):
    """
    SQLite-backed implementation for settings persistence.

    Schema:
      - api_keys(provider TEXT PRIMARY KEY, key TEXT)
      - prefs(key TEXT PRIMARY KEY, value TEXT)
    """

    def __init__(self, db_path: Optional[str] = None) -> None:
        # Default DB location: project-local ./data/settings.db (creates directory)
        default_dir = Path(os.getenv("AGENT_TOOLS_DATA_DIR", ".")) / "data"
        default_dir.mkdir(parents=True, exist_ok=True)
        self.db_path = db_path or str(default_dir / "settings.db")
        self._conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self._conn.execute("PRAGMA journal_mode=WAL;")
        self._conn.execute("PRAGMA synchronous=NORMAL;")
        self._ensure_schema()

    def _ensure_schema(self) -> None:
        cur = self._conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS api_keys (
              provider TEXT PRIMARY KEY,
              key      TEXT
            )
            """
        )
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS prefs (
              key   TEXT PRIMARY KEY,
              value TEXT
            )
            """
        )
        self._conn.commit()

    # ------------- API Keys -------------

    def get_api_key(self, provider: str) -> Optional[str]:
        cur = self._conn.cursor()
        cur.execute("SELECT key FROM api_keys WHERE provider = ?", (provider.lower().strip(),))
        row = cur.fetchone()
        return row[0] if row else None

    def set_api_key(self, provider: str, key: Optional[str]) -> None:
        p = provider.lower().strip()
        cur = self._conn.cursor()
        if key is None or key == "":
            cur.execute("DELETE FROM api_keys WHERE provider = ?", (p,))
        else:
            cur.execute(
                "INSERT INTO api_keys(provider, key) VALUES(?, ?) "
                "ON CONFLICT(provider) DO UPDATE SET key = excluded.key",
                (p, key),
            )
        self._conn.commit()

    def all_api_keys(self) -> Dict[str, str]:
        cur = self._conn.cursor()
        cur.execute("SELECT provider, key FROM api_keys")
        return {row[0]: row[1] for row in cur.fetchall()}

    # ------------- Preferences -------------

    def get_pref(self, key: str) -> Optional[str]:
        cur = self._conn.cursor()
        cur.execute("SELECT value FROM prefs WHERE key = ?", (key,))
        row = cur.fetchone()
        return row[0] if row else None

    def set_pref(self, key: str, value: Optional[str]) -> None:
        cur = self._conn.cursor()
        if value is None:
            cur.execute("DELETE FROM prefs WHERE key = ?", (key,))
        else:
            cur.execute(
                "INSERT INTO prefs(key, value) VALUES(?, ?) "
                "ON CONFLICT(key) DO UPDATE SET value = excluded.value",
                (key, value),
            )
        self._conn.commit()

    def all_prefs(self) -> Dict[str, str]:
        cur = self._conn.cursor()
        cur.execute("SELECT key, value FROM prefs")
        return {row[0]: row[1] for row in cur.fetchall()}]]></content>
    </file>
    <file>
      <path>tools/__init__.py</path>
      <content><![CDATA["""
Tools package for Anthropic Claude tool use.
"""

from .tool_base import Tool, ToolResult
from .tool_manager import ToolManager
from .code_runner_tool import CodeRunnerTool ]]></content>
    </file>
    <file>
      <path>tools/advanced_file_tool.py</path>
      <content><![CDATA[# selfprompter/tools/advanced_file_tool.py

import os
import shutil
from typing import Dict, Any, Optional, List, Union
from .tool_base import Tool, ToolResult

class AdvancedFileTool(Tool):
    """
    Advanced file and directory operations tool.
    Follows Anthropic Claude tool use standards.
    """

    def __init__(self, repo_root: str = "./"):
        """
        repo_root is the top-level directory in which files can be manipulated.
        Default is current directory.
        """
        self.repo_root = os.path.abspath(repo_root)
        os.makedirs(self.repo_root, exist_ok=True)

    @property
    def name(self) -> str:
        return "advanced_file_operations"

    @property
    def description(self) -> str:
        return (
            "Advanced file and directory management tool that can read, write, edit files "
            "and manage directories. Supports partial file reads, in-place edits, directory "
            "creation/deletion, and file moving/renaming operations. All operations are "
            "restricted to a safe root directory."
        )

    @property
    def input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "operation": {
                    "type": "string",
                    "enum": ["read", "read_chunk", "write", "append", "edit", "mkdir", "rmdir", "delete_file", "move"],
                    "description": "The operation to perform"
                },
                "path": {
                    "type": "string",
                    "description": "Target file or directory path"
                },
                "content": {
                    "type": "string",
                    "description": "Content to write (for write/append/edit operations)"
                },
                "start_line": {
                    "type": "integer",
                    "description": "Starting line number for chunk read or edit operations"
                },
                "end_line": {
                    "type": "integer",
                    "description": "Ending line number for edit operations"
                },
                "num_lines": {
                    "type": "integer",
                    "description": "Number of lines to read for chunk operations"
                },
                "src": {
                    "type": "string",
                    "description": "Source path for move operations"
                },
                "dest": {
                    "type": "string",
                    "description": "Destination path for move operations"
                }
            },
            "required": ["operation"],
            "additionalProperties": False
        }

    def _safe_path(self, user_path: str) -> str:
        """Ensure path is within repo_root directory."""
        normalized = os.path.normpath(user_path)
        full_path = os.path.join(self.repo_root, normalized)
        if not full_path.startswith(self.repo_root):
            raise ValueError("Path is outside the repository root")
        return full_path

    def run(self, input: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the requested file or directory operation.
        
        Args:
            input: Dictionary containing operation-specific input_schema:
                operation: The operation to perform
                path: Target file or directory path
                content: Content to write (for write/append/edit operations)
                start_line: Starting line number for chunk read or edit operations
                end_line: Ending line number for edit operations
                num_lines: Number of lines to read for chunk operations
                src: Source path for move operations
                dest: Destination path for move operations
            
        Returns:
            Dictionary containing operation result or error message
        """
        try:
            operation = input.get("operation", "").lower()
            path = input.get("path", "")
            content = input.get("content", "")
            start_line = input.get("start_line")
            end_line = input.get("end_line")
            num_lines = input.get("num_lines")
            src = input.get("src")
            dest = input.get("dest")

            if operation in ["read", "read_chunk", "write", "append", "edit"]:
                result = self._handle_file_operations(
                    operation=operation,
                    path=path,
                    content=content,
                    start_line=start_line,
                    end_line=end_line,
                    num_lines=num_lines
                )
            elif operation == "mkdir":
                result = self._mkdir(path)
            elif operation == "rmdir":
                result = self._rmdir(path)
            elif operation == "delete_file":
                result = self._delete_file(path)
            elif operation == "move":
                if not src or not dest:
                    return {
                        "type": "tool_response",
                        "content": "Error: 'move' operation requires both 'src' and 'dest' input_schema"
                    }
                result = self._move(src, dest)
            else:
                return {
                    "type": "tool_response",
                    "content": f"Error: Unknown operation '{operation}'"
                }

            return {
                "type": "tool_response",
                "content": result
            }

        except Exception as e:
            return {
                "type": "tool_response",
                "content": f"Error: {str(e)}"
            }

    def _handle_file_operations(
        self,
        operation: str,
        path: str,
        content: str = "",
        start_line: Optional[int] = None,
        end_line: Optional[int] = None,
        num_lines: Optional[int] = None
    ) -> str:
        filepath = self._safe_path(path)

        if operation == "read":
            return self._read_file(filepath)
        elif operation == "read_chunk":
            if not start_line:
                raise ValueError("'read_chunk' requires 'start_line'")
            return self._read_chunk(filepath, start_line, num_lines or 50)
        elif operation == "write":
            return self._write_file(filepath, content)
        elif operation == "append":
            return self._append_file(filepath, content)
        elif operation == "edit":
            if not start_line:
                raise ValueError("'edit' requires 'start_line'")
            return self._edit_file(filepath, start_line, end_line or start_line, content)
        else:
            raise ValueError(f"Unexpected operation '{operation}'")

    def _read_file(self, filepath: str) -> str:
        if not os.path.isfile(filepath):
            raise FileNotFoundError(f"File '{filepath}' not found")
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()

    def _read_chunk(self, filepath: str, start_line: int, num_lines: int) -> str:
        if not os.path.isfile(filepath):
            raise FileNotFoundError(f"File '{filepath}' not found")
        
        lines_out = []
        with open(filepath, "r", encoding="utf-8") as f:
            all_lines = f.readlines()

        actual_start = max(0, start_line - 1)
        actual_end = min(len(all_lines), actual_start + num_lines)
        chunk = all_lines[actual_start:actual_end]
        
        for i, line in enumerate(chunk, start=actual_start+1):
            lines_out.append(f"{i}: {line.rstrip()}")
        return "\n".join(lines_out)

    def _write_file(self, filepath: str, content: str) -> str:
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        return f"Successfully wrote to '{filepath}'"

    def _append_file(self, filepath: str, content: str) -> str:
        with open(filepath, "a", encoding="utf-8") as f:
            if content:
                f.write("\n" + content)
        return f"Successfully appended to '{filepath}'"

    def _edit_file(self, filepath: str, start_line: int, end_line: int, new_content: str) -> str:
        if not os.path.isfile(filepath):
            raise FileNotFoundError(f"File '{filepath}' not found")

        with open(filepath, "r", encoding="utf-8") as f:
            original = f.readlines()

        start_idx = max(0, start_line - 1)
        end_idx = min(len(original), end_line)
        
        edited = (
            original[:start_idx] +
            [line + "\n" for line in new_content.split("\n")] +
            original[end_idx:]
        )
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.writelines(edited)

        return f"Successfully edited lines {start_line} through {end_line} in '{filepath}'"

    def _mkdir(self, path: str) -> str:
        dirpath = self._safe_path(path)
        os.makedirs(dirpath, exist_ok=True)
        return f"Successfully created directory '{dirpath}'"

    def _rmdir(self, path: str) -> str:
        dirpath = self._safe_path(path)
        if not os.path.isdir(dirpath):
            raise NotADirectoryError(f"'{dirpath}' is not a directory or does not exist")
        shutil.rmtree(dirpath)
        return f"Successfully removed directory '{dirpath}'"

    def _delete_file(self, path: str) -> str:
        filepath = self._safe_path(path)
        if not os.path.isfile(filepath):
            raise FileNotFoundError(f"File '{filepath}' not found")
        os.remove(filepath)
        return f"Successfully deleted '{filepath}'"

    def _move(self, src: str, dest: str) -> str:
        src_path = self._safe_path(src)
        dest_path = self._safe_path(dest)
        
        if not os.path.exists(src_path):
            raise FileNotFoundError(f"Source path '{src_path}' does not exist")
            
        shutil.move(src_path, dest_path)
        return f"Successfully moved '{src_path}' to '{dest_path}'"]]></content>
    </file>
    <file>
      <path>tools/code_runner_tool.py</path>
      <content><![CDATA[# selfprompter/tools/code_runner_tool.py

import subprocess
import os
import json
import shutil
import tempfile
import signal
import sys
import psutil
from typing import Dict, Any, Optional, List, Literal, Union
from pathlib import Path
from src.tools.tool_base import Tool

def kill_proc_tree(pid, including_parent=True):
    """Kill a process tree (including grandchildren) with signal.SIGTERM"""
    try:
        parent = psutil.Process(pid)
        children = parent.children(recursive=True)
        for child in children:
            try:
                child.terminate()
            except psutil.NoSuchProcess:
                pass
        if including_parent:
            try:
                parent.terminate()
            except psutil.NoSuchProcess:
                pass
        _, alive = psutil.wait_procs(children + ([parent] if including_parent else []), timeout=1)
        for p in alive:
            try:
                p.kill()
            except psutil.NoSuchProcess:
                pass
    except psutil.NoSuchProcess:
        pass

def run_with_timeout(cmd: str, cwd: str, timeout: int, env: Dict[str, str] = None) -> subprocess.CompletedProcess:
    """Run a command with timeout and proper cleanup on Windows"""
    process = None
    try:
        # Merge environment variables with system environment
        merged_env = os.environ.copy()
        if env:
            merged_env.update({k: str(v) for k, v in env.items() if v is not None})

        if os.name == 'nt':
            # On Windows, use absolute path for python
            python_exe = sys.executable
            if cmd.startswith('python '):
                cmd = f'"{python_exe}" {cmd[7:]}'
            
            # Create startupinfo to properly configure Windows process
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            startupinfo.wShowWindow = subprocess.SW_HIDE
            
            process = subprocess.Popen(
                cmd,
                cwd=cwd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                env=merged_env,
                shell=True,
                startupinfo=startupinfo
            )
        else:
            process = subprocess.Popen(
                cmd,
                cwd=cwd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                env=merged_env,
                shell=True,
                preexec_fn=os.setsid
            )

        stdout, stderr = process.communicate(timeout=timeout)
        if process.returncode != 0:
            return subprocess.CompletedProcess(
                args=cmd,
                returncode=process.returncode,
                stdout="",
                stderr=f"Error: Execution failed with code {process.returncode}\n{stderr}"
            )
        return subprocess.CompletedProcess(
            args=cmd,
            returncode=process.returncode,
            stdout=stdout,
            stderr=stderr
        )
    except subprocess.TimeoutExpired:
        if process:
            kill_proc_tree(process.pid)
            process.kill()
        raise
    except Exception as e:
        if process:
            kill_proc_tree(process.pid)
            process.kill()
        return subprocess.CompletedProcess(
            args=cmd,
            returncode=1,
            stdout="",
            stderr=f"Error: Execution failed with code 1: {str(e)}"
        )

class CodeRunnerTool(Tool):
    """
    Advanced tool for executing complex code projects in various languages.
    Supports multi-file projects, dependencies, and build steps.
    Follows Anthropic Claude tool use standards.
    """

    name: Literal["code_runner"] = "code_runner"

    def __init__(self, working_dir: str = "./", timeout: int = 30):
        """
        Initialize with working directory and default timeout.
        
        Args:
            working_dir: Base directory for code execution
            timeout: Default timeout in seconds
        """
        self.working_dir = os.path.abspath(working_dir)
        self.timeout = min(timeout, 3600)  # Max 1 hour timeout
        self.language_configs = {
            "python": {
                "file_ext": ".py",
                "run_cmd": "python",
                "install_cmd": "pip install -r requirements.txt",
                "package_file": "requirements.txt"
            },
            "typescript": {
                "file_ext": ".ts",
                "run_cmd": "npx ts-node",
                "install_cmd": "npm install",
                "package_file": "package.json"
            },
            "go": {
                "file_ext": ".go", 
                "run_cmd": "go run",
                "install_cmd": "go mod download",
                "package_file": "go.mod"
            },
            "rust": {
                "file_ext": ".rs",
                "run_cmd": "cargo run",
                "install_cmd": "cargo build",
                "package_file": "Cargo.toml"
            }
        }

    @property
    def description(self) -> str:
        return """Executes code files in various programming languages (Python, TypeScript, Go, Rust).
        Supports multi-file projects, package dependencies, and build steps.
        Required input_schema:
        - files: List of files with paths and contents
        - language: Programming language (python, typescript, go, rust) 
        - main_file: Path to the main file to execute
        Optional input_schema:
        - args: Command line arguments
        - env: Environment variables
        - timeout: Maximum execution time in seconds
        - build_args: Additional build arguments"""

    @property
    def input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "files": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "path": {"type": "string"},
                            "content": {"type": "string"}
                        },
                        "required": ["path", "content"]
                    }
                },
                "language": {"type": "string"},
                "main_file": {"type": "string"},
                "args": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "env": {
                    "type": "object",
                    "additionalProperties": {"type": "string"}
                },
                "timeout": {"type": "integer"},
                "build_args": {"type": "string"}
            },
            "required": ["files", "language", "main_file"]
        }

    def run(self, input: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute a complex code project.
        
        Args:
            input: Dictionary containing:
                files: List of files to create (path and content)
                language: Programming language
                main_file: Entry point file
                args: Command line arguments
                env: Environment variables
                timeout: Execution timeout
            
        Returns:
            Dictionary containing execution output or error message
        """
        try:
            files = input.get("files", [])
            language = input.get("language", "").lower()
            main_file = input.get("main_file", "")
            args = input.get("args", [])
            env_vars = input.get("env", {})
            timeout = min(input.get("timeout", self.timeout), 3600)
            build_args = input.get("build_args", "")

            # Validate language
            if language not in self.language_configs:
                return self._error(f"Unsupported language: {language}")

            config = self.language_configs[language]
            
            # Validate file extension
            if not main_file.endswith(config["file_ext"]):
                return self._error(f"File extension not valid for {language}")

            # Create temporary project directory
            with tempfile.TemporaryDirectory() as temp_dir:
                try:
                    # Create all files
                    for file_info in files:
                        file_path = os.path.join(temp_dir, file_info["path"])
                        os.makedirs(os.path.dirname(file_path), exist_ok=True)
                        with open(file_path, "w") as f:
                            f.write(file_info["content"])

                    # Install dependencies if package file exists
                    package_file = os.path.join(temp_dir, config["package_file"])
                    if os.path.exists(package_file):
                        try:
                            print(f"Installing dependencies with command: {config['install_cmd']}")
                            install_result = run_with_timeout(
                                config["install_cmd"],
                                temp_dir,
                                timeout,
                                env_vars
                            )
                            print(f"Install result: {install_result.stdout}\n{install_result.stderr}")
                            if install_result.returncode != 0:
                                return self._error(f"Dependency installation failed: {install_result.stderr}")
                        except subprocess.TimeoutExpired:
                            return self._error(f"Dependency installation timed out after {timeout} seconds")

                    # Run build step if needed
                    if "build_cmd" in config:
                        build_cmd = f"{config['build_cmd']} {build_args}".strip()
                        try:
                            print(f"Running build command: {build_cmd}")
                            build_result = run_with_timeout(
                                build_cmd,
                                temp_dir,
                                timeout,
                                env_vars
                            )
                            print(f"Build result: {build_result.stdout}\n{build_result.stderr}")
                            if build_result.returncode != 0:
                                return self._error(f"Build failed: {build_result.stderr}")
                        except subprocess.TimeoutExpired:
                            return self._error(f"Build step timed out after {timeout} seconds")

                    # Execute the main file
                    cmd = f"{config['run_cmd']} {main_file}"
                    if args:
                        cmd += f" {' '.join(args)}"

                    env = os.environ.copy()
                    env.update(env_vars)

                    try:
                        print(f"Running command: {cmd}")
                        result = run_with_timeout(
                            cmd,
                            temp_dir,
                            timeout,
                            env
                        )
                        print(f"Execution result: {result.stdout}\n{result.stderr}")
                        if result.returncode != 0:
                            return self._error(f"Execution failed: {result.stderr}")

                        output = result.stdout.strip() or "Execution completed successfully (no output)"
                        return self._success(output)

                    except subprocess.TimeoutExpired:
                        return self._error(f"Execution timed out after {timeout} seconds")

                except Exception as e:
                    return self._error(f"Error: {str(e)}")

        except Exception as e:
            return self._error(str(e))

    def _success(self, content: str) -> Dict[str, Any]:
        return {
            "type": "tool_response",
            "tool_use_id": "",
            "content": content
        }
        
    def _error(self, message: str) -> Dict[str, Any]:
        return {
            "type": "tool_response",
            "tool_use_id": "",
            "content": f"Error: {message}"
        }
]]></content>
    </file>
    <file>
      <path>tools/computer_tool.py</path>
      <content><![CDATA["""
Windows-specific implementation of ComputerTool for screen, keyboard, and mouse interaction.
Uses pyautogui for cross-platform control and win32api for Windows-specific functionality.
"""

import base64
import io
import os
import time
from enum import Enum
from typing import Dict, Any, Literal, Optional, Tuple, Union

import pyautogui
from PIL import Image
# Cross-platform import safety for Windows-only pywin32 APIs.
# Provide lightweight shims on non-Windows so module import and tests can run.
import sys as _sys, types as _types
try:
    import win32api, win32con, win32gui  # type: ignore
except Exception:
    win32api = _types.SimpleNamespace(GetCursorPos=lambda: (0, 0))  # type: ignore
    win32con = _types.SimpleNamespace(SW_NORMAL=1, SW_RESTORE=9)    # type: ignore
    win32gui = _types.SimpleNamespace(                              # type: ignore
        EnumWindows=lambda *a, **k: None,
        GetWindowRect=lambda *a, **k: (0, 0, 100, 100),
        GetWindowText=lambda *a, **k: "",
        SetWindowPlacement=lambda *a, **k: None,
        MoveWindow=lambda *a, **k: None,
        SetForegroundWindow=lambda *a, **k: None,
        FindWindow=lambda *a, **k: 0,
        IsWindowVisible=lambda *a, **k: False,
        GetClassName=lambda *a, **k: "",
        IsIconic=lambda *a, **k: False,
        ShowWindow=lambda *a, **k: None,
        GetForegroundWindow=lambda *a, **k: 0,
    )
    _sys.modules.setdefault('win32api', win32api)
    _sys.modules.setdefault('win32con', win32con)
    _sys.modules.setdefault('win32gui', win32gui)

from .tool_base import Tool

# Configure pyautogui
pyautogui.FAILSAFE = True  # Move mouse to corner to abort
pyautogui.PAUSE = 0.5  # Increase delay between actions for stability

class Action(str, Enum):
    KEY = "key"
    TYPE = "type"
    MOUSE_MOVE = "mouse_move"
    LEFT_CLICK = "left_click"
    RIGHT_CLICK = "right_click"
    MIDDLE_CLICK = "middle_click"
    DOUBLE_CLICK = "double_click"
    SCREENSHOT = "screenshot"
    CURSOR_POSITION = "cursor_position"
    LEFT_CLICK_DRAG = "left_click_drag"
    FIND_WINDOW = "find_window"
    MOVE_WINDOW = "move_window"
    SET_WINDOW_FOCUS = "set_window_focus"
    GET_WINDOW_INFO = "get_window_info"

class ComputerTool(Tool):
    """Tool for interacting with the computer's screen, keyboard, and mouse."""
    
    name = "computer"
    description = """A tool for controlling the computer's mouse, keyboard, and windows.
    
    Available actions:
    1. Mouse Movement:
       - action: "mouse_move"
       - coordinate: [x, y] (required)
       Example: {"action": "mouse_move", "coordinate": [500, 500]}
    
    2. Mouse Clicks:
       - action: "left_click", "right_click", "middle_click", or "double_click"
       - coordinate: [x, y] (optional)
       Example: {"action": "left_click", "coordinate": [500, 500]}
    
    3. Keyboard Input:
       - action: "type" for text, "key" for key combinations
       - text: string (required)
       Examples: 
       - {"action": "type", "text": "Hello World"}
       - {"action": "key", "text": "ctrl+c"}
    
    4. Screen Capture:
       - action: "screenshot"
       Example: {"action": "screenshot"}
    
    5. Cursor Position:
       - action: "cursor_position"
       Example: {"action": "cursor_position"}

    6. Window Control:
       - action: "find_window"
       - title: window title to find (required)
       Example: {"action": "find_window", "title": "Notepad"}

       - action: "move_window"
       - window_title: title of window to move (required)
       - position: [x, y] (required)
       - size: [width, height] (optional)
       Example: {"action": "move_window", "window_title": "Notepad", "position": [0, 0], "size": [800, 600]}

       - action: "set_window_focus"
       - window_title: title of window to focus (required)
       Example: {"action": "set_window_focus", "window_title": "Notepad"}

       - action: "get_window_info"
       - window_title: title of window to get info for (required)
       Example: {"action": "get_window_info", "window_title": "Notepad"}"""
    
    @property
    def input_schema(self) -> Dict[str, Any]:
        """Get the input schema for the tool."""
        return {
            "type": "object",
            "properties": {
                "action": {
                    "type": "string",
                    "enum": [e.value for e in Action],
                    "description": "The action to perform"
                },
                "text": {
                    "type": "string",
                    "description": "Text to type or key command to send"
                },
                "coordinate": {
                    "type": "array",
                    "items": {"type": "integer"},
                    "minItems": 2,
                    "maxItems": 2,
                    "description": "Screen coordinates [x, y]"
                },
                "window_title": {
                    "type": "string",
                    "description": "Title of window to control"
                },
                "position": {
                    "type": "array",
                    "items": {"type": "integer"},
                    "minItems": 2,
                    "maxItems": 2,
                    "description": "Window position [x, y]"
                },
                "size": {
                    "type": "array",
                    "items": {"type": "integer"},
                    "minItems": 2,
                    "maxItems": 2,
                    "description": "Window size [width, height]"
                }
            },
            "required": ["action"]
        }

    def __init__(self):
        """Initialize the tool with screen dimensions."""
        super().__init__()
        self.screen_width, self.screen_height = pyautogui.size()

    def _validate_coordinates(self, x: int, y: int) -> Tuple[int, int]:
        """Validate and adjust coordinates to be within screen bounds."""
        x = max(0, min(x, self.screen_width - 1))
        y = max(0, min(y, self.screen_height - 1))
        return x, y

    def _take_screenshot(self) -> str:
        """Take a screenshot and return as base64 string."""
        screenshot = pyautogui.screenshot()
        buffered = io.BytesIO()
        screenshot.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()

    def _get_active_window_rect(self) -> Tuple[int, int, int, int]:
        """Get the active window rectangle (left, top, right, bottom)."""
        try:
            hwnd = win32gui.GetForegroundWindow()
            rect = win32gui.GetWindowRect(hwnd)
            return rect
        except Exception:
            # Unknown or unavailable window info
            return (0, 0, -1, -1)

    def _is_safe_coordinate(self, x: int, y: int) -> bool:
        """
        Determine if the coordinate is safe (i.e., not overlapping the active window region).
        Tests expect that if a point lies within a window area, we redirect to a safe position.
        """
        try:
            left, top, right, bottom = self._get_active_window_rect()
            # If invalid rect, consider safe
            if right <= left or bottom <= top:
                return True
            # Unsafe if within active window bounds
            return not (left <= x <= right and top <= y <= bottom)
        except Exception:
            return True

    def _get_safe_position(self) -> Tuple[int, int]:
        """
        Return a safe default position away from windowed regions and edges.
        Tests expect (screen_width - 100, screen_height - 100) as a safe landing zone.
        """
        safe_x = max(0, min(self.screen_width - 1, self.screen_width - 100))
        safe_y = max(0, min(self.screen_height - 1, self.screen_height - 100))
        return safe_x, safe_y

    def find_and_move_window(self, title_substring: str) -> Optional[int]:
        """Find a window by title substring and move it to the primary monitor."""
        def callback(hwnd, windows):
            if win32gui.IsWindowVisible(hwnd):
                title = win32gui.GetWindowText(hwnd)
                if title_substring.lower() in title.lower():
                    windows.append(hwnd)
            return True
        
        windows = []
        win32gui.EnumWindows(callback, windows)
        
        if not windows:
            return None
            
        hwnd = windows[0]
        try:
            # Get current window placement
            placement = win32gui.GetWindowPlacement(hwnd)
            rect = win32gui.GetWindowRect(hwnd)
            
            # Calculate window size
            width = rect[2] - rect[0]
            height = rect[3] - rect[1]
            
            # Move to primary monitor (0,0 is top-left)
            new_x = 100
            new_y = 100
            
            # Set window to normal state (not minimized/maximized)
            placement = list(placement)
            placement[1] = win32con.SW_NORMAL
            win32gui.SetWindowPlacement(hwnd, tuple(placement))
            
            # Move window
            win32gui.MoveWindow(hwnd, new_x, new_y, width, height, True)
            
            # Bring to front and focus
            win32gui.SetForegroundWindow(hwnd)
            
            return hwnd
        except Exception as e:
            print(f"Error moving window: {e}")
            return None

    def find_window_by_title(self, title: str) -> Optional[int]:
        """Find a window by its title."""
        try:
            hwnd = win32gui.FindWindow(None, title)
            if hwnd and win32gui.IsWindowVisible(hwnd):
                return hwnd
            # Try partial match
            def callback(hwnd, windows):
                if win32gui.IsWindowVisible(hwnd):
                    window_title = win32gui.GetWindowText(hwnd)
                    if title.lower() in window_title.lower():
                        windows.append(hwnd)
            windows = []
            win32gui.EnumWindows(callback, windows)
            return windows[0] if windows else None
        except Exception as e:
            print(f"Error finding window: {e}")
            return None

    def run(self, input: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the requested computer interaction action."""
        tool_use_id = input.get("tool_use_id", "")
        
        try:
            action = Action(input["action"])
            text = input.get("text")
            coordinate = input.get("coordinate")
            window_title = input.get("window_title")
            position = input.get("position")
            size = input.get("size")

            # Window management actions
            if action == Action.FIND_WINDOW:
                if not window_title:
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "error": "window_title is required for find_window action",
                            "action": action.value
                        }
                    }
                hwnd = self.find_window_by_title(window_title)
                if hwnd:
                    rect = win32gui.GetWindowRect(hwnd)
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "status": "success",
                            "action": action.value,
                            "window_handle": hwnd,
                            "position": [rect[0], rect[1]],
                            "size": [rect[2] - rect[0], rect[3] - rect[1]],
                            "title": win32gui.GetWindowText(hwnd),
                            "class": win32gui.GetClassName(hwnd)
                        }
                    }
                return {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": {
                        "error": f"Window with title '{window_title}' not found",
                        "action": action.value
                    }
                }

            elif action == Action.MOVE_WINDOW:
                if not window_title or not position:
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "error": "window_title and position are required for move_window action",
                            "action": action.value
                        }
                    }
                hwnd = self.find_window_by_title(window_title)
                if hwnd:
                    if not size:
                        rect = win32gui.GetWindowRect(hwnd)
                        size = [rect[2] - rect[0], rect[3] - rect[1]]
                    win32gui.MoveWindow(hwnd, position[0], position[1], size[0], size[1], True)
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "status": "success",
                            "action": action.value,
                            "message": f"Moved window to ({position[0]}, {position[1]}) with size ({size[0]}, {size[1]})"
                        }
                    }
                return {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": {
                        "error": f"Window with title '{window_title}' not found",
                        "action": action.value
                    }
                }

            elif action == Action.SET_WINDOW_FOCUS:
                if not window_title:
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "error": "window_title is required for set_window_focus action",
                            "action": action.value
                        }
                    }
                hwnd = self.find_window_by_title(window_title)
                if hwnd:
                    # Ensure window is not minimized
                    if win32gui.IsIconic(hwnd):
                        win32gui.ShowWindow(hwnd, win32con.SW_RESTORE)
                    win32gui.SetForegroundWindow(hwnd)
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "status": "success",
                            "action": action.value,
                            "message": f"Set focus to window '{window_title}'"
                        }
                    }
                return {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": {
                        "error": f"Window with title '{window_title}' not found",
                        "action": action.value
                    }
                }

            elif action == Action.GET_WINDOW_INFO:
                if not window_title:
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "error": "window_title is required for get_window_info action",
                            "action": action.value
                        }
                    }
                hwnd = self.find_window_by_title(window_title)
                if hwnd:
                    rect = win32gui.GetWindowRect(hwnd)
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "status": "success",
                            "action": action.value,
                            "window_handle": hwnd,
                            "position": [rect[0], rect[1]],
                            "size": [rect[2] - rect[0], rect[3] - rect[1]],
                            "title": win32gui.GetWindowText(hwnd),
                            "class": win32gui.GetClassName(hwnd),
                            "visible": win32gui.IsWindowVisible(hwnd),
                            "enabled": win32gui.IsWindowEnabled(hwnd),
                            "minimized": win32gui.IsIconic(hwnd),
                            "foreground": hwnd == win32gui.GetForegroundWindow()
                        }
                    }
                return {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": {
                        "error": f"Window with title '{window_title}' not found",
                        "action": action.value
                    }
                }

            # Existing actions
            if action in [Action.MOUSE_MOVE, Action.LEFT_CLICK_DRAG]:
                if not coordinate:
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "error": "coordinate is required for mouse movement",
                            "action": action.value
                        }
                    }
                x, y = self._validate_coordinates(coordinate[0], coordinate[1])

                # Safety: avoid moving into active window region; redirect to safe spot
                if not self._is_safe_coordinate(x, y):
                    x, y = self._get_safe_position()
                
                if action == Action.MOUSE_MOVE:
                    pyautogui.moveTo(x, y, duration=0.5)  # Slower movement
                else:  # LEFT_CLICK_DRAG
                    pyautogui.dragTo(x, y, button='left', duration=0.5)
                
                return {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": {
                        "status": "success",
                        "action": action.value,
                        "message": f"Mouse {'moved' if action == Action.MOUSE_MOVE else 'dragged'} to ({x}, {y})"
                    }
                }

            elif action in [Action.KEY, Action.TYPE]:
                if not text:
                    return {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": {
                            "error": "text is required for keyboard actions",
                            "action": action.value
                        }
                    }
                
                # Store current window focus
                active_window = win32gui.GetForegroundWindow()
                
                if action == Action.KEY:
                    pyautogui.hotkey(*text.split('+'))
                else:  # TYPE
                    pyautogui.write(text, interval=0.1)  # Slower typing
                
                # Restore window focus
                if active_window:
                    win32gui.SetForegroundWindow(active_window)
                
                return {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": {
                        "status": "success",
                        "action": action.value,
                        "message": f"{'Keys pressed' if action == Action.KEY else 'Text typed'}: {text}"
                    }
                }

            elif action == Action.SCREENSHOT:
                return {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": {
                        "status": "success",
                        "action": action.value,
                        "type": "image",
                        "format": "base64",
                        "data": self._take_screenshot()
                    }
                }

            elif action == Action.CURSOR_POSITION:
                x, y = win32api.GetCursorPos()
                return {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": {
                        "status": "success",
                        "action": action.value,
                        "position": {"x": x, "y": y},
                        "message": f"Cursor position: ({x}, {y})"
                    }
                }

            else:  # Click actions
                if coordinate:
                    x, y = self._validate_coordinates(coordinate[0], coordinate[1])
                    pyautogui.moveTo(x, y, duration=0.5)

                click_params = {
                    Action.LEFT_CLICK: {"button": "left"},
                    Action.RIGHT_CLICK: {"button": "right"},
                    Action.MIDDLE_CLICK: {"button": "middle"},
                    Action.DOUBLE_CLICK: {"button": "left", "clicks": 2, "interval": 0.25}
                }[action]

                pyautogui.click(**click_params)
                return {
                    "type": "tool_result",
                    "tool_use_id": tool_use_id,
                    "content": {
                        "status": "success",
                        "action": action.value,
                        "message": f"Mouse {action.value} performed"
                    }
                }

        except Exception as e:
            return {
                "type": "tool_result",
                "tool_use_id": tool_use_id,  # Now tool_use_id is always defined
                "content": {
                    "error": str(e),
                    "action": input.get("action"),
                    "traceback": str(e.__traceback__)
                }
            } ]]></content>
    </file>
    <file>
      <path>tools/config.py</path>
      <content><![CDATA["""
Configuration module for loading environment variables and settings.

This module handles:
1. Loading API keys from .env file
2. Setting default configurations
3. Validating required settings (lax; does not force Deepseek key for local/Ollama)
"""

import os
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class Config:
    """Configuration manager for tool settings and API keys."""
    
    # OpenAI-compatible endpoints (support DeepSeek, Ollama, custom)
    OPENAI_API_KEY: str = os.getenv('OPENAI_API_KEY', '')
    OPENAI_BASE_URL: str = os.getenv('OPENAI_BASE_URL', '')
    OPENAI_MODEL: str = os.getenv('OPENAI_MODEL', '')
    
    # DeepSeek (OpenAI-compatible)
    DEEPSEEK_API_KEY: str = os.getenv('DEEPSEEK_API_KEY', '')
    DEEPSEEK_BASE_URL: str = os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')
    DEEPSEEK_MODEL: str = os.getenv('DEEPSEEK_MODEL', 'deepseek-reasoner')
    
    # Ollama (OpenAI-compatible shim at /v1)
    OLLAMA_API_KEY: str = os.getenv('OLLAMA_API_KEY', 'ollama')  # placeholder, often unused
    OLLAMA_BASE_URL: str = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434/v1')
    OLLAMA_MODEL: str = os.getenv('OLLAMA_MODEL', 'llama3.1')
    
    # Optional API Keys
    ANTHROPIC_API_KEY: str = os.getenv('ANTHROPIC_API_KEY', '')
    GOOGLE_SEARCH_API_KEY: str = os.getenv('GOOGLE_SEARCH_API_KEY', '')
    GOOGLE_SEARCH_ENGINE_ID: str = os.getenv('GOOGLE_SEARCH_ENGINE_ID', '')
    BING_SEARCH_API_KEY: str = os.getenv('BING_SEARCH_API_KEY', '')
    SERP_API_KEY: str = os.getenv('SERP_API_KEY', '')
    
    # Optional configurations with defaults
    WEB_BROWSER_TIMEOUT: int = int(os.getenv('WEB_BROWSER_TIMEOUT', '30'))
    DEFAULT_SEARCH_RESULTS: int = int(os.getenv('DEFAULT_SEARCH_RESULTS', '10'))
    
    # Package manager settings
    PACKAGE_MANAGER_CONFIG = {
        "use_module_pip": True,  # Default to python -m pip for better venv support
        "pip_command": None,     # Custom pip command if needed
    }
    
    # Validation flags
    REQUIRE_DEEPSEEK_KEY: bool = os.getenv('REQUIRE_DEEPSEEK_KEY', 'false').lower() in ('1', 'true', 'yes')

    def __init__(self):
        """Initialize configuration without forcing Deepseek API key."""
        # No hard validation here to allow local/Ollama-first usage
        pass
    
    @classmethod
    def validate_api_keys(cls) -> None:
        """
        Optionally validate that required API keys are set.
        By default, we do NOT require Deepseek key to allow local/Ollama usage.
        """
        if cls.REQUIRE_DEEPSEEK_KEY and not cls.DEEPSEEK_API_KEY:
            raise ValueError(
                "Missing required API key: DEEPSEEK_API_KEY. "
                "Set REQUIRE_DEEPSEEK_KEY=true to enforce; otherwise it's optional."
            )
    
    @classmethod
    def get_api_key(cls, key_name: str) -> Optional[str]:
        """
        Get an API key by name.
        
        Args:
            key_name: Name of the API key to retrieve
            
        Returns:
            The API key value or None if not found
        """
        return getattr(cls, key_name, None)

# Lax validation on import (no exception by default)
Config.validate_api_keys()]]></content>
    </file>
    <file>
      <path>tools/deepseek_wrapper.py</path>
      <content><![CDATA["""
Compatibility shim to preserve legacy import path.

Do NOT implement wrapper logic here. This file simply re-exports the
DeepseekToolWrapper from the canonical wrappers package.

Original imports in tests:
    from src.tools.deepseek_wrapper import DeepseekToolWrapper
"""

from ..wrappers.deepseek_wrapper import DeepseekToolWrapper  # noqa: F401]]></content>
    </file>
    <file>
      <path>tools/file_tool.py</path>
      <content><![CDATA[# selfprompter/tools/file_tool.py

from collections import defaultdict
from pathlib import Path
from typing import Dict, Any, Optional, List, Literal, get_args
from .tool_base import Tool
import os
import shutil

Command = Literal[
    "view",
    "create", 
    "str_replace",
    "insert",
    "undo_edit",
]

SNIPPET_LINES: int = 4

class FileTool(Tool):
    """
    A powerful file system tool that supports both precise edits and bulk operations.
    """

    api_type: Literal["text_editor_20241022"] = "text_editor_20241022"
    name: Literal["file"] = "file"

    def __init__(self):
        self._file_history = defaultdict(list)
        super().__init__()

    @property
    def description(self) -> str:
        return (
            "A powerful file system tool that supports reading, writing, and editing files.\n"
            "Operations:\n"
            "- write: Create or overwrite a file (requires path and content)\n"
            "- read: Read entire file content (requires path)\n" 
            "- read_lines: Read specific lines (requires path, start_line, end_line)\n"
            "- edit_lines: Edit specific lines (requires path, start_line, end_line, content)\n"
            "- delete: Delete a file or directory (requires path)\n"
            "- mkdir: Create a directory (requires path)\n"
            "- copy: Copy a file or directory (requires path and dest)\n"
            "- move: Move a file or directory (requires path and dest)\n"
        )

    @property 
    def input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "operation": {
                    "type": "string",
                    "description": "The operation to perform",
                    "enum": ["write", "read", "read_lines", "edit_lines", "delete", "mkdir", "copy", "move", "append", "list_dir"]
                },
                "path": {
                    "type": "string", 
                    "description": "Path to the file or directory to operate on"
                },
                "content": {
                    "type": "string",
                    "description": "Content to write or edit"
                },
                "start_line": {
                    "type": "integer",
                    "description": "Starting line number for line operations (1-based)"
                },
                "end_line": {
                    "type": "integer", 
                    "description": "Ending line number for line operations (1-based)"
                },
                "dest": {
                    "type": "string",
                    "description": "Destination path for copy/move operations"
                },
                "recursive": {
                    "type": "boolean",
                    "description": "Whether to operate recursively on directories"
                }
            },
            "required": ["operation", "path"]
        }

    def run(self, input: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the requested file operation."""
        operation = input.get("operation", "")
        path = input.get("path", "")
        
        if not operation or not path:
            return {
                "type": "tool_response",
                "tool_use_id": input.get("tool_use_id", ""),
                "content": "Error: operation and path are required"
            }

        # Convert relative path to absolute
        path = str(Path(path).absolute())
        
        try:
            if operation == "write":
                content = input.get("content")
                if not content:
                    return self._error("content is required for write operation")
                os.makedirs(os.path.dirname(path), exist_ok=True)
                with open(path, 'w') as f:
                    f.write(content)
                return self._success(f"Successfully wrote to {path}")
                
            elif operation == "read":
                if not os.path.exists(path):
                    return self._error(f"File not found: {path}")
                with open(path) as f:
                    content = f.read()
                return self._success(content)
                
            elif operation == "read_lines":
                start = input.get("start_line")
                end = input.get("end_line")
                if not start or not end:
                    return self._error("start_line and end_line required for read_lines")
                if not os.path.exists(path):
                    return self._error(f"File not found: {path}")
                    
                with open(path) as f:
                    lines = f.readlines()
                if start < 1 or end > len(lines):
                    return self._error(f"Line numbers out of range (1-{len(lines)})")
                    
                result = []
                for i in range(start-1, end):
                    result.append(f"{i+1}: {lines[i].rstrip()}")
                return self._success("\n".join(result))
                
            elif operation == "edit_lines":
                start = input.get("start_line")
                end = input.get("end_line")
                content = input.get("content")
                if not all([start, end, content]):
                    return self._error("start_line, end_line and content required for edit_lines")
                if not os.path.exists(path):
                    return self._error(f"File not found: {path}")
                    
                with open(path) as f:
                    lines = f.readlines()
                if start < 1 or end > len(lines):
                    return self._error(f"Line numbers out of range (1-{len(lines)})")
                    
                new_lines = content.splitlines()
                lines[start-1:end] = [line + '\n' for line in new_lines]
                
                with open(path, 'w') as f:
                    f.writelines(lines)
                return self._success(f"Successfully edited lines {start}-{end} in {path}")
                
            elif operation == "delete":
                if not os.path.exists(path):
                    return self._error(f"Path not found: {path}")
                if os.path.isdir(path):
                    shutil.rmtree(path)
                else:
                    os.remove(path)
                return self._success(f"Successfully deleted {path}")
                
            elif operation == "mkdir":
                recursive = input.get("recursive", False)
                if recursive:
                    os.makedirs(path, exist_ok=True)
                else:
                    os.mkdir(path)
                return self._success(f"Successfully created directory {path}")
                
            elif operation == "copy":
                dest = input.get("dest")
                if not dest:
                    return self._error("dest is required for copy operation")
                dest = str(Path(dest).absolute())
                if not os.path.exists(path):
                    return self._error(f"Source not found: {path}")
                    
                if os.path.isdir(path):
                    shutil.copytree(path, dest)
                else:
                    os.makedirs(os.path.dirname(dest), exist_ok=True)
                    shutil.copy2(path, dest)
                return self._success(f"Successfully copied {path} to {dest}")
                
            elif operation == "move":
                dest = input.get("dest")
                if not dest:
                    return self._error("dest is required for move operation")
                dest = str(Path(dest).absolute())
                if not os.path.exists(path):
                    return self._error(f"Source not found: {path}")
                    
                os.makedirs(os.path.dirname(dest), exist_ok=True)
                shutil.move(path, dest)
                return self._success(f"Successfully moved {path} to {dest}")

            elif operation == "append":
                content = input.get("content")
                if not content:
                    return self._error("content is required for append operation")
                if not os.path.exists(path):
                    return self._error(f"File not found: {path}")
                with open(path, 'a') as f:
                    f.write(content)
                return self._success(f"Successfully appended to {path}")

            elif operation == "list_dir":
                if not os.path.exists(path):
                    return self._error(f"Directory not found: {path}")
                if not os.path.isdir(path):
                    return self._error(f"Path is not a directory: {path}")
                
                recursive = input.get("recursive", False)
                result = []
                
                def list_items(dir_path, prefix=""):
                    for item in os.listdir(dir_path):
                        item_path = os.path.join(dir_path, item)
                        rel_path = os.path.relpath(item_path, path)
                        if os.path.isdir(item_path):
                            result.append(f"DIR  {prefix}{rel_path}")
                            if recursive:
                                list_items(item_path, prefix)
                        else:
                            result.append(f"FILE {prefix}{rel_path}")
                            
                list_items(path)
                return self._success("\n".join(result))
                
            else:
                return self._error(f"Unknown operation: {operation}")
                
        except Exception as e:
            return self._error(str(e))
            
    def _success(self, content: str) -> Dict[str, Any]:
        return {
            "type": "tool_response",
            "tool_use_id": "",
            "content": content
        }
        
    def _error(self, message: str) -> Dict[str, Any]:
        return {
            "type": "tool_response", 
            "tool_use_id": "",
            "content": f"Error: {message}"
        }

    def validate_path(self, command: str, path: Path):
        """Check that the path/command combination is valid."""
        if not path.exists() and command != "create":
            raise ValueError(f"The path {path} does not exist")
        if path.exists() and command == "create":
            raise ValueError(f"File already exists at: {path}")
        if path.is_dir() and command != "view":
            raise ValueError(f"The path {path} is a directory and only view command is allowed")

    def _view(self, path: Path, view_range: Optional[List[int]] = None) -> str:
        """Implement the view command"""
        if path.is_dir():
            files = [str(p) for p in path.glob("**/*") if not str(p).startswith(".")]
            return f"Contents of directory {path}:\n" + "\n".join(files)

        with open(path, "r", encoding="utf-8") as f:
            content = f.read()

        if view_range:
            if len(view_range) != 2:
                raise ValueError("view_range must contain exactly 2 integers")
            
            lines = content.split("\n")
            start, end = view_range
            if start < 1 or start > len(lines):
                raise ValueError(f"Invalid start line {start}")
            if end > len(lines):
                raise ValueError(f"Invalid end line {end}")
            if end < start:
                raise ValueError(f"End line {end} cannot be less than start line {start}")
                
            content = "\n".join(lines[start-1:end])

        return f"Contents of {path}:\n{content}"

    def _create(self, path: Path, content: str) -> str:
        """Implement the create command"""
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
        return f"File created successfully at: {path}"

    def _str_replace(self, path: Path, old_str: str, new_str: str) -> str:
        """Implement the str_replace command"""
        with open(path, "r", encoding="utf-8") as f:
            content = f.read()

        occurrences = content.count(old_str)
        if occurrences == 0:
            raise ValueError(f"Text '{old_str}' not found in file")
        if occurrences > 1:
            lines = [i+1 for i, line in enumerate(content.split("\n")) if old_str in line]
            raise ValueError(f"Multiple occurrences of '{old_str}' found in lines {lines}")

        new_content = content.replace(old_str, new_str)
        
        # Save history before making changes
        self._file_history[path].append(content)
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(new_content)

        # Create snippet around the edit
        lines = content.split("\n")
        edit_line = content.split(old_str)[0].count("\n")
        start = max(0, edit_line - SNIPPET_LINES)
        end = min(len(lines), edit_line + SNIPPET_LINES + new_str.count("\n"))
        snippet = "\n".join(new_content.split("\n")[start:end])

        return f"File edited successfully. Snippet of changes:\n{snippet}"

    def _insert(self, path: Path, insert_line: int, new_str: str) -> str:
        """Implement the insert command"""
        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        if insert_line < 0 or insert_line > len(lines):
            raise ValueError(f"Invalid insert line {insert_line}")

        # Save history
        self._file_history[path].append("".join(lines))

        # Insert the new content
        new_lines = new_str.split("\n")
        lines[insert_line:insert_line] = new_lines

        with open(path, "w", encoding="utf-8") as f:
            f.writelines(lines)

        # Create snippet
        start = max(0, insert_line - SNIPPET_LINES)
        end = min(len(lines), insert_line + len(new_lines) + SNIPPET_LINES)
        snippet = "".join(lines[start:end])

        return f"Text inserted successfully. Snippet of changes:\n{snippet}"

    def _undo_edit(self, path: Path) -> str:
        """Implement the undo_edit command"""
        if not self._file_history[path]:
            raise ValueError(f"No edit history for {path}")

        previous_content = self._file_history[path].pop()
        with open(path, "w", encoding="utf-8") as f:
            f.write(previous_content)

        return f"Successfully reverted last edit to {path}"
]]></content>
    </file>
    <file>
      <path>tools/mcp_tools/mcp_base_tool.py</path>
      <content><![CDATA[# Base class for the mcp tools]]></content>
    </file>
    <file>
      <path>tools/package_manager_tool.py</path>
      <content><![CDATA[# selfprompter/tools/package_manager_tool.py

import os
import subprocess
import sys
from typing import Any, Dict, List, Optional, TypedDict

from .tool_base import Tool
from .config import Config

class PackageManagerOptions(TypedDict):
    """Options for package manager operations."""
    virtual_env: Optional[str]
    index_url: Optional[str]
    extra_index_url: Optional[str]
    trusted_host: Optional[str]
    require_virtualenv: bool
    no_deps: bool
    pre: bool

class PackageManagerTool(Tool):
    """Tool for managing Python packages using pip."""

    @property
    def name(self) -> str:
        return "package_manager"

    @property
    def description(self) -> str:
        return "Manages Python packages using pip"

    @property
    def input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "action": {
                    "type": "string",
                    "description": "Package management action to perform",
                    "enum": ["install", "uninstall", "list", "check", "info", "upgrade", "freeze", "config"]
                },
                "package": {
                    "type": "string",
                    "description": "Package name with optional version specification"
                },
                "requirements_file": {
                    "type": "string",
                    "description": "Path to requirements.txt file"
                }
            },
            "required": ["action"]
        }

    def __init__(self, options: Optional[PackageManagerOptions] = None):
        self.options = options or {}
        self.pip_cmd = self._get_pip_cmd()

    def _get_pip_cmd(self) -> str:
        """Get the appropriate pip command based on environment."""
        if Config.PACKAGE_MANAGER_CONFIG["pip_command"]:
            return Config.PACKAGE_MANAGER_CONFIG["pip_command"]
        
        venv = self.options.get('virtual_env') or os.environ.get('VIRTUAL_ENV')
        if Config.PACKAGE_MANAGER_CONFIG["use_module_pip"]:
            return sys.executable  # Will be used with -m pip
        
        if venv:
            if os.name == 'nt':  # Windows
                return os.path.join(venv, 'Scripts', 'pip.exe')
            return os.path.join(venv, 'bin', 'pip')
        return 'pip'

    def _run_pip(self, *args: str) -> Dict[str, Any]:
        """Run pip command and return result."""
        try:
            if Config.PACKAGE_MANAGER_CONFIG["use_module_pip"]:
                cmd = [sys.executable, "-m", "pip"] + list(args)
            else:
                cmd = [self.pip_cmd] + list(args)
            
            if index_url := self.options.get('index_url'):
                cmd.extend(['--index-url', index_url])
            if extra_index := self.options.get('extra_index_url'):
                cmd.extend(['--extra-index-url', extra_index])
            if trusted_host := self.options.get('trusted_host'):
                cmd.extend(['--trusted-host', trusted_host])
            if self.options.get('require_virtualenv'):
                cmd.append('--require-virtualenv')
            if self.options.get('no_deps'):
                cmd.append('--no-deps')
            if self.options.get('pre'):
                cmd.append('--pre')

            result = self._try_run_command(cmd)
            if "Error" not in result["content"]:
                return result
            
            # If failed, try python -m pip
            cmd = [sys.executable, "-m", "pip"] + list(args)
            return self._try_run_command(cmd)
        except Exception as e:
            return {"content": f"Error: Error running pip: {str(e)}"}

    def _try_run_command(self, cmd: List[str]) -> Dict[str, Any]:
        """Try running a command and return result."""
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        return {"content": result.stdout}

    def run(self, input: Dict[str, Any]) -> Dict[str, Any]:
        """Execute package management action."""
        action = input.get("action", "install")
        package = input.get("package", "")
        requirements_file = input.get("requirements_file")

        if action == "install":
            if requirements_file:
                return self._run_pip('install', '-r', requirements_file)
            elif package:
                return self._run_pip('install', package)
            else:
                return {"content": "Error: Either package or requirements_file must be specified for install"}
        
        elif action == "uninstall":
            if not package:
                return {"content": "Error: Package name is required for uninstall"}
            return self._run_pip('uninstall', '-y', package)
        
        elif action == "list":
            return self._run_pip('list')
        
        elif action == "check":
            if not package:
                return {"content": "Error: Package name is required for check"}
            return self._run_pip('show', package)
        
        elif action == "info":
            if not package:
                return {"content": "Error: Package name is required for info"}
            return self._run_pip('show', package)
        
        elif action == "upgrade":
            if not package:
                return {"content": "Error: Package name is required for upgrade"}
            return self._run_pip('install', '--upgrade', package)
        
        elif action == "freeze":
            return self._run_pip('freeze')
        
        elif action == "config":
            return self._run_pip('-V')
        
        else:
            return {"content": f"Error: Unsupported action: {action}"}

    def install(self, package_spec: str) -> Dict[str, Any]:
        """Install a package."""
        return self._run_pip('install', package_spec)

    def uninstall(self, package_name: str) -> Dict[str, Any]:
        """Uninstall a package."""
        return self._run_pip('uninstall', '-y', package_name)

    def list(self) -> Dict[str, Any]:
        """List installed packages."""
        return self._run_pip('list')

    def check(self, package_name: str) -> Dict[str, Any]:
        """Check if a package is installed."""
        return self._run_pip('show', package_name)

    def info(self, package_name: str) -> Dict[str, Any]:
        """Get package information."""
        return self._run_pip('show', package_name)

    def install_requirements(self, requirements_file: str) -> Dict[str, Any]:
        """Install packages from requirements file."""
        return self._run_pip('install', '-r', requirements_file)

    def upgrade(self, package_name: str) -> Dict[str, Any]:
        """Upgrade a package to latest version."""
        return self._run_pip('install', '--upgrade', package_name)

    def freeze(self) -> Dict[str, Any]:
        """Export installed packages as requirements."""
        return self._run_pip('freeze')

    def check_outdated(self) -> Dict[str, Any]:
        """Check for outdated packages."""
        return self._run_pip('list', '--outdated')

    def config(self) -> Dict[str, Any]:
        """Get pip configuration and location."""
        return self._run_pip('-V')

    def cache_info(self) -> Dict[str, Any]:
        """Get pip cache information."""
        return self._run_pip('cache', 'info')

    def cache_clear(self) -> Dict[str, Any]:
        """Clear pip cache."""
        return self._run_pip('cache', 'purge')

    def wheel_info(self, wheel_file: str) -> Dict[str, Any]:
        """Get information about a wheel file."""
        return self._run_pip('show', '-f', wheel_file)
]]></content>
    </file>
    <file>
      <path>tools/requests_tool.py</path>
      <content><![CDATA[# selfprompter/tools/requests_tool.py

import requests
from typing import Dict, Any, Optional
from .tool_base import Tool, ToolResult

class RequestsTool(Tool):
    """
    Tool for making HTTP requests.
    Follows Anthropic Claude tool use standards.
    """

    def __init__(self, default_headers: Optional[Dict[str, str]] = None):
        """
        Initialize with optional default headers.
        
        Args:
            default_headers: Default headers to include in all requests
        """
        self.default_headers = default_headers or {}

    @property
    def name(self) -> str:
        return "http_request"

    @property
    def description(self) -> str:
        return (
            "Makes HTTP requests to specified URLs. Supports GET, POST, PUT, DELETE methods. "
            "Can send custom headers and data. Returns response content and status."
        )

    @property
    def input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "The URL to send the request to"
                },
                "method": {
                    "type": "string",
                    "description": "HTTP method to use",
                    "enum": ["GET", "POST", "PUT", "DELETE"],
                    "default": "GET"
                },
                "headers": {
                    "type": "object",
                    "description": "Request headers",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "data": {
                    "type": "string",
                    "description": "Request body data"
                },
                "timeout": {
                    "type": "integer",
                    "description": "Request timeout in seconds",
                    "minimum": 1,
                    "maximum": 60,
                    "default": 30
                }
            },
            "required": ["url"],
            "additionalProperties": False
        }

    def run(self, tool_call_id: str, **kwargs) -> ToolResult:
        """
        Execute an HTTP request.
        
        Args:
            tool_call_id: Unique ID for this tool call
            url: The URL to send the request to
            method: HTTP method (default: "GET")
            headers: Request headers
            data: Request body data
            timeout: Request timeout in seconds (default: 30)
            
        Returns:
            ToolResult containing response data or error message
        """
        try:
            url = kwargs.get("url")
            if not url:
                raise ValueError("URL is required")

            method = kwargs.get("method", "GET").upper()
            if method not in ["GET", "POST", "PUT", "DELETE"]:
                raise ValueError(f"Unsupported HTTP method: {method}")

            headers = {**self.default_headers, **(kwargs.get("headers") or {})}
            data = kwargs.get("data")
            timeout = min(max(1, kwargs.get("timeout", 30)), 60)

            response = requests.request(
                method=method,
                url=url,
                headers=headers,
                data=data,
                timeout=timeout
            )

            response.raise_for_status()
            
            result = {
                "status_code": response.status_code,
                "headers": dict(response.headers),
                "content": response.text
            }

            return self.format_result(tool_call_id, str(result))

        except requests.RequestException as e:
            return self.format_error(tool_call_id, f"Request failed: {str(e)}")
        except Exception as e:
            return self.format_error(tool_call_id, str(e))
]]></content>
    </file>
    <file>
      <path>tools/shell_tool.py</path>
      <content><![CDATA[# selfprompter/tools/shell_tool.py

import subprocess
from typing import Dict, Any, Optional, List
from .tool_base import Tool

class ShellTool(Tool):
    """
    Tool for executing shell commands safely.
    Follows Anthropic Claude tool use standards.
    """

    def __init__(self, allowed_commands: Optional[List[str]] = None):
        """
        Initialize with optional allowed commands whitelist.
        
        Args:
            allowed_commands: List of allowed shell commands
        """
        self.allowed_commands = allowed_commands or []

    @property
    def name(self) -> str:
        return "shell"

    @property
    def description(self) -> str:
        return (
            "Executes shell commands in a controlled environment. Commands are validated "
            "against a whitelist if provided. Returns command output or error messages."
        )

    @property
    def input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "command": {
                    "type": "string",
                    "description": "The shell command to execute"
                },
                "timeout": {
                    "type": "integer",
                    "description": "Command timeout in seconds",
                    "minimum": 1,
                    "maximum": 300,
                    "default": 60
                },
                "working_dir": {
                    "type": "string",
                    "description": "Working directory for command execution"
                }
            },
            "required": ["command"]
        }

    def _is_command_allowed(self, command: str) -> bool:
        """Check if command is in allowed list."""
        if not self.allowed_commands:
            return True
        return any(command.startswith(cmd) for cmd in self.allowed_commands)

    def run(self, input: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute a shell command.
        
        Args:
            input: Dictionary containing:
                command: The shell command to execute
                timeout: Command timeout in seconds (default: 60)
                working_dir: Working directory for execution
            
        Returns:
            Dictionary containing command output or error message
        """
        try:
            command = input.get("command")
            if not command:
                return {
                    "type": "tool_response",
                    "content": "Error: Command is required"
                }

            if not self._is_command_allowed(command):
                return {
                    "type": "tool_response",
                    "content": f"Error: Command '{command}' is not in the allowed list"
                }

            timeout = min(max(1, input.get("timeout", 60)), 300)
            working_dir = input.get("working_dir")

            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=working_dir
            )

            if result.returncode != 0:
                error_msg = result.stderr.strip() or f"Command failed with exit code {result.returncode}"
                return {
                    "type": "tool_response",
                    "content": f"Error: {error_msg}"
                }

            output = result.stdout.strip() or "Command executed successfully (no output)"
            return {
                "type": "tool_response",
                "content": output
            }

        except subprocess.TimeoutExpired:
            return {
                "type": "tool_response",
                "content": f"Error: Command timed out after {timeout} seconds"
            }
        except Exception as e:
            return {
                "type": "tool_response",
                "content": f"Error: {str(e)}"
            }
]]></content>
    </file>
    <file>
      <path>tools/tool_base.py</path>
      <content><![CDATA[# selfprompter/tools/tool_base.py
"""
Base classes for Anthropic Claude tool implementation.
Based on: https://docs.anthropic.com/claude/docs/tool-use
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, TypedDict, List, Literal

class ToolInput_schema(TypedDict):
    type: str
    function: Dict[str, Any]

class ToolCall(TypedDict):
    id: str
    type: str
    function: Dict[str, Any]

class ToolResult(TypedDict):
    type: str
    tool_use_id: str
    content: str

class Tool(ABC):
    """
    Abstract base class for tools following Anthropic Claude standards.
    See: https://docs.anthropic.com/claude/docs/tool-use
    """
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Tool name used in function calling format."""
        pass
        
    @property
    @abstractmethod
    def description(self) -> str:
        """Human-readable description of what the tool does."""
        pass
        
    @property
    @abstractmethod
    def input_schema(self) -> Dict[str, Any]:
        """
        JSONSchema object defining accepted input_schema.
        Must include:
        - type: "object"
        - properties: Parameter definitions
        - required: List of required input_schema
        """
        pass

    @abstractmethod
    def run(self, input: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the tool with the given input input_schema."""
        pass

    def get_tool_definition(self) -> Dict[str, Any]:
        """Get the tool definition in Anthropic's format.

        Robust against mis-implemented input_schema properties in subclasses that
        accidentally recurse (e.g., property returns self.input_schema).
        """
        schema: Dict[str, Any] | None = None

        # Primary attempt - may raise RecursionError if subclass is faulty
        try:
            schema = self.input_schema
        except RecursionError:
            schema = None
        except Exception:
            # Ignore and try fallbacks
            schema = None

        # Fallback: walk MRO to find a sane input_schema on a base class
        if not isinstance(schema, dict) or not schema:
            for cls in type(self).mro()[1:]:
                if "input_schema" in cls.__dict__:
                    attr = cls.__dict__["input_schema"]
                    try:
                        if isinstance(attr, property):
                            candidate = attr.__get__(self, cls)
                        else:
                            candidate = attr
                    except RecursionError:
                        continue
                    except Exception:
                        continue
                    if isinstance(candidate, dict) and candidate:
                        schema = candidate
                        break

        # Final guard: ensure dictionary shape
        if not isinstance(schema, dict):
            schema = {"type": "object", "properties": {}, "required": []}

        properties = schema.get("properties", {}) or {}
        required = schema.get("required", []) or []

        return {
            "name": self.name,
            "description": self.description,
            "input_schema": {
                "type": "object",
                "properties": properties,
                "required": required,
            },
        }

    def format_result(self, tool_call_id: str, content: str) -> ToolResult:
        """Format a successful result in Anthropic's format."""
        return {
            "type": "tool_result",
            "tool_use_id": tool_call_id,
            "content": content
        }

    def format_error(self, tool_call_id: str, error: str) -> ToolResult:
        """Format an error result in Anthropic's format."""
        return {
            "type": "tool_result",
            "tool_use_id": tool_call_id,
            "content": f"Error: {error}"
        }
]]></content>
    </file>
    <file>
      <path>tools/tool_manager.py</path>
      <content><![CDATA[# selfprompter/tools/tool_manager.py

from typing import Dict, Any, List, Type
from .tool_base import Tool

# Import all available tools
from .shell_tool import ShellTool
from .requests_tool import RequestsTool
from .file_tool import FileTool
from .web_search_tool import WebSearchTool
from .web_browser_tool import WebBrowserTool
from .package_manager_tool import PackageManagerTool
from .advanced_file_tool import AdvancedFileTool
from .code_runner_tool import CodeRunnerTool

class ToolManager:
    """
    Manages a collection of tools and handles tool registration and execution.
    Follows Anthropic Claude tool use standards.
    """

    def __init__(self, register_defaults: bool = True):
        """
        Initialize tool registry.
        
        Args:
            register_defaults: Whether to register default tools
        """
        self.tools: Dict[str, Tool] = {}
        if register_defaults:
            self.register_default_tools()

    def register_default_tools(self) -> None:
        """Register all default tools with standard configurations."""
        default_tools = [
            ShellTool(),
            RequestsTool(),
            FileTool(),
            WebSearchTool(),       # No API key args; uses direct HTTP fetch
            WebBrowserTool(),      # No timeout arg; accepts optional user_agent only
            PackageManagerTool(),
            AdvancedFileTool(),
            CodeRunnerTool(),
        ]
        for tool in default_tools:
            self.register_tool(tool)

    def register_tool(self, tool: Tool) -> None:
        """
        Register a tool instance.
        
        Args:
            tool: Tool instance to register
        """
        self.tools[tool.name] = tool

    def register_tool_class(self, tool_class: Type[Tool], **kwargs) -> None:
        """
        Register a tool class by instantiating and registering it.
        
        Args:
            tool_class: Tool class to instantiate and register
            **kwargs: Arguments to pass to tool constructor
        """
        tool = tool_class(**kwargs)
        self.register_tool(tool)

    def get_tool(self, name: str) -> Tool:
        """
        Get a registered tool by name.
        
        Args:
            name: Name of the tool to retrieve
            
        Returns:
            The requested tool instance
            
        Raises:
            KeyError: If tool is not found
        """
        if name not in self.tools:
            raise KeyError(f"Tool '{name}' not found")
        return self.tools[name]

    def list_tools(self) -> List[Dict[str, Any]]:
        """
        Get information about all registered tools.
        
        Returns:
            List of tool information dictionaries containing name,
            description and input_schema for each tool
        """
        return [
            {
                "name": tool.name,
                "description": tool.description,
                "input_schema": tool.input_schema
            }
            for tool in self.tools.values()
        ]

    def execute_tool(self, tool_call_id: str, name: str, **kwargs) -> Any:
        """
        Execute a tool by name with given arguments.
        
        Args:
            tool_call_id: Unique ID for this tool call
            name: Name of the tool to execute
            **kwargs: Arguments to pass to the tool
            
        Returns:
            Tool execution result
            
        Raises:
            KeyError: If tool is not found
        """
        tool = self.get_tool(name)
        return tool.run(tool_call_id, **kwargs)

    ]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/README.md</path>
      <content/>
    </file>
    <file>
      <path>tools/voidkit_tools/__init__.py</path>
      <content><![CDATA[# FUM Advanced Math package: expose void_dynamics APIs
from .void_dynamics.FUM_Void_Equations import universal_void_dynamics
from .void_dynamics.FUM_Void_Debt_Modulation import VoidDebtModulation

__all__ = ["universal_void_dynamics", "VoidDebtModulation"]]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/calculate_descriptive_stats.py</path>
      <content><![CDATA[import numpy as np
from scipy import stats

def calculate_descriptive_stats(data, nan_policy='propagate'):
    """
    Calculate descriptive statistics for a given dataset.
    
    Parameters
    ----------
    data : array_like
        Input data array. Must be convertible to a 1D NumPy array of numbers.
    nan_policy : {'propagate', 'omit', 'raise'}, optional
        Defines how to handle NaN values:
        - 'propagate': returns nan for statistics when NaN values are present (default)
        - 'omit': ignores NaN values when computing statistics
        - 'raise': raises an error if NaN values are present
    
    Returns
    -------
    dict
        Dictionary containing the following descriptive statistics:
        - 'count': Number of observations
        - 'mean': Arithmetic mean
        - 'stdev': Standard deviation (sample)
        - 'variance': Variance (sample)
        - 'min': Minimum value
        - 'max': Maximum value
        - 'skewness': Skewness (third standardized moment)
        - 'kurtosis': Kurtosis (fourth standardized moment, Fisher's definition: normal = 0.0)
    
    Raises
    ------
    TypeError
        If input data cannot be converted to a numeric array.
    ValueError
        If nan_policy is not one of {'propagate', 'omit', 'raise'}.
        If input data is empty.
        If NaN values are present and nan_policy is 'raise'.
    
    Examples
    --------
    >>> import numpy as np
    >>> data = [1, 2, 3, 4, 5]
    >>> stats = calculate_descriptive_stats(data)
    >>> print(stats['mean'])
    3.0
    >>> print(stats['stdev'])
    1.5811388300841898
    """
    # Validate nan_policy parameter
    valid_policies = ['propagate', 'omit', 'raise']
    if nan_policy not in valid_policies:
        raise ValueError(f"nan_policy must be one of {valid_policies}, got '{nan_policy}'")
    
    # Check if data is None
    if data is None:
        raise ValueError("Input data cannot be None")
    
    # Convert input to numpy array
    try:
        data_array = np.asarray(data, dtype=float)
    except (ValueError, TypeError) as e:
        raise TypeError(f"Failed to convert input data to numeric array: {str(e)}")
    
    # Ensure data is 1D
    if data_array.ndim > 1:
        raise ValueError(f"Input data must be 1-dimensional, got {data_array.ndim}-dimensional data")
    
    # Check if data is empty
    if data_array.size == 0:
        raise ValueError("Input data cannot be empty")
    
    # Check for NaN values if policy is 'raise'
    if nan_policy == 'raise' and np.isnan(data_array).any():
        raise ValueError("Input data contains NaN values")
    
    # Special case for single value
    if data_array.size == 1:
        value = data_array[0]
        return {
            'count': 1,
            'mean': value,
            'variance': 0.0,
            'stdev': 0.0,
            'min': value,
            'max': value,
            'skewness': 0.0,
            'kurtosis': 0.0
        }
    
    # Special case for constant values (zero variance)
    if np.all(data_array == data_array[0]):
        value = data_array[0]
        return {
            'count': data_array.size,
            'mean': value,
            'variance': 0.0,
            'stdev': 0.0,
            'min': value,
            'max': value,
            'skewness': 0.0,
            'kurtosis': 0.0
        }
    
    # Handle NaN values
    has_nan = np.isnan(data_array).any()
    
    # Use scipy.stats.describe to efficiently calculate multiple statistics
    try:
        # For propagate policy with NaNs, we need special handling for min/max
        if nan_policy == 'propagate' and has_nan:
            # Get non-NaN values for min/max calculation
            non_nan_data = data_array[~np.isnan(data_array)]
            
            result = stats.describe(data_array, nan_policy=nan_policy)
            
            # Process the DescribeResult object into a dictionary with standardized keys
            stats_dict = {
                'count': np.sum(~np.isnan(data_array)),  # Count of non-NaN values
                'mean': result.mean,
                'variance': result.variance,
                'stdev': np.sqrt(result.variance) if not np.isnan(result.variance) else np.nan,
                'min': np.min(non_nan_data) if non_nan_data.size > 0 else np.nan,
                'max': np.max(non_nan_data) if non_nan_data.size > 0 else np.nan,
                'skewness': result.skewness,
                'kurtosis': result.kurtosis
            }
        else:
            result = stats.describe(data_array, nan_policy=nan_policy)
            
            # Process the DescribeResult object into a dictionary with standardized keys
            stats_dict = {
                'count': result.nobs,
                'mean': result.mean,
                'variance': result.variance,
                'stdev': np.sqrt(result.variance),
                'min': result.minmax[0],
                'max': result.minmax[1],
                'skewness': result.skewness,
                'kurtosis': result.kurtosis
            }
        
        return stats_dict
        
    except Exception as e:
        # Handle specific known errors
        if "zero variance" in str(e).lower():
            # Special handling for zero variance case
            # Calculate what we can and set others to appropriate values
            stats_dict = {
                'count': len(data_array),
                'mean': np.mean(data_array),
                'variance': 0.0,
                'stdev': 0.0,
                'min': np.min(data_array),
                'max': np.max(data_array),
                'skewness': 0.0,  # Undefined for constant data, set to 0
                'kurtosis': 0.0   # Undefined for constant data, set to 0
            }
            return stats_dict
        else:
            # Re-raise with a more informative error message
            raise type(e)(f"Error calculating descriptive statistics: {str(e)}")
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/causal_inference/__init__.py</path>
      <content><![CDATA[from .causal_inference import granger_causality, calculate_transfer_entropy
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/causal_inference/causal_inference.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from statsmodels.tsa.stattools import grangercausalitytests
from typing import Tuple, Dict, Any

def granger_causality(
    data: np.ndarray, 
    max_lag: int, 
    test: str = 'ssr_chi2test'
) -> Dict[str, Any]:
    """
    Performs a Granger causality test on a multivariate time series.

    Parameters
    ----------
    data : np.ndarray
        A 2D numpy array of shape (n_obs, 2) where each column is a time series.
    max_lag : int
        The maximum number of lags to test for.
    test : str, optional
        The test to perform, by default 'ssr_chi2test'.

    Returns
    -------
    Dict[str, Any]
        A dictionary containing the test results.
    """
    return grangercausalitytests(data, maxlag=max_lag, verbose=False)

def calculate_transfer_entropy(
    x: np.ndarray, 
    y: np.ndarray, 
    lag: int = 1, 
    n_bins: int = 10
) -> float:
    """
    Calculates the transfer entropy from time series x to time series y.

    Parameters
    ----------
    x : np.ndarray
        The source time series.
    y : np.ndarray
        The target time series.
    lag : int, optional
        The time lag, by default 1.
    n_bins : int, optional
        The number of bins to use for discretizing the data, by default 10.

    Returns
    -------
    float
        The transfer entropy from x to y.
    """
    if len(x) != len(y):
        raise ValueError("Time series must have the same length.")

    # Discretize the data
    x_binned = np.digitize(x, bins=np.linspace(x.min(), x.max(), n_bins + 1)) - 1
    y_binned = np.digitize(y, bins=np.linspace(y.min(), y.max(), n_bins + 1)) - 1

    # Create lagged versions of the series
    y_t = y_binned[lag:]
    y_t_minus_1 = y_binned[:-lag]
    x_t_minus_1 = x_binned[:-lag]

    # Calculate probabilities
    p_y_t_y_t_minus_1_x_t_minus_1 = np.zeros((n_bins, n_bins, n_bins))
    p_y_t_minus_1_x_t_minus_1 = np.zeros((n_bins, n_bins))
    p_y_t_y_t_minus_1 = np.zeros((n_bins, n_bins))
    p_y_t_minus_1 = np.zeros(n_bins)

    for i in range(len(y_t)):
        p_y_t_y_t_minus_1_x_t_minus_1[y_t[i], y_t_minus_1[i], x_t_minus_1[i]] += 1
        p_y_t_minus_1_x_t_minus_1[y_t_minus_1[i], x_t_minus_1[i]] += 1
        p_y_t_y_t_minus_1[y_t[i], y_t_minus_1[i]] += 1
        p_y_t_minus_1[y_t_minus_1[i]] += 1

    # Normalize to get probabilities
    p_y_t_y_t_minus_1_x_t_minus_1 /= len(y_t)
    p_y_t_minus_1_x_t_minus_1 /= len(y_t)
    p_y_t_y_t_minus_1 /= len(y_t)
    p_y_t_minus_1 /= len(y_t)

    # Calculate transfer entropy
    te = 0.0
    for i in range(n_bins):
        for j in range(n_bins):
            for k in range(n_bins):
                if p_y_t_y_t_minus_1_x_t_minus_1[i, j, k] > 0:
                    p_cond1 = p_y_t_y_t_minus_1_x_t_minus_1[i, j, k] / p_y_t_minus_1_x_t_minus_1[j, k]
                    p_cond2 = p_y_t_y_t_minus_1[i, j] / p_y_t_minus_1[j]
                    te += p_y_t_y_t_minus_1_x_t_minus_1[i, j, k] * np.log2(p_cond1 / p_cond2)
    
    return te
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/clustering/__init__.py</path>
      <content><![CDATA[from .adaptive_clustering import calculate_adaptive_clustering_interval
from .spectral_clustering import spectral_clustering_with_temporal_kernel
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/clustering/adaptive_clustering.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
import networkx as nx
from scipy.stats import entropy

def calculate_adaptive_clustering_interval(
    graph: nx.Graph,
    base_interval: float = 100000.0,
    alpha: float = 0.05
) -> float:
    """
    Calculates the adaptive clustering interval based on graph entropy.

    t_cluster = base_interval * e^(-α * graph_entropy)

    Parameters
    ----------
    graph : nx.Graph
        The graph to analyze.
    base_interval : float, optional
        The base clustering interval, by default 100000.0.
    alpha : float, optional
        The scaling factor for the entropy, by default 0.05.

    Returns
    -------
    float
        The calculated adaptive clustering interval.
    """
    degrees = [d for n, d in graph.degree()]
    if not degrees:
        return base_interval
        
    degree_counts = np.bincount(degrees)
    degree_distribution = degree_counts / np.sum(degree_counts)
    
    graph_entropy = entropy(degree_distribution, base=2)
    
    return base_interval * np.exp(-alpha * graph_entropy)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/clustering/spectral_clustering.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
import networkx as nx
from sklearn.cluster import SpectralClustering
from typing import Tuple

def spectral_clustering_with_temporal_kernel(
    spike_rates: np.ndarray,
    spike_times: np.ndarray,
    sigma: float = 1.0,
    tau: float = 1.0,
    max_clusters: int = 10
) -> Tuple[int, np.ndarray]:
    """
    Performs spectral clustering with a temporal kernel to find the optimal
    number of clusters.

    The affinity matrix W is defined as:
    W_ij = exp(-||rate_i - rate_j||² / σ² - |Δt_ij| / τ)

    Parameters
    ----------
    spike_rates : np.ndarray
        A 1D numpy array of spike rates for each neuron.
    spike_times : np.ndarray
        A 1D numpy array of the last spike time for each neuron.
    sigma : float, optional
        The width of the Gaussian kernel for the rates, by default 1.0.
    tau : float, optional
        The time constant for the temporal kernel, by default 1.0.
    max_clusters : int, optional
        The maximum number of clusters to test for, by default 10.

    Returns
    -------
    Tuple[int, np.ndarray]
        A tuple containing:
        - The optimal number of clusters found.
        - The labels for each neuron.
    """
    n_neurons = len(spike_rates)
    
    # Calculate rate differences
    rate_diff = spike_rates[:, np.newaxis] - spike_rates[np.newaxis, :]
    
    # Calculate time differences
    time_diff = spike_times[:, np.newaxis] - spike_times[np.newaxis, :]
    
    # Calculate affinity matrix
    affinity_matrix = np.exp(-rate_diff**2 / sigma**2 - np.abs(time_diff) / tau)
    
    # Find optimal k using the eigengap heuristic
    eigenvalues, _ = np.linalg.eigh(affinity_matrix)
    eigengaps = np.diff(eigenvalues)
    optimal_k = np.argmax(eigengaps) + 1
    
    if optimal_k > max_clusters:
        optimal_k = max_clusters
        
    # Perform spectral clustering with the optimal k
    sc = SpectralClustering(n_clusters=optimal_k, affinity='precomputed', assign_labels='kmeans')
    labels = sc.fit_predict(affinity_matrix)
    
    return optimal_k, labels
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/dynamical_systems/__init__.py</path>
      <content><![CDATA[from .find_fixed_points import find_fixed_points
from .calculate_jacobian import calculate_jacobian
from .analyze_stability import analyze_stability
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/dynamical_systems/analyze_stability.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Dict, Any

def analyze_stability(
    jacobian: np.ndarray
) -> Dict[str, Any]:
    """
    Analyzes the stability of a fixed point by examining the eigenvalues of the Jacobian.

    Parameters
    ----------
    jacobian : np.ndarray
        The Jacobian matrix at the fixed point.

    Returns
    -------
    Dict[str, Any]
        A dictionary containing the stability analysis, including:
        - 'eigenvalues': The eigenvalues of the Jacobian.
        - 'stability_type': The type of stability (e.g., stable node, saddle point).
    """
    eigenvalues = np.linalg.eigvals(jacobian)
    
    real_parts = np.real(eigenvalues)
    imag_parts = np.imag(eigenvalues)

    if np.all(real_parts < 0):
        if np.all(imag_parts == 0):
            stability_type = "Stable Node"
        else:
            stability_type = "Stable Spiral"
    elif np.all(real_parts > 0):
        if np.all(imag_parts == 0):
            stability_type = "Unstable Node"
        else:
            stability_type = "Unstable Spiral"
    elif np.any(real_parts > 0) and np.any(real_parts < 0):
        stability_type = "Saddle Point"
    else:
        stability_type = "Center (Marginally Stable)"

    return {
        'eigenvalues': eigenvalues,
        'stability_type': stability_type
    }
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/dynamical_systems/calculate_jacobian.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Callable, Any

def calculate_jacobian(
    func: Callable[[np.ndarray, Any], np.ndarray],
    point: np.ndarray,
    epsilon: float = 1e-6
) -> np.ndarray:
    """
    Calculates the Jacobian matrix of a dynamical system at a given point.

    Parameters
    ----------
    func : Callable[[np.ndarray, Any], np.ndarray]
        A function representing the dynamical system.
    point : np.ndarray
        The point at which to calculate the Jacobian.
    epsilon : float, optional
        The step size for the finite difference approximation, by default 1e-6.

    Returns
    -------
    np.ndarray
        The Jacobian matrix at the given point.
    """
    n = len(point)
    jacobian = np.zeros((n, n))
    f0 = func(point)
    for i in range(n):
        p_plus = point.copy()
        p_plus[i] += epsilon
        f_plus = func(p_plus)
        jacobian[:, i] = (f_plus - f0) / epsilon
    return jacobian
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/dynamical_systems/find_fixed_points.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from scipy.optimize import fsolve
from typing import Callable, List, Any

def find_fixed_points(
    func: Callable[[np.ndarray, Any], np.ndarray],
    initial_guesses: List[np.ndarray]
) -> np.ndarray:
    """
    Finds the fixed points (equilibria) of a dynamical system.

    Parameters
    ----------
    func : Callable[[np.ndarray, Any], np.ndarray]
        A function representing the dynamical system, where func(y, *args) = dy/dt.
    initial_guesses : List[np.ndarray]
        A list of initial guesses for the fixed points.

    Returns
    -------
    np.ndarray
        An array of the found fixed points.
    """
    fixed_points = []
    for guess in initial_guesses:
        fixed_point, _, _, _ = fsolve(func, guess, full_output=True)
        fixed_points.append(fixed_point)
    return np.array(fixed_points)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/evolutionary/__init__.py</path>
      <content><![CDATA[from .apply_mutation import apply_mutation
from .apply_recombination import apply_recombination
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/evolutionary/apply_mutation.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def apply_mutation(
    weights: np.ndarray,
    mutation_rate: float = 0.01,
    mutation_scale: float = 0.1
) -> np.ndarray:
    """
    Applies random mutations to a set of weights.

    Parameters
    ----------
    weights : np.ndarray
        The weights to mutate.
    mutation_rate : float, optional
        The probability of each weight being mutated, by default 0.01.
    mutation_scale : float, optional
        The standard deviation of the Gaussian noise to add to the weights,
        by default 0.1.

    Returns
    -------
    np.ndarray
        The mutated weights.
    """
    mutation_mask = np.random.rand(*weights.shape) < mutation_rate
    mutations = np.random.normal(0, mutation_scale, weights.shape)
    
    mutated_weights = weights.copy()
    mutated_weights[mutation_mask] += mutations[mutation_mask]
    
    return mutated_weights
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/evolutionary/apply_recombination.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def apply_recombination(
    weights1: np.ndarray,
    weights2: np.ndarray,
    recombination_prob: float = 0.5
) -> np.ndarray:
    """
    Performs crossover/recombination between two sets of weights.

    Parameters
    ----------
    weights1 : np.ndarray
        The first set of weights.
    weights2 : np.ndarray
        The second set of weights.
    recombination_prob : float, optional
        The probability of choosing a weight from the first set, by default 0.5.

    Returns
    -------
    np.ndarray
        The new set of weights after recombination.
    """
    if weights1.shape != weights2.shape:
        raise ValueError("Weight arrays must have the same shape.")
        
    recombination_mask = np.random.rand(*weights1.shape) < recombination_prob
    
    new_weights = weights1.copy()
    new_weights[~recombination_mask] = weights2[~recombination_mask]
    
    return new_weights
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/fractal_analysis/__init__.py</path>
      <content><![CDATA[from .calculate_fractal_dimension import calculate_fractal_dimension
from .fractal_spike_train import generate_fractal_spike_train
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/fractal_analysis/calculate_fractal_dimension.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def calculate_fractal_dimension(points: np.ndarray, threshold: float = 0.9) -> float:
    """
    Calculates the fractal dimension of a point set using the box-counting algorithm.

    Parameters
    ----------
    points : np.ndarray
        A numpy array of shape (n_points, n_features) representing the data.
    threshold : float, optional
        The threshold for the number of points in a box to be considered "filled",
        by default 0.9.

    Returns
    -------
    float
        The estimated fractal dimension.
    """
    # Find the bounding box of the point set
    min_coords = np.min(points, axis=0)
    max_coords = np.max(points, axis=0)
    
    # A list of scales to use
    scales = np.logspace(0.01, 1, num=10, endpoint=False, base=2)
    
    counts = []
    for scale in scales:
        box_size = (max_coords - min_coords) / scale
        
        # Create a grid of boxes
        grid = {}
        for point in points:
            box_index = tuple(np.floor((point - min_coords) / box_size).astype(int))
            if box_index not in grid:
                grid[box_index] = 0
            grid[box_index] += 1
            
        # Count the number of non-empty boxes
        counts.append(len(grid))
        
    # Fit a line to the log-log plot of counts vs. scales
    coeffs = np.polyfit(np.log(scales), np.log(counts), 1)
    
    return -coeffs[0]
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/fractal_analysis/fractal_spike_train.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def generate_fractal_spike_train(
    fractal_dimension: float,
    k: float,
    tau_f: float,
    duration: float,
    dt: float = 1.0
) -> np.ndarray:
    """
    Generates a spike train based on a fractal dynamics rule.

    spike_rate_i = k * D_f * e^(-t/τ_f)

    Parameters
    ----------
    fractal_dimension : float
        The fractal dimension D_f.
    k : float
        A scaling factor.
    tau_f : float
        The time constant for the exponential decay.
    duration : float
        The duration of the spike train to generate.
    dt : float, optional
        The time step, by default 1.0.

    Returns
    -------
    np.ndarray
        A 1D numpy array of spike times.
    """
    n_steps = int(duration / dt)
    time = np.arange(0, duration, dt)
    
    spike_rate = k * fractal_dimension * np.exp(-time / tau_f)
    
    # Generate spikes using a Poisson process with the given rate
    spike_train = np.random.rand(n_steps) < (spike_rate * dt / 1000.0) # Assuming dt is in ms
    
    return np.where(spike_train)[0] * dt
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/fractional_calculus/__init__.py</path>
      <content><![CDATA[from .caputo_derivative import caputo_derivative
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/fractional_calculus/caputo_derivative.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from scipy.special import gamma

def caputo_derivative(
    f: np.ndarray, 
    alpha: float, 
    dt: float = 1.0
) -> np.ndarray:
    """
    Calculates the Caputo fractional derivative of a time series.

    This implementation uses the Grunwald-Letnikov formula.

    Parameters
    ----------
    f : np.ndarray
        A 1D numpy array representing the time series.
    alpha : float
        The order of the fractional derivative (0 < alpha < 1).
    dt : float, optional
        The time step between samples, by default 1.0.

    Returns
    -------
    np.ndarray
        The Caputo fractional derivative of the time series.
    """
    n = len(f)
    result = np.zeros(n)
    
    for i in range(n):
        summation = 0
        for k in range(i + 1):
            # Grunwald-Letnikov coefficients
            coeff = (gamma(k - alpha) / (gamma(-alpha) * gamma(k + 1)))
            summation += coeff * f[i - k]
        result[i] = summation / (dt**alpha)
        
    return result
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/graph/__init__.py</path>
      <content><![CDATA[from .graph_theory_toolkit import calculate_graph_metrics, detect_communities
from .advanced_graph_analysis import calculate_graph_edit_distance, calculate_pagerank
from .coarse_grain_graph import coarse_grain_graph
from .graph_dynamics import simulate_sparsity_evolution, calculate_path_score
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/graph/advanced_graph_analysis.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import networkx as nx
from typing import Dict

def calculate_graph_edit_distance(graph1: nx.Graph, graph2: nx.Graph) -> float:
    """
    Calculates the graph edit distance between two graphs.

    Note: This is a computationally expensive operation.
    
    Parameters
    ----------
    graph1 : nx.Graph
        The first graph.
    graph2 : nx.Graph
        The second graph.

    Returns
    -------
    float
        The graph edit distance between the two graphs.
    """
    return nx.graph_edit_distance(graph1, graph2)

def calculate_pagerank(graph: nx.Graph, alpha: float = 0.85) -> Dict[Any, float]:
    """
    Calculates the PageRank of the nodes in a graph.

    Parameters
    ----------
    graph : nx.Graph
        A NetworkX graph object.
    alpha : float, optional
        The damping parameter for PageRank, by default 0.85.

    Returns
    -------
    Dict[Any, float]
        A dictionary where keys are nodes and values are their PageRank scores.
    """
    return nx.pagerank(graph, alpha=alpha)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/graph/coarse_grain_graph.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import networkx as nx
from typing import List

def coarse_grain_graph(
    graph: nx.Graph,
    partitions: List[List[int]]
) -> nx.Graph:
    """
    Coarse-grains a graph by merging nodes based on a given partition.

    Parameters
    ----------
    graph : nx.Graph
        The original graph.
    partitions : List[List[int]]
        A list of lists, where each inner list contains the nodes of a partition.

    Returns
    -------
    nx.Graph
        The coarse-grained graph.
    """
    coarse_graph = nx.Graph()
    
    # Add nodes to the coarse graph, one for each partition
    for i, partition in enumerate(partitions):
        coarse_graph.add_node(i, members=partition)
        
    # Add edges to the coarse graph
    for i in range(len(partitions)):
        for j in range(i + 1, len(partitions)):
            weight = 0
            for u in partitions[i]:
                for v in partitions[j]:
                    if graph.has_edge(u, v):
                        weight += graph[u][v].get('weight', 1.0)
            if weight > 0:
                coarse_graph.add_edge(i, j, weight=weight)
                
    return coarse_graph
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/graph/graph_dynamics.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def simulate_sparsity_evolution(
    initial_sparsity: float,
    kappa: float,
    mu: float,
    spike_events: np.ndarray,
    dt: float,
    t_max: float
) -> np.ndarray:
    """
    Simulates the evolution of graph sparsity over time.

    ds/dt = -κ * s * (1 - s) + μ * Σspike_t

    Parameters
    ----------
    initial_sparsity : float
        The initial sparsity of the graph.
    kappa : float
        The decay parameter.
    mu : float
        The growth parameter.
    spike_events : np.ndarray
        A 1D array of the number of spike events at each time step.
    dt : float
        The time step.
    t_max : float
        The maximum simulation time.

    Returns
    -------
    np.ndarray
        An array of the sparsity at each time step.
    """
    n_steps = int(t_max / dt)
    sparsity = np.zeros(n_steps + 1)
    sparsity[0] = initial_sparsity

    for i in range(n_steps):
        ds_dt = -kappa * sparsity[i] * (1 - sparsity[i]) + mu * spike_events[i]
        sparsity[i+1] = sparsity[i] + ds_dt * dt
        
    return sparsity

def calculate_path_score(
    weights: np.ndarray,
    spike_times: np.ndarray,
    distances: np.ndarray,
    lambda_reg: float
) -> float:
    """
    Calculates a score for a path in the graph.

    path_score = Σw_ij * spike_t * e^(-d_ij/λ)

    Parameters
    ----------
    weights : np.ndarray
        The weights of the edges in the path.
    spike_times : np.ndarray
        The spike times at each node in the path.
    distances : np.ndarray
        The distances between nodes in the path.
    lambda_reg : float
        The distance decay parameter.

    Returns
    -------
    float
        The calculated path score.
    """
    path_score = 0.0
    for i in range(len(weights)):
        path_score += weights[i] * spike_times[i] * np.exp(-distances[i] / lambda_reg)
    return path_score
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/graph/graph_theory_toolkit.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import networkx as nx
from typing import Dict, Any, List
import numpy as np

def calculate_graph_metrics(graph: nx.Graph) -> Dict[str, Any]:
    """
    Calculates a set of standard metrics for a given graph.

    Parameters
    ----------
    graph : nx.Graph
        A NetworkX graph object.

    Returns
    -------
    Dict[str, Any]
        A dictionary containing the calculated graph metrics, including:
        - 'num_nodes': Number of nodes.
        - 'num_edges': Number of edges.
        - 'density': The density of the graph.
        - 'avg_degree': The average degree of the nodes.
        - 'avg_clustering_coefficient': The average clustering coefficient.
        - 'avg_shortest_path_length': The average shortest path length (if connected).
    """
    if not isinstance(graph, nx.Graph):
        raise TypeError("Input must be a NetworkX graph.")

    metrics = {}
    metrics['num_nodes'] = graph.number_of_nodes()
    metrics['num_edges'] = graph.number_of_edges()
    metrics['density'] = nx.density(graph)
    
    degrees = [d for n, d in graph.degree()]
    if degrees:
        metrics['avg_degree'] = np.mean(degrees)
    else:
        metrics['avg_degree'] = 0

    metrics['avg_clustering_coefficient'] = nx.average_clustering(graph)

    if nx.is_connected(graph):
        metrics['avg_shortest_path_length'] = nx.average_shortest_path_length(graph)
    else:
        metrics['avg_shortest_path_length'] = float('inf') # Or handle disconnected components separately

    return metrics

def detect_communities(graph: nx.Graph, method: str = 'louvain') -> List[List[Any]]:
    """
    Detects communities in a graph using a specified algorithm.

    Parameters
    ----------
    graph : nx.Graph
        A NetworkX graph object.
    method : str, optional
        The community detection algorithm to use. Currently, only 'louvain' is supported.
        Default is 'louvain'.

    Returns
    -------
    List[List[Any]]
        A list of lists, where each inner list contains the nodes of a community.
    """
    if not isinstance(graph, nx.Graph):
        raise TypeError("Input must be a NetworkX graph.")

    if method == 'louvain':
        # Note: The 'louvain' algorithm is in the 'community' package, which is a separate dependency.
        # For simplicity, we will use the greedy modularity maximization from NetworkX,
        # which is similar in spirit.
        try:
            from networkx.algorithms.community import greedy_modularity_communities
            communities = list(greedy_modularity_communities(graph))
            return [list(c) for c in communities]
        except ImportError:
            raise ImportError("The 'louvain' method requires the 'python-louvain' or 'community' package, or use networkx's greedy_modularity_communities.")
    else:
        raise ValueError(f"Method '{method}' is not supported. Currently, only 'louvain' is available.")
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/iit/__init__.py</path>
      <content><![CDATA[from .calculate_simplified_phi import calculate_simplified_phi
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/iit/calculate_simplified_phi.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Dict

def calculate_simplified_phi(
    weights: np.ndarray,
    spike_rates: np.ndarray,
    spike_probabilities: np.ndarray
) -> float:
    """
    Calculates a simplified version of the integrated information (Φ) metric.

    Φ = Σ_i log(1 + I(i,j) / H(j))
    where:
    I(i,j) = w_ij * spike_rate_j
    H(j) = -Σ p(spike_j) log p(spike_j)

    Parameters
    ----------
    weights : np.ndarray
        A 2D numpy array of synaptic weights (w_ij).
    spike_rates : np.ndarray
        A 1D numpy array of spike rates for each neuron.
    spike_probabilities : np.ndarray
        A 1D numpy array of spike probabilities for each neuron.

    Returns
    -------
    float
        The calculated simplified Φ value.
    """
    n_neurons = weights.shape[0]
    phi = 0.0

    for j in range(n_neurons):
        # Calculate entropy H(j)
        p_j = spike_probabilities[j]
        if p_j > 0 and p_j < 1:
            h_j = -(p_j * np.log2(p_j) + (1 - p_j) * np.log2(1 - p_j))
        else:
            h_j = 0

        if h_j > 0:
            for i in range(n_neurons):
                # Calculate information I(i,j)
                i_ij = weights[i, j] * spike_rates[j]
                
                # Add to total Phi
                phi += np.log2(1 + np.abs(i_ij) / h_j)
                
    return phi
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/info_theory/__init__.py</path>
      <content><![CDATA[from .information_theory import calculate_entropy, calculate_mutual_information, calculate_kl_divergence
from .information_bottleneck import information_bottleneck
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/info_theory/information_bottleneck.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from .information_theory import calculate_mutual_information

def information_bottleneck(
    p_xy: np.ndarray,
    p_xt: np.ndarray,
    beta: float
) -> float:
    """
    Calculates the Information Bottleneck objective function.

    L(T) = I(X;T) - β * I(X;Y)

    Note: This function calculates the objective for a given set of distributions.
    The optimization of this objective is a separate, more complex task.

    Parameters
    ----------
    p_xy : np.ndarray
        The joint probability distribution P(X, Y).
    p_xt : np.ndarray
        The joint probability distribution P(X, T), where T is the compressed
        representation of X.
    beta : float
        The Lagrange multiplier that balances compression and information
        preservation.

    Returns
    -------
    float
        The value of the Information Bottleneck objective function.
    """
    i_xt = calculate_mutual_information(p_xt)
    i_xy = calculate_mutual_information(p_xy)
    
    return i_xt - beta * i_xy
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/info_theory/information_theory.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from scipy.stats import entropy as scipy_entropy

def calculate_entropy(pk: np.ndarray, base: int = 2) -> float:
    """
    Calculates the Shannon entropy of a discrete probability distribution.

    Parameters
    ----------
    pk : np.ndarray
        A 1D numpy array representing the probability distribution.
        Must sum to 1.
    base : int, optional
        The logarithmic base to use, by default 2.

    Returns
    -------
    float
        The Shannon entropy of the distribution.
    """
    if not np.isclose(np.sum(pk), 1.0):
        raise ValueError("Probabilities must sum to 1.")
    return scipy_entropy(pk, base=base)

def calculate_mutual_information(
    p_xy: np.ndarray, base: int = 2
) -> float:
    """
    Calculates the mutual information between two random variables.

    Parameters
    ----------
    p_xy : np.ndarray
        A 2D numpy array representing the joint probability distribution
        P(X, Y).
    base : int, optional
        The logarithmic base to use, by default 2.

    Returns
    -------
    float
        The mutual information I(X; Y).
    """
    if not np.isclose(np.sum(p_xy), 1.0):
        raise ValueError("Probabilities must sum to 1.")
    
    p_x = np.sum(p_xy, axis=1)
    p_y = np.sum(p_xy, axis=0)
    
    p_x_p_y = np.outer(p_x, p_y)
    
    # We need to avoid log(0) for cases where p_xy is 0.
    # The contribution to the sum is 0 in these cases.
    non_zero = p_xy > 0
    
    return np.sum(p_xy[non_zero] * np.log(p_xy[non_zero] / p_x_p_y[non_zero])) / np.log(base)

def calculate_kl_divergence(
    p: np.ndarray, q: np.ndarray, base: int = 2
) -> float:
    """
    Calculates the Kullback-Leibler (KL) divergence between two
    discrete probability distributions.

    Parameters
    ----------
    p : np.ndarray
        A 1D numpy array representing the true probability distribution.
    q : np.ndarray
        A 1D numpy array representing the approximate probability distribution.
    base : int, optional
        The logarithmic base to use, by default 2.

    Returns
    -------
    float
        The KL divergence D_KL(P || Q).
    """
    if not np.isclose(np.sum(p), 1.0) or not np.isclose(np.sum(q), 1.0):
        raise ValueError("Probabilities must sum to 1.")
    if len(p) != len(q):
        raise ValueError("Distributions must have the same length.")

    return scipy_entropy(p, q, base=base)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/linear_system_solver.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def linear_system_solver(matrix_A, vector_b):
    """
    Solves a system of linear equations in the form Ax = b.
    
    Parameters
    ----------
    matrix_A : array_like
        Coefficient matrix of the linear system. Must be square and non-singular.
        
    vector_b : array_like
        Right-hand side vector of the linear system.
        
    Returns
    -------
    np.ndarray
        Solution vector x that satisfies Ax = b.
        
    Raises
    ------
    TypeError
        If matrix_A is not a 2D array or vector_b is not a 1D array.
        If inputs contain non-numeric data.
    ValueError
        If matrix_A is not square or if the shapes of matrix_A and vector_b are incompatible.
        If inputs contain NaN or infinity values.
    np.linalg.LinAlgError
        If matrix_A is singular or the computation does not converge.
        
    Examples
    --------
    >>> import numpy as np
    >>> # Define a simple 2x2 system: 3x + 2y = 7, x + y = 3
    >>> A = np.array([[3, 2], [1, 1]])
    >>> b = np.array([7, 3])
    >>> x = linear_system_solver(A, b)
    >>> print(f"Solution: x = {x[0]}, y = {x[1]}")
    Solution: x = 1.0, y = 2.0
    
    Notes
    -----
    This function uses numpy.linalg.solve to compute the solution to the linear
    system Ax = b. The solution is computed using LU factorization.
    """
    # Check if inputs are provided
    if matrix_A is None:
        raise ValueError("matrix_A cannot be None")
    if vector_b is None:
        raise ValueError("vector_b cannot be None")
    
    # Convert inputs to numpy arrays
    try:
        matrix_A = np.asarray(matrix_A, dtype=float)
        vector_b = np.asarray(vector_b, dtype=float)
    except (ValueError, TypeError) as e:
        raise TypeError(f"Failed to convert inputs to numeric arrays: {str(e)}")
    
    # Check for NaN or infinity values
    if np.isnan(matrix_A).any() or np.isinf(matrix_A).any():
        raise ValueError("matrix_A contains NaN or infinity values")
    if np.isnan(vector_b).any() or np.isinf(vector_b).any():
        raise ValueError("vector_b contains NaN or infinity values")
    
    # Input validation for dimensions
    if matrix_A.ndim != 2:
        raise TypeError(f"matrix_A must be a 2D array, got {matrix_A.ndim}D array")
    
    if vector_b.ndim != 1:
        raise TypeError(f"vector_b must be a 1D array, got {vector_b.ndim}D array")
    
    # Input validation for shape
    if matrix_A.shape[0] != matrix_A.shape[1]:
        raise ValueError(f"matrix_A must be square, got shape {matrix_A.shape}")
    
    if matrix_A.shape[0] != vector_b.shape[0]:
        raise ValueError(f"Incompatible shapes: matrix_A has {matrix_A.shape[0]} rows, but vector_b has {vector_b.shape[0]} elements")
    
    # Check if matrix is empty
    if matrix_A.size == 0 or vector_b.size == 0:
        raise ValueError("Empty arrays are not valid inputs")
    
    # Solve the linear system
    try:
        x = np.linalg.solve(matrix_A, vector_b)
        return x
    except np.linalg.LinAlgError as e:
        if "singular" in str(e).lower():
            raise np.linalg.LinAlgError("The coefficient matrix is singular. The system has no unique solution.") from e
        else:
            raise np.linalg.LinAlgError(f"Failed to solve the linear system: {str(e)}") from e
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/neuro/README.md</path>
      <content><![CDATA[# IMPORTANT
### Deprecated Features
This folder contains implementations for stdp and stc. STC is now deprecated in favor of much more efficient strategies. STDP is now replaced with Resonance Enhanced Valenve Gated Synaptic Plasticity.

This logic will need to be replaced.]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/neuro/__init__.py</path>
      <content><![CDATA[from .apply_stdp import apply_stdp
from .apply_stc import apply_stc
from .advanced_sie import calculate_stabilized_reward, apply_quadratic_stdp_modulation
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/neuro/advanced_sie.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def calculate_stabilized_reward(
    td_error: float,
    novelty: float,
    habituation: float,
    self_benefit: float,
    external_reward: float,
    w_r_base: float = 0.6,
    w_n: float = 0.3,
    w_s: float = 0.1,
    lambda_reg: float = 0.05
) -> float:
    """
    Calculates a stabilized multi-objective reward function.

    R_tot(t) = w_r * TD_error(t) + w_n * novelty(t) * (1 - tanh(habituation(t))) + w_s * self_benefit(t)
    where w_r = w_r_base * e^(-λ * |external_reward|)

    Parameters
    ----------
    td_error : float
    novelty : float
    habituation : float
    self_benefit : float
    external_reward : float
    w_r_base : float, optional
    w_n : float, optional
    w_s : float, optional
    lambda_reg : float, optional

    Returns
    -------
    float
        The calculated stabilized reward.
    """
    w_r = w_r_base * np.exp(-lambda_reg * np.abs(external_reward))
    
    reward = (w_r * td_error + 
              w_n * novelty * (1 - np.tanh(habituation)) + 
              w_s * self_benefit)
              
    return reward

def apply_quadratic_stdp_modulation(
    eta_base: float = 0.12,
    beta: float = 0.15,
    tau: float = 15.0,
    delta_t: float = 0.0,
    total_reward: float = 0.0
) -> float:
    """
    Calculates the STDP weight change with quadratic reward modulation.

    Δw_ij = η * (1 + β * R_tot^2) * e^(-Δt/τ)

    Parameters
    ----------
    eta_base : float, optional
    beta : float, optional
    tau : float, optional
    delta_t : float, optional
    total_reward : float, optional

    Returns
    -------
    float
        The calculated STDP weight change.
    """
    eta = eta_base * (1 + beta * total_reward**2)
    return eta * np.exp(-delta_t / tau)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/neuro/apply_stc.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Tuple

def apply_stc(
    current_weight: float,
    eligibility_trace: float,
    synaptic_tag: float,
    reward: float,
    tag_decay: float = 0.9,
    consolidation_threshold: float = 0.7,
    consolidation_rate: float = 0.1
) -> Tuple[float, float, float]:
    """
    Applies a Synaptic Tagging and Capture (STC) rule.

    Parameters
    ----------
    current_weight : float
        The current synaptic weight.
    eligibility_trace : float
        The current eligibility trace for the synapse (e.g., from STDP).
    synaptic_tag : float
        The current synaptic tag value.
    reward : float
        The global or local reward signal.
    tag_decay : float, optional
        The decay rate of the synaptic tag, by default 0.9.
    consolidation_threshold : float, optional
        The reward threshold for triggering consolidation, by default 0.7.
    consolidation_rate : float, optional
        The rate at which the weight is consolidated, by default 0.1.

    Returns
    -------
    Tuple[float, float, float]
        A tuple containing:
        - The updated synaptic weight.
        - The updated eligibility trace (reset after use).
        - The updated synaptic tag.
    """
    # Update the synaptic tag based on the eligibility trace
    new_tag = tag_decay * synaptic_tag + eligibility_trace

    # Check for consolidation
    if reward > consolidation_threshold:
        # Consolidate the weight based on the tag
        new_weight = current_weight + consolidation_rate * new_tag
        # Reset the tag after consolidation
        new_tag = 0.0
    else:
        new_weight = current_weight

    # Reset the eligibility trace
    new_eligibility_trace = 0.0
    
    return new_weight, new_eligibility_trace, new_tag
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/neuro/apply_stdp.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Tuple, List, Union, Optional

def apply_stdp(
    spike_times_pre: Union[List[float], np.ndarray],
    spike_times_post: Union[List[float], np.ndarray],
    current_weight: float,
    is_inhibitory: bool = False,
    A_plus_base: float = 0.1,
    A_minus_base: float = 0.12,
    tau_plus: float = 20.0,
    tau_minus: float = 20.0,
    eligibility_trace: float = 0.0,
    gamma: float = 0.9,
    cluster_reward: float = 0.0,
    max_reward: float = 1.0,
    spike_rate_pre: float = 0.0,
    dt: float = 1.0,
    target_rate: float = 0.3,
    eta: float = 1.0,
    A_plus_inh: Optional[float] = None,
    A_minus_inh: Optional[float] = None,
    tau_plus_inh: Optional[float] = None,
    tau_minus_inh: Optional[float] = None,
    weight_bounds: Optional[Tuple[float, float]] = None
) -> Tuple[float, float]:
    """
    Applies Spike-Timing-Dependent Plasticity (STDP) rules to update synaptic weights
    based on the relative timing of pre- and post-synaptic spikes.
    
    Parameters
    ----------
    spike_times_pre : array_like
        List or array of spike times for the pre-synaptic neuron (in ms).
    
    spike_times_post : array_like
        List or array of spike times for the post-synaptic neuron (in ms).
    
    current_weight : float
        The current synaptic weight (w_ij) between the neurons.
    
    is_inhibitory : bool, optional
        Flag indicating whether the synapse is inhibitory (True) or excitatory (False).
        Default is False (excitatory).
    
    A_plus_base : float, optional
        Base potentiation strength for excitatory synapses. Default is 0.1.
    
    A_minus_base : float, optional
        Base depression strength for excitatory synapses. Default is 0.12.
    
    tau_plus : float, optional
        Time constant for potentiation (in ms) for excitatory synapses. Default is 20.0.
    
    tau_minus : float, optional
        Time constant for depression (in ms) for excitatory synapses. Default is 20.0.
    
    eligibility_trace : float, optional
        The current eligibility trace for the synapse. Default is 0.0.
    
    gamma : float, optional
        Decay factor for the eligibility trace. Default is 0.9.
    
    cluster_reward : float, optional
        The reward signal specific to the post-synaptic neuron's cluster. Default is 0.0.
    
    max_reward : float, optional
        The maximum possible reward value. Default is 1.0.
    
    spike_rate_pre : float, optional
        The recent firing rate of the pre-synaptic neuron (in Hz). Default is 0.0.
    
    dt : float, optional
        The time step in ms. Default is 1.0.
    
    target_rate : float, optional
        Target firing rate for homeostatic regulation (in Hz). Default is 0.3.
    
    eta : float, optional
        Base learning rate. Default is 1.0.
    
    A_plus_inh : float, optional
        Potentiation strength for inhibitory synapses. If None, uses A_plus_base.
    
    A_minus_inh : float, optional
        Depression strength for inhibitory synapses. If None, uses A_minus_base.
    
    tau_plus_inh : float, optional
        Time constant for potentiation (in ms) for inhibitory synapses. If None, uses tau_plus.
    
    tau_minus_inh : float, optional
        Time constant for depression (in ms) for inhibitory synapses. If None, uses tau_minus.
    
    weight_bounds : tuple of float, optional
        Minimum and maximum allowed values for the synaptic weight.
        If None, uses (0.0, 1.0) for excitatory and (-1.0, 0.0) for inhibitory synapses.
    
    Returns
    -------
    Tuple[float, float]
        A tuple containing:
        - The updated synaptic weight (w_ij)
        - The updated eligibility trace (e_ij)
    
    Raises
    ------
    ValueError
        If input parameters are invalid (e.g., negative time constants).
    TypeError
        If input parameters have incorrect types.
    
    Notes
    -----
    This function implements the STDP rules for the FUM (Fully Unified Model) project,
    handling both excitatory and inhibitory synapses, eligibility traces, and parameter
    heterogeneity as described in the FUM documentation.
    
    The STDP rule for excitatory synapses is:
    - Δw_ij = A_+ * exp(-Δt / τ_+) if Δt > 0 (pre precedes post, potentiation)
    - Δw_ij = A_- * exp(Δt / τ_-) if Δt < 0 (post precedes pre, depression)
    
    The STDP rule for inhibitory synapses is the reverse:
    - Δw_ij = A_+_inh * exp(Δt / τ_+_inh) if Δt < 0 (post precedes pre, potentiation)
    - Δw_ij = A_-_inh * exp(-Δt / τ_-_inh) if Δt > 0 (pre precedes post, depression)
    
    The eligibility trace is updated as:
    - e_ij(t+dt) = gamma * e_ij(t) + Δw_ij
    
    Examples
    --------
    >>> # Example for excitatory synapse
    >>> spike_times_pre = [10.0, 20.0, 30.0]
    >>> spike_times_post = [15.0, 25.0, 35.0]
    >>> current_weight = 0.5
    >>> new_weight, new_trace = apply_stdp(spike_times_pre, spike_times_post, current_weight)
    >>> print(f"New weight: {new_weight:.4f}, New trace: {new_trace:.4f}")
    
    >>> # Example for inhibitory synapse
    >>> spike_times_pre = [10.0, 20.0, 30.0]
    >>> spike_times_post = [5.0, 15.0, 25.0]
    >>> current_weight = -0.5
    >>> new_weight, new_trace = apply_stdp(
    ...     spike_times_pre, spike_times_post, current_weight, 
    ...     is_inhibitory=True, cluster_reward=0.5
    ... )
    >>> print(f"New weight: {new_weight:.4f}, New trace: {new_trace:.4f}")
    """
    # Input validation
    # Check types
    if not isinstance(spike_times_pre, (list, np.ndarray)):
        raise TypeError("spike_times_pre must be a list or numpy array")
    if not isinstance(spike_times_post, (list, np.ndarray)):
        raise TypeError("spike_times_post must be a list or numpy array")
    if not isinstance(current_weight, (int, float)):
        raise TypeError("current_weight must be a number")
    if not isinstance(is_inhibitory, bool):
        raise TypeError("is_inhibitory must be a boolean")
    
    # Check values
    if not (isinstance(A_plus_base, (int, float)) and A_plus_base > 0):
        raise ValueError("A_plus_base must be a positive number")
    if not (isinstance(A_minus_base, (int, float)) and A_minus_base > 0):
        raise ValueError("A_minus_base must be a positive number")
    if not (isinstance(tau_plus, (int, float)) and tau_plus > 0):
        raise ValueError("tau_plus must be a positive number")
    if not (isinstance(tau_minus, (int, float)) and tau_minus > 0):
        raise ValueError("tau_minus must be a positive number")
    if not isinstance(eligibility_trace, (int, float)):
        raise TypeError("eligibility_trace must be a number")
    if not (isinstance(gamma, (int, float)) and 0 <= gamma <= 1):
        raise ValueError("gamma must be a number between 0 and 1")
    if not (isinstance(cluster_reward, (int, float)) and cluster_reward >= 0):
        raise ValueError("cluster_reward must be a non-negative number")
    if not (isinstance(max_reward, (int, float)) and max_reward > 0):
        raise ValueError("max_reward must be a positive number")
    if cluster_reward > max_reward:
        raise ValueError("cluster_reward cannot exceed max_reward")
    if not (isinstance(spike_rate_pre, (int, float)) and spike_rate_pre >= 0):
        raise ValueError("spike_rate_pre must be a non-negative number")
    if not (isinstance(dt, (int, float)) and dt > 0):
        raise ValueError("dt must be a positive number")
    if not (isinstance(target_rate, (int, float)) and target_rate > 0):
        raise ValueError("target_rate must be a positive number")
    if not (isinstance(eta, (int, float)) and eta > 0):
        raise ValueError("eta must be a positive number")
    
    # Check optional parameters if provided
    if A_plus_inh is not None and not (isinstance(A_plus_inh, (int, float)) and A_plus_inh > 0):
        raise ValueError("A_plus_inh must be a positive number")
    if A_minus_inh is not None and not (isinstance(A_minus_inh, (int, float)) and A_minus_inh > 0):
        raise ValueError("A_minus_inh must be a positive number")
    if tau_plus_inh is not None and not (isinstance(tau_plus_inh, (int, float)) and tau_plus_inh > 0):
        raise ValueError("tau_plus_inh must be a positive number")
    if tau_minus_inh is not None and not (isinstance(tau_minus_inh, (int, float)) and tau_minus_inh > 0):
        raise ValueError("tau_minus_inh must be a positive number")
    
    # Check weight bounds if provided
    if weight_bounds is not None:
        if not isinstance(weight_bounds, tuple) or len(weight_bounds) != 2:
            raise TypeError("weight_bounds must be a tuple of (min_weight, max_weight)")
        if not (isinstance(weight_bounds[0], (int, float)) and isinstance(weight_bounds[1], (int, float))):
            raise TypeError("weight_bounds values must be numbers")
        if weight_bounds[0] >= weight_bounds[1]:
            raise ValueError("weight_bounds[0] must be less than weight_bounds[1]")
    
    # Check consistency between weight and inhibitory flag
    if is_inhibitory and current_weight > 0:
        raise ValueError("Inhibitory synapses must have negative weights")
    if not is_inhibitory and current_weight < 0:
        raise ValueError("Excitatory synapses must have positive weights")
    
    # Convert inputs to numpy arrays if they aren't already
    spike_times_pre = np.asarray(spike_times_pre)
    spike_times_post = np.asarray(spike_times_post)
    
    # Set default weight bounds if not provided
    if weight_bounds is None:
        if is_inhibitory:
            weight_bounds = (-1.0, 0.0)
        else:
            weight_bounds = (0.0, 1.0)
    
    # Initialize weight change
    delta_w = 0.0
    
    # Implement STDP rules for excitatory synapses
    if not is_inhibitory:
        # Modulate A_plus based on cluster reward and pre-synaptic firing rate
        A_plus = A_plus_base * (cluster_reward / max_reward)
        
        # Homeostatic regulation based on pre-synaptic firing rate
        if spike_rate_pre > 0:
            A_plus *= spike_rate_pre / target_rate
        
        # Vectorized implementation for excitatory synapses
        if len(spike_times_pre) > 0 and len(spike_times_post) > 0:
            # Compute all pairwise time differences (Δt = t_post - t_pre)
            delta_t_matrix = np.subtract.outer(spike_times_post, spike_times_pre)
            
            # Potentiation: when pre-synaptic spike precedes post-synaptic spike (Δt > 0)
            potentiation_mask = delta_t_matrix > 0
            if np.any(potentiation_mask):
                potentiation_values = A_plus * np.exp(-delta_t_matrix[potentiation_mask] / tau_plus)
                delta_w += np.sum(potentiation_values)
            
            # Depression: when post-synaptic spike precedes pre-synaptic spike (Δt < 0)
            depression_mask = delta_t_matrix < 0
            if np.any(depression_mask):
                depression_values = A_minus_base * np.exp(delta_t_matrix[depression_mask] / tau_minus)
                delta_w -= np.sum(depression_values)
    
    # Implement STDP rules for inhibitory synapses
    elif is_inhibitory:
        # Set inhibitory parameters if not provided
        if A_plus_inh is None:
            A_plus_inh = A_plus_base
        if A_minus_inh is None:
            A_minus_inh = A_minus_base
        if tau_plus_inh is None:
            tau_plus_inh = tau_plus
        if tau_minus_inh is None:
            tau_minus_inh = tau_minus
        
        # Vectorized implementation for inhibitory synapses
        if len(spike_times_pre) > 0 and len(spike_times_post) > 0:
            # Compute all pairwise time differences (Δt = t_post - t_pre)
            delta_t_matrix = np.subtract.outer(spike_times_post, spike_times_pre)
            
            # For inhibitory synapses, the timing dependency is reversed:
            # Depression: when pre-synaptic spike precedes post-synaptic spike (Δt > 0)
            depression_mask = delta_t_matrix > 0
            if np.any(depression_mask):
                depression_values = A_minus_inh * np.exp(-delta_t_matrix[depression_mask] / tau_minus_inh)
                delta_w += np.sum(depression_values)
            
            # Potentiation: when post-synaptic spike precedes pre-synaptic spike (Δt < 0)
            potentiation_mask = delta_t_matrix < 0
            if np.any(potentiation_mask):
                potentiation_values = A_plus_inh * np.exp(delta_t_matrix[potentiation_mask] / tau_plus_inh)
                delta_w -= np.sum(potentiation_values)
    
    # Implement eligibility trace integration
    # Update the eligibility trace: e_ij(t+dt) = gamma * e_ij(t) + Δw_ij
    new_eligibility_trace = gamma * eligibility_trace + delta_w
    
    # Apply SIE modulation (learning rate modulation)
    # Δw_ij = eta_effective * Δw_ij
    delta_w = eta * delta_w
    
    # Update the weight based on the eligibility trace and apply bounds
    new_weight = current_weight + delta_w
    
    # Apply weight bounds
    new_weight = np.clip(new_weight, weight_bounds[0], weight_bounds[1])
    
    return new_weight, new_eligibility_trace
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/numerical_integrate.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from scipy import integrate
from typing import Callable, Tuple, Optional, Any, Union

def numerical_integrate(
    func: Callable[[float], float],
    a: float,
    b: float,
    args: Tuple = ()
) -> Tuple[float, float]:
    """
    Numerically calculate the definite integral of a given function over a specified interval.
    
    This function uses scipy.integrate.quad to compute the definite integral of func(x)
    from a to b. It provides a robust interface to standard numerical integration libraries.
    
    Parameters
    ----------
    func : callable
        The function to integrate. Must be a Python callable that accepts a float
        as its first argument and returns a float. The function can have additional
        parameters which can be passed through the args parameter.
    
    a : float
        The lower limit of integration.
    
    b : float
        The upper limit of integration.
    
    args : tuple, optional
        Additional arguments to pass to the function. Default is an empty tuple.
    
    Returns
    -------
    Tuple[float, float]
        A tuple containing:
        - The computed value of the definite integral
        - The estimated absolute error in the result
    
    Raises
    ------
    TypeError
        If func is not callable, or if a or b are not numbers.
    ValueError
        If a is greater than or equal to b.
    RuntimeError
        If the integration fails to converge or encounters other numerical issues.
    
    Notes
    -----
    This function is a wrapper around scipy.integrate.quad, which uses adaptive
    quadrature methods to compute the integral with error control. The underlying
    algorithm is based on QUADPACK, a Fortran library for numerical integration.
    
    Examples
    --------
    >>> import numpy as np
    >>> # Simple integral of x^2 from 0 to 1 (equals 1/3)
    >>> result, error = numerical_integrate(lambda x: x**2, 0, 1)
    >>> print(f"Result: {result:.6f}, Error: {error:.6e}")
    Result: 0.333333, Error: 3.700743e-15
    
    >>> # Integral with additional parameters
    >>> def integrand(x, a, b):
    ...     return a * np.sin(b * x)
    >>> result, error = numerical_integrate(integrand, 0, np.pi, args=(2.0, 1.0))
    >>> print(f"Result: {result:.6f}, Error: {error:.6e}")
    Result: 4.000000, Error: 4.440892e-14
    
    >>> # Integral of a more complex function
    >>> result, error = numerical_integrate(lambda x: np.exp(-x**2), -np.inf, np.inf)
    >>> print(f"Result: {result:.6f}, Error: {error:.6e}")
    Result: 1.772454, Error: 1.420416e-14
    """
    # Input validation
    if not callable(func):
        raise TypeError("func must be a callable")
    
    if not isinstance(a, (int, float)):
        raise TypeError("Lower limit 'a' must be a number")
    
    if not isinstance(b, (int, float)):
        raise TypeError("Upper limit 'b' must be a number")
    
    if a >= b:
        raise ValueError("Lower limit 'a' must be less than upper limit 'b'")
    
    if not isinstance(args, tuple):
        raise TypeError("args must be a tuple")
    
    try:
        # Perform the numerical integration using scipy.integrate.quad
        result, error = integrate.quad(func, a, b, args=args)
        return result, error
    except Exception as e:
        # Handle potential errors from the integration
        if "singular" in str(e).lower():
            raise RuntimeError(f"Integration failed due to singularity in the function: {str(e)}")
        elif "converge" in str(e).lower():
            raise RuntimeError(f"Integration failed to converge: {str(e)}")
        else:
            raise RuntimeError(f"Integration failed: {str(e)}")
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/numerical_ode_solver.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from scipy.integrate import solve_ivp
from typing import Callable, Tuple, Union, List, Optional, Any

def numerical_ode_solver(
    fun: Callable,
    t_span: Tuple[float, float],
    y0: Union[List[float], np.ndarray],
    t_eval: Optional[Union[List[float], np.ndarray]] = None,
    args: Tuple = (),
    method: str = 'RK45',
    rtol: float = 1e-3,
    atol: float = 1e-6,
    max_step: Optional[float] = None
) -> Any:
    """
    Numerically solve an ordinary differential equation (ODE) initial value problem.
    
    This function solves a system of first-order ODEs:
        dy/dt = fun(t, y, *args)
    with initial conditions:
        y(t0) = y0
    
    Parameters
    ----------
    fun : callable
        Function that defines the ODE system. The calling signature is fun(t, y, *args).
        It must return an array-like with the same shape as y.
    
    t_span : tuple of float
        Interval of integration (t0, tf). The solver starts with t=t0 and integrates
        until it reaches t=tf.
    
    y0 : array_like
        Initial state. Must be a 1-D array or list of floats.
    
    t_eval : array_like or None, optional
        Times at which to store the computed solution. If None (default), the solver
        will choose the time points automatically.
    
    args : tuple, optional
        Extra arguments to pass to the function `fun`. Default is an empty tuple.
    
    method : str, optional
        Integration method to use. Options include:
        - 'RK45': Explicit Runge-Kutta method of order 5(4) (default)
        - 'RK23': Explicit Runge-Kutta method of order 3(2)
        - 'DOP853': Explicit Runge-Kutta method of order 8
        - 'Radau': Implicit Runge-Kutta method of the Radau IIA family of order 5
        - 'BDF': Implicit multi-step variable-order (1 to 5) method
        - 'LSODA': Adams/BDF method with automatic stiffness detection
    
    rtol : float, optional
        Relative tolerance for the solver. Default is 1e-3.
    
    atol : float, optional
        Absolute tolerance for the solver. Default is 1e-6.
    
    max_step : float or None, optional
        Maximum allowed step size. If None (default), the solver will determine it automatically.
    
    Returns
    -------
    sol : OdeResult
        Object with the following attributes:
        - t: array, times at which the solution was computed
        - y: array, values of the solution at corresponding times in t
        - sol: callable, interpolated solution
        - t_events, y_events: arrays (only if events were detected)
        - nfev, njev, nlu: number of evaluations of the right-hand side, Jacobian, LU decompositions
        - status: int, reason for algorithm termination
        - message: str, human-readable description of the termination reason
        - success: bool, whether the solver succeeded
    
    Raises
    ------
    ValueError
        If input parameters are invalid (e.g., t_span is not a 2-element tuple,
        y0 is not array-like, or method is not recognized).
    TypeError
        If input parameters have incorrect types.
    RuntimeError
        If the solver fails to converge or encounters other runtime issues.
    
    Examples
    --------
    Example 1: Simple exponential decay
    
    >>> def exponential_decay(t, y, rate_constant):
    ...     return -rate_constant * y
    ...
    >>> t_span = (0, 10)
    >>> y0 = [1.0]
    >>> rate_constant = 0.1
    >>> sol = numerical_ode_solver(exponential_decay, t_span, y0, args=(rate_constant,))
    >>> import matplotlib.pyplot as plt
    >>> plt.plot(sol.t, sol.y[0])
    >>> plt.xlabel('Time')
    >>> plt.ylabel('y(t)')
    >>> plt.title('Exponential Decay')
    
    Example 2: Lotka-Volterra predator-prey model
    
    >>> def lotka_volterra(t, z, a, b, c, d):
    ...     x, y = z
    ...     dx_dt = a * x - b * x * y
    ...     dy_dt = -c * y + d * x * y
    ...     return [dx_dt, dy_dt]
    ...
    >>> t_span = (0, 15)
    >>> y0 = [10, 5]  # Initial populations
    >>> params = (1.5, 1, 3, 1)  # a, b, c, d
    >>> t_eval = np.linspace(0, 15, 1000)  # Specific evaluation points
    >>> sol = numerical_ode_solver(lotka_volterra, t_span, y0, t_eval, args=params)
    >>> plt.plot(sol.t, sol.y[0], label='Prey')
    >>> plt.plot(sol.t, sol.y[1], label='Predator')
    >>> plt.xlabel('Time')
    >>> plt.ylabel('Population')
    >>> plt.legend()
    >>> plt.title('Lotka-Volterra Model')
    """
    # Input validation
    if not callable(fun):
        raise TypeError("The 'fun' parameter must be a callable function.")
    
    if not isinstance(t_span, tuple) or len(t_span) != 2:
        raise ValueError("The 't_span' parameter must be a tuple of two floats: (t0, tf).")
    
    if t_span[0] >= t_span[1]:
        raise ValueError("The 't_span' parameter must have t0 < tf.")
    
    try:
        y0_array = np.asarray(y0, dtype=float)
    except (TypeError, ValueError):
        raise TypeError("The 'y0' parameter must be array-like with numeric values.")
    
    if y0_array.ndim != 1:
        raise ValueError("The 'y0' parameter must be a 1-D array or list.")
    
    if t_eval is not None:
        try:
            t_eval_array = np.asarray(t_eval, dtype=float)
        except (TypeError, ValueError):
            raise TypeError("The 't_eval' parameter must be array-like with numeric values.")
    
    if not isinstance(args, tuple):
        raise TypeError("The 'args' parameter must be a tuple.")
    
    # Validate method parameter
    valid_methods = ['RK45', 'RK23', 'DOP853', 'Radau', 'BDF', 'LSODA']
    if method not in valid_methods:
        raise ValueError(f"Invalid method '{method}'. Valid methods are: {', '.join(valid_methods)}")
    
    # Validate tolerance parameters
    if not isinstance(rtol, (int, float)) or rtol <= 0:
        raise ValueError("The 'rtol' parameter must be a positive number.")
    
    if not isinstance(atol, (int, float)) or atol <= 0:
        raise ValueError("The 'atol' parameter must be a positive number.")
    
    if max_step is not None:
        if not isinstance(max_step, (int, float)):
            raise TypeError("The 'max_step' parameter must be a number or None.")
        if max_step <= 0:
            raise ValueError("The 'max_step' parameter must be positive when specified.")
    
    # Solve the ODE system
    try:
        # First, verify that the function works with the provided arguments
        try:
            test_result = fun(t_span[0], y0_array, *args)
            test_result_array = np.asarray(test_result, dtype=float)
            if test_result_array.shape != y0_array.shape:
                raise ValueError(
                    f"The function 'fun' returned an array of shape {test_result_array.shape}, "
                    f"but expected shape {y0_array.shape} matching the initial state 'y0'."
                )
        except Exception as e:
            if isinstance(e, ValueError) and "shape" in str(e):
                raise
            else:
                raise ValueError(
                    f"Error when testing the ODE function with initial conditions: {str(e)}. "
                    f"Make sure 'fun' accepts arguments (t, y, *args) and returns an array of the same shape as y."
                )
        
        # Now solve the ODE
        # Only include max_step in kwargs if it's not None to avoid SciPy's internal validation issues
        kwargs = {
            'fun': fun,
            't_span': t_span,
            'y0': y0_array,
            't_eval': t_eval,
            'args': args,
            'method': method,
            'rtol': rtol,
            'atol': atol
        }
        
        if max_step is not None:
            kwargs['max_step'] = max_step
            
        sol = solve_ivp(**kwargs)
        
        if not sol.success:
            raise RuntimeError(f"ODE solver failed: {sol.message}")
        
        return sol
        
    except Exception as e:
        # Handle specific exceptions from solve_ivp with more informative messages
        if "The solver successfully" in str(e):
            # This is actually a success message, not an error
            return sol
        elif "The solver could not" in str(e) or "Maximum number of steps" in str(e):
            raise RuntimeError(
                f"ODE solver failed to converge: {str(e)}. Try adjusting rtol, atol, or max_step parameters."
            )
        elif "Invalid initial condition" in str(e):
            raise ValueError(f"Invalid initial condition: {str(e)}")
        elif isinstance(e, RuntimeError) and "ODE solver failed" in str(e):
            raise
        elif isinstance(e, ValueError) and ("shape" in str(e) or "testing the ODE function" in str(e)):
            raise
        else:
            raise RuntimeError(f"Error in numerical_ode_solver: {str(e)}")
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/optimization/__init__.py</path>
      <content><![CDATA[from .bayesian_optimization import bayesian_optimization
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/optimization/bayesian_optimization.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Callable, List, Tuple, Dict, Any
from skopt import gp_minimize
from skopt.space import Real, Integer, Categorical

def bayesian_optimization(
    objective_func: Callable[[List[Any]], float],
    param_space: List[Dict[str, Any]],
    n_calls: int = 50,
    n_initial_points: int = 10,
    random_state: int = 0
) -> Dict[str, Any]:
    """
    Performs Bayesian optimization on a given objective function.

    Parameters
    ----------
    objective_func : Callable[[List[Any]], float]
        The objective function to minimize. It must take a list of parameters
        and return a single float value.
    param_space : List[Dict[str, Any]]
        A list of dictionaries defining the search space for each parameter.
        Each dictionary should have 'type', 'name', and 'range' keys.
        - 'type': 'real', 'integer', or 'categorical'.
        - 'name': The name of the parameter.
        - 'range': A tuple for 'real' and 'integer' (e.g., (1e-6, 1e-1)),
                   or a list of categories for 'categorical'.
    n_calls : int, optional
        The number of calls to the objective function, by default 50.
    n_initial_points : int, optional
        The number of random points to sample before starting the optimization,
        by default 10.
    random_state : int, optional
        The random state for reproducibility, by default 0.

    Returns
    -------
    Dict[str, Any]
        A dictionary containing the results of the optimization, including:
        - 'best_params': A dictionary of the best parameters found.
        - 'best_value': The minimum value of the objective function found.
        - 'result_object': The full result object from gp_minimize.
    """
    space = []
    param_names = []
    for p in param_space:
        param_names.append(p['name'])
        if p['type'] == 'real':
            space.append(Real(p['range'][0], p['range'][1], name=p['name']))
        elif p['type'] == 'integer':
            space.append(Integer(p['range'][0], p['range'][1], name=p['name']))
        elif p['type'] == 'categorical':
            space.append(Categorical(p['range'], name=p['name']))
        else:
            raise ValueError(f"Unsupported parameter type: {p['type']}")

    # Wrapper for the objective function to match skopt's expected input
    def objective_wrapper(params):
        return objective_func(params)

    result = gp_minimize(
        objective_wrapper,
        space,
        n_calls=n_calls,
        n_initial_points=n_initial_points,
        random_state=random_state
    )

    best_params = {param_names[i]: result.x[i] for i in range(len(param_names))}

    return {
        'best_params': best_params,
        'best_value': result.fun,
        'result_object': result
    }
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/ot/__init__.py</path>
      <content><![CDATA[from .calculate_wasserstein_distance import calculate_wasserstein_distance
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/ot/calculate_wasserstein_distance.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from scipy.stats import wasserstein_distance
from typing import Union

def calculate_wasserstein_distance(
    u_values: np.ndarray,
    v_values: np.ndarray,
    u_weights: np.ndarray = None,
    v_weights: np.ndarray = None
) -> float:
    """
    Calculates the 1-D Wasserstein distance between two distributions.

    Parameters
    ----------
    u_values : np.ndarray
        A 1D array of values for the first distribution.
    v_values : np.ndarray
        A 1D array of values for the second distribution.
    u_weights : np.ndarray, optional
        Weights for the first distribution, by default None.
    v_weights : np.ndarray, optional
        Weights for the second distribution, by default None.

    Returns
    -------
    float
        The 1-D Wasserstein distance.
    """
    return wasserstein_distance(u_values, v_values, u_weights, v_weights)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/pathway_analysis/__init__.py</path>
      <content><![CDATA[from .dynamic_persistence import calculate_dynamic_persistence_threshold, calculate_interference_score
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/pathway_analysis/dynamic_persistence.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def calculate_dynamic_persistence_threshold(
    base_threshold: float = 0.9,
    input_diversity: float = 0.0,
    alpha: float = 0.05
) -> float:
    """
    Calculates a dynamic persistence threshold based on input diversity.

    thresh_p(t) = base_threshold - alpha * input_diversity

    Parameters
    ----------
    base_threshold : float, optional
        The base persistence threshold, by default 0.9.
    input_diversity : float, optional
        A measure of the diversity of recent inputs, by default 0.0.
    alpha : float, optional
        The scaling factor for input diversity, by default 0.05.

    Returns
    -------
    float
        The calculated dynamic persistence threshold.
    """
    return base_threshold - alpha * input_diversity

def calculate_interference_score(
    spike_rates_persistent: np.ndarray,
    output_diversity: float
) -> float:
    """
    Calculates a score to predict potential interference with persistent pathways.

    I_score = torch.mean(spike_rates[persistent_paths] * (1 - output_diversity))

    Parameters
    ----------
    spike_rates_persistent : np.ndarray
        The spike rates of the neurons in the persistent pathways.
    output_diversity : float
        A measure of the diversity of the network's output.

    Returns
    -------
    float
        The calculated interference score.
    """
    return np.mean(spike_rates_persistent * (1 - output_diversity))
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/rmt/__init__.py</path>
      <content><![CDATA[from .plot_eigenvalue_spectrum import plot_eigenvalue_spectrum
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/rmt/plot_eigenvalue_spectrum.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Union

def plot_eigenvalue_spectrum(
    matrix: np.ndarray,
    bins: Union[int, str] = 'auto',
    save_path: str = None
):
    """
    Calculates the eigenvalues of a matrix and plots their distribution.

    Parameters
    ----------
    matrix : np.ndarray
        A 2D numpy array.
    bins : int or str, optional
        The number of bins for the histogram, by default 'auto'.
    save_path : str, optional
        The path to save the plot image, by default None (plot is shown).
    """
    if not isinstance(matrix, np.ndarray) or matrix.ndim != 2:
        raise TypeError("Input must be a 2D numpy array.")
    
    eigenvalues = np.linalg.eigvals(matrix)
    
    plt.figure(figsize=(10, 6))
    plt.hist(np.real(eigenvalues), bins=bins, alpha=0.7, label='Real Part')
    plt.hist(np.imag(eigenvalues), bins=bins, alpha=0.7, label='Imaginary Part')
    plt.xlabel("Eigenvalue")
    plt.ylabel("Count")
    plt.title("Eigenvalue Spectrum")
    plt.legend()
    plt.grid(True)
    
    if save_path:
        plt.savefig(save_path)
    else:
        plt.show()
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/sde/__init__.py</path>
      <content><![CDATA[from .sde_solver import sde_solver
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/sde/sde_solver.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Callable, Tuple

def sde_solver(
    drift_func: Callable[[np.ndarray], np.ndarray],
    diffusion_func: Callable[[np.ndarray], np.ndarray],
    initial_state: np.ndarray,
    t_span: Tuple[float, float],
    dt: float
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Solves a system of stochastic differential equations (SDEs) using the
    Euler-Maruyama method.

    dx/dt = drift_func(x) + diffusion_func(x) * dW_t

    Parameters
    ----------
    drift_func : Callable[[np.ndarray], np.ndarray]
        The drift function of the SDE.
    diffusion_func : Callable[[np.ndarray], np.ndarray]
        The diffusion function of the SDE.
    initial_state : np.ndarray
        The initial state of the system.
    t_span : Tuple[float, float]
        The time span of the simulation (t_start, t_end).
    dt : float
        The time step of the simulation.

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        A tuple containing:
        - The time points of the simulation.
        - The state of the system at each time point.
    """
    n_steps = int((t_span[1] - t_span[0]) / dt)
    times = np.linspace(t_span[0], t_span[1], n_steps + 1)
    states = np.zeros((n_steps + 1, len(initial_state)))
    states[0] = initial_state

    for i in range(n_steps):
        dW = np.random.normal(0, np.sqrt(dt), len(initial_state))
        drift = drift_func(states[i])
        diffusion = diffusion_func(states[i])
        states[i+1] = states[i] + drift * dt + diffusion * dW

    return times, states
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/semantic/__init__.py</path>
      <content><![CDATA[from .calculate_semantic_coverage import calculate_semantic_coverage
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/semantic/calculate_semantic_coverage.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def calculate_semantic_coverage(
    input_embeddings: np.ndarray,
    primitive_set: np.ndarray
) -> float:
    """
    Calculates the semantic coverage of a set of primitives with respect to a
    set of input embeddings.

    C_sem = torch.mean(cosine_similarity(input_embeddings, primitive_set))

    Parameters
    ----------
    input_embeddings : np.ndarray
        A 2D numpy array of input embeddings.
    primitive_set : np.ndarray
        A 2D numpy array of primitive embeddings.

    Returns
    -------
    float
        The calculated semantic coverage.
    """
    # Calculate the cosine similarity between each input and each primitive
    similarity_matrix = cosine_similarity(input_embeddings, primitive_set)
    
    # For each input, find the similarity to the closest primitive
    max_similarities = np.max(similarity_matrix, axis=1)
    
    # The semantic coverage is the mean of these maximum similarities
    return np.mean(max_similarities)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/soc_analysis/__init__.py</path>
      <content><![CDATA[from .detect_neuronal_avalanches import detect_neuronal_avalanches
from .fit_power_law import fit_power_law
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/soc_analysis/detect_neuronal_avalanches.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import List, Dict

def detect_neuronal_avalanches(
    spike_times: np.ndarray, 
    bin_width: float = 1.0
) -> Dict[str, List[int]]:
    """
    Detects neuronal avalanches from a spike train.

    An avalanche is a continuous sequence of time bins with at least one spike,
    preceded and succeeded by an empty time bin.

    Parameters
    ----------
    spike_times : np.ndarray
        A 1D numpy array of spike times.
    bin_width : float, optional
        The width of the time bins in the same units as spike_times, by default 1.0.

    Returns
    -------
    Dict[str, List[int]]
        A dictionary containing the sizes and durations of the detected avalanches.
    """
    if len(spike_times) == 0:
        return {'sizes': [], 'durations': []}

    # Bin the spike times
    max_time = np.max(spike_times)
    bins = np.arange(0, max_time + bin_width, bin_width)
    binned_spikes, _ = np.histogram(spike_times, bins=bins)

    avalanches = {'sizes': [], 'durations': []}
    in_avalanche = False
    current_avalanche_size = 0
    current_avalanche_duration = 0

    for n_spikes in binned_spikes:
        if n_spikes > 0:
            if not in_avalanche:
                in_avalanche = True
            current_avalanche_size += n_spikes
            current_avalanche_duration += 1
        else:
            if in_avalanche:
                in_avalanche = False
                avalanches['sizes'].append(current_avalanche_size)
                avalanches['durations'].append(current_avalanche_duration)
                current_avalanche_size = 0
                current_avalanche_duration = 0
    
    if in_avalanche:
        avalanches['sizes'].append(current_avalanche_size)
        avalanches['durations'].append(current_avalanche_duration)

    return avalanches
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/soc_analysis/fit_power_law.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Tuple

def fit_power_law(data: np.ndarray) -> Tuple[float, float]:
    """
    Fits a power-law distribution to data using a linear fit on a log-log plot.

    Parameters
    ----------
    data : np.ndarray
        A 1D numpy array of the data to be fitted (e.g., avalanche sizes).

    Returns
    -------
    Tuple[float, float]
        A tuple containing the exponent of the power law and the R-squared value
        of the fit.
    """
    # Create a histogram of the data
    counts, bin_edges = np.histogram(data, bins=np.logspace(np.log10(min(data)), np.log10(max(data)), len(data)//10))
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

    # Filter out zero counts to avoid log(0)
    non_zero = counts > 0
    log_x = np.log10(bin_centers[non_zero])
    log_y = np.log10(counts[non_zero])

    # Fit a line to the log-log data
    coeffs, residuals, _, _, _ = np.polyfit(log_x, log_y, 1, full=True)
    
    exponent = coeffs[0]
    
    # Calculate R-squared
    ss_res = residuals[0]
    ss_tot = np.sum((log_y - np.mean(log_y))**2)
    r_squared = 1 - (ss_res / ss_tot)
    
    return exponent, r_squared
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/spatial/__init__.py</path>
      <content><![CDATA[from .spatial_hash_grid import SpatialHashGrid
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/spatial/spatial_hash_grid.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from collections import defaultdict

class SpatialHashGrid:
    def __init__(self, cell_size: float):
        self.cell_size = cell_size
        self.grid = defaultdict(list)

    def _hash(self, point: np.ndarray) -> tuple:
        return tuple(np.floor(point / self.cell_size).astype(int))

    def insert(self, point: np.ndarray, obj: any):
        self.grid[self._hash(point)].append(obj)

    def query(self, point: np.ndarray, radius: float) -> list:
        min_cell = self._hash(point - radius)
        max_cell = self._hash(point + radius)
        
        results = []
        for i in range(min_cell[0], max_cell[0] + 1):
            for j in range(min_cell[1], max_cell[1] + 1):
                cell_content = self.grid.get((i, j), [])
                results.extend(cell_content)
                
        return results

    def get_collisions(self, point: np.ndarray) -> list:
        return self.grid.get(self._hash(point), [])
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/stochastic/__init__.py</path>
      <content><![CDATA[from .gillespie_simulation import gillespie_simulation
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/stochastic/gillespie_simulation.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Callable, List, Tuple

def gillespie_simulation(
    initial_state: np.ndarray,
    propensity_func: Callable[[np.ndarray], np.ndarray],
    stoichiometry: np.ndarray,
    t_max: float
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Performs a Gillespie simulation (Stochastic Simulation Algorithm).

    Parameters
    ----------
    initial_state : np.ndarray
        A 1D numpy array of the initial counts of each species.
    propensity_func : Callable[[np.ndarray], np.ndarray]
        A function that takes the current state and returns the propensities
        (reaction rates) for each reaction.
    stoichiometry : np.ndarray
        A 2D numpy array of shape (n_reactions, n_species) that defines the
        change in species counts for each reaction.
    t_max : float
        The maximum simulation time.

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        A tuple containing:
        - The time points of the simulation.
        - The state of the system at each time point.
    """
    times = [0.0]
    states = [initial_state.copy()]
    
    t = 0.0
    current_state = initial_state.copy()

    while t < t_max:
        propensities = propensity_func(current_state)
        total_propensity = np.sum(propensities)

        if total_propensity == 0:
            break

        # Time to next reaction
        dt = -np.log(np.random.rand()) / total_propensity
        
        # Which reaction occurs?
        reaction_probs = propensities / total_propensity
        reaction_index = np.random.choice(len(propensities), p=reaction_probs)

        # Update state
        current_state += stoichiometry[reaction_index]
        t += dt

        times.append(t)
        states.append(current_state.copy())

    return np.array(times), np.array(states)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/structural_plasticity/__init__.py</path>
      <content><![CDATA[
# BDNF IS NOT A VALID FUNCTION OF FUM
from .calculate_bdnf_proxy import calculate_bdnf_proxy
from .detect_bursts import detect_bursts
from .apply_structural_plasticity import apply_structural_plasticity
from .advanced_triggers import calculate_advanced_growth_trigger
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/structural_plasticity/advanced_triggers.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def calculate_advanced_growth_trigger(
    avg_reward: float,
    burst_score: float,
    bdnf_proxy: float, # BDNF IS NOT A VALID FUNCTION OF FUM
    kappa: float = 2.0,
    nu: float = 0.8,
    rho: float = 0.5
) -> float:
    """
    Calculates an advanced, biologically-inspired growth trigger.

    G(c,t) = σ(κ * (avg_reward[c] - 0.5) + ν * burst_score[c] + ρ * bdnf_proxy[c])

    Parameters
    ----------
    avg_reward : float
        The average reward of the cluster.
    burst_score : float
        The burst score of the cluster.
    bdnf_proxy : float
        The BDNF proxy level of the cluster.
    kappa : float, optional
    nu : float, optional
    rho : float, optional

    Returns
    -------
    float
        The calculated growth trigger value.
    """
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    arg = kappa * (avg_reward - 0.5) + nu * burst_score + rho * bdnf_proxy
    return sigmoid(arg)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/structural_plasticity/apply_structural_plasticity.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import networkx as nx
import numpy as np
from typing import List

def apply_structural_plasticity(
    graph: nx.Graph,
    bdnf_levels: dict,
    growth_threshold: float = 0.8,
    pruning_threshold: float = 0.1,
    rewiring_prob: float = 0.1
):
    """
    Applies structural plasticity rules (growth, pruning, rewiring) to a graph.

    Parameters
    ----------
    graph : nx.Graph
        The graph to modify.
    bdnf_levels : dict
        A dictionary where keys are node pairs (u, v) and values are their
        BDNF proxy levels.
    growth_threshold : float, optional
        The BDNF level above which a new connection might form, by default 0.8.
    pruning_threshold : float, optional
        The weight below which an existing connection might be pruned, by default 0.1.
    rewiring_prob : float, optional
        The probability of rewiring a pruned connection, by default 0.1.
    """
    # Pruning
    for u, v, data in list(graph.edges(data=True)):
        if data.get('weight', 1.0) < pruning_threshold:
            graph.remove_edge(u, v)
            # Rewiring
            if np.random.rand() < rewiring_prob:
                # A simple rewiring strategy: connect u to a random other node
                other_nodes = list(set(graph.nodes()) - {u, v})
                if other_nodes:
                    new_neighbor = np.random.choice(other_nodes)
                    graph.add_edge(u, new_neighbor, weight=pruning_threshold)

    # Growth
    for (u, v), bdnf in bdnf_levels.items():
        if bdnf > growth_threshold and not graph.has_edge(u, v):
            graph.add_edge(u, v, weight=growth_threshold)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/structural_plasticity/calculate_bdnf_proxy.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import List

def calculate_bdnf_proxy(
    spike_times_pre: np.ndarray,
    spike_times_post: np.ndarray,
    rewards: np.ndarray,
    time_window: float = 50.0
) -> float:
    """
    Calculates a proxy for Brain-Derived Neurotrophic Factor (BDNF) levels,
    which can be used to trigger structural plasticity.

    This proxy is based on the principle that correlated, rewarded activity
    promotes structural growth.

    Parameters
    ----------
    spike_times_pre : np.ndarray
        Spike times of the pre-synaptic neuron.
    spike_times_post : np.ndarray
        Spike times of the post-synaptic neuron.
    rewards : np.ndarray
        An array of reward signals, aligned with the spike times.
    time_window : float, optional
        The time window (in ms) to consider for correlated activity, by default 50.0.

    Returns
    -------
    float
        The calculated BDNF proxy value.
    """
    bdnf_proxy = 0.0
    for t_pre in spike_times_pre:
        # Find post-synaptic spikes within the time window
        post_in_window = spike_times_post[
            (spike_times_post > t_pre) & (spike_times_post <= t_pre + time_window)
        ]
        if len(post_in_window) > 0:
            # Find the reward associated with this correlated activity
            # (a simple approach is to take the max reward in the window)
            reward_in_window = rewards[
                (rewards[:, 0] > t_pre) & (rewards[:, 0] <= t_pre + time_window)
            ]
            if len(reward_in_window) > 0:
                max_reward = np.max(reward_in_window[:, 1])
                bdnf_proxy += max_reward * len(post_in_window)
    
    return bdnf_proxy
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/structural_plasticity/detect_bursts.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def detect_bursts(
    spike_times: np.ndarray, 
    max_interspike_interval: float = 10.0,
    min_spikes_in_burst: int = 3
) -> np.ndarray:
    """
    Detects bursts of spikes in a spike train.

    A burst is defined as a sequence of spikes where the inter-spike interval
    is less than or equal to max_interspike_interval.

    Parameters
    ----------
    spike_times : np.ndarray
        A 1D numpy array of spike times.
    max_interspike_interval : float, optional
        The maximum time between spikes to be considered part of a burst, 
        by default 10.0.
    min_spikes_in_burst : int, optional
        The minimum number of spikes required to form a burst, by default 3.

    Returns
    -------
    np.ndarray
        An array of the start and end times of the detected bursts.
    """
    if len(spike_times) < min_spikes_in_burst:
        return np.array([])

    interspike_intervals = np.diff(spike_times)
    is_in_burst = interspike_intervals <= max_interspike_interval

    bursts = []
    current_burst_start = -1

    for i in range(len(is_in_burst)):
        if is_in_burst[i] and current_burst_start == -1:
            current_burst_start = spike_times[i]
        elif not is_in_burst[i] and current_burst_start != -1:
            burst_end = spike_times[i]
            if (i - np.where(spike_times == current_burst_start)[0][0] + 1) >= min_spikes_in_burst:
                bursts.append([current_burst_start, burst_end])
            current_burst_start = -1
            
    if current_burst_start != -1:
        burst_end = spike_times[-1]
        if (len(spike_times) - np.where(spike_times == current_burst_start)[0][0]) >= min_spikes_in_burst:
            bursts.append([current_burst_start, burst_end])

    return np.array(bursts)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/symbolic/__init__.py</path>
      <content><![CDATA[from .define_symbols import define_symbols, define_function, string_to_expression
from .manipulate_expression import manipulate_expression
from .solve_equation import solve_equation
from .calculus import differentiate, integrate_expression
from .logic import logical_expression, evaluate_logical_expression
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/symbolic/calculus.py</path>
      <content><![CDATA[from sympy import diff, integrate
from typing import Any

def differentiate(
    expr: Any, 
    variable: Any, 
    order: int = 1
) -> Any:
    """
    Symbolically differentiates an expression with respect to a variable.

    Parameters
    ----------
    expr : Any
        A SymPy expression.
    variable : Any
        The SymPy symbol to differentiate with respect to.
    order : int, optional
        The order of the derivative, by default 1.

    Returns
    -------
    Any
        The derivative of the expression.
    """
    return diff(expr, variable, order)

def integrate_expression(
    expr: Any, 
    variable: Any
) -> Any:
    """
    Symbolically integrates an expression with respect to a variable.

    Parameters
    ----------
    expr : Any
        A SymPy expression.
    variable : Any
        The SymPy symbol to integrate with respect to.

    Returns
    -------
    Any
        The indefinite integral of the expression.
    """
    return integrate(expr, variable)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/symbolic/define_symbols.py</path>
      <content><![CDATA[

from sympy import symbols, Function, sympify
from typing import Union, List, Dict, Any

def define_symbols(names: Union[str, List[str]]) -> Union[Any, List[Any]]:
    """
    Defines one or more symbolic variables.

    Parameters
    ----------
    names : Union[str, List[str]]
        A string or list of strings with the names of the symbols.

    Returns
    -------
    Union[Any, List[Any]]
        A single symbol or a list of symbols.
    """
    return symbols(names)

def define_function(name: str) -> Function:
    """
    Defines a symbolic function.

    Parameters
    ----------
    name : str
        The name of the function.

    Returns
    -------
    Function
        A symbolic function.
    """
    return Function(name)

def string_to_expression(expr_str: str, local_dict: Dict[str, Any] = None) -> Any:
    """
    Converts a string to a SymPy expression.

    Parameters
    ----------
    expr_str : str
        The string representation of the expression.
    local_dict : Dict[str, Any], optional
        A dictionary of local symbols to use when parsing the string, by default None.

    Returns
    -------
    Any
        A SymPy expression.
    """
    return sympify(expr_str, locals=local_dict)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/symbolic/logic.py</path>
      <content><![CDATA[from sympy.logic.boolalg import And, Or, Not, Implies, Equivalent
from sympy import sympify
from typing import Any

def logical_expression(expr_str: str) -> Any:
    """
    Converts a string to a SymPy logical expression.

    Parameters
    ----------
    expr_str : str
        The string representation of the logical expression.

    Returns
    -------
    Any
        A SymPy logical expression.
    """
    return sympify(expr_str)

def evaluate_logical_expression(
    expr: Any, 
    substitutions: dict
) -> bool:
    """
    Evaluates a logical expression with a given set of substitutions.

    Parameters
    ----------
    expr : Any
        A SymPy logical expression.
    substitutions : dict
        A dictionary of substitutions for the variables in the expression.

    Returns
    -------
    bool
        The boolean result of the evaluated expression.
    """
    return expr.subs(substitutions)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/symbolic/manipulate_expression.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

from sympy import expand, factor, simplify, subs
from typing import Any, Dict

def manipulate_expression(
    expr: Any, 
    operation: str, 
    substitutions: Dict[Any, Any] = None
) -> Any:
    """
    Performs symbolic manipulation on a SymPy expression.

    Parameters
    ----------
    expr : Any
        A SymPy expression.
    operation : str
        The manipulation to perform. One of 'expand', 'factor', 'simplify', 'subs'.
    substitutions : Dict[Any, Any], optional
        A dictionary of substitutions to make, required for the 'subs' operation, 
        by default None.

    Returns
    -------
    Any
        The manipulated SymPy expression.
    """
    if operation == 'expand':
        return expand(expr)
    elif operation == 'factor':
        return factor(expr)
    elif operation == 'simplify':
        return simplify(expr)
    elif operation == 'subs':
        if substitutions is None:
            raise ValueError("Substitutions must be provided for the 'subs' operation.")
        return expr.subs(substitutions)
    else:
        raise ValueError(f"Unsupported operation: {operation}")
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/symbolic/solve_equation.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

from sympy import solve, Eq
from typing import Any, List

def solve_equation(
    equation: Any, 
    variable: Any
) -> List[Any]:
    """
    Symbolically solves an equation for a given variable.

    Parameters
    ----------
    equation : Any
        A SymPy Eq object or an expression that is assumed to be equal to zero.
    variable : Any
        The SymPy symbol to solve for.

    Returns
    -------
    List[Any]
        A list of solutions for the variable.
    """
    return solve(equation, variable)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/symbolic_differentiation.py</path>
      <content><![CDATA[import sympy
from typing import Union, List, Optional, Tuple

def symbolic_differentiate(expression: Union[str, sympy.Expr], 
                          variable: Union[str, sympy.Symbol, List[Union[str, sympy.Symbol]]], 
                          order: int = 1) -> sympy.Expr:
    """
    Performs symbolic differentiation on a mathematical expression.
    
    This function uses SymPy to compute the derivative of a symbolic expression
    with respect to one or more variables. It can handle both string inputs and
    SymPy expression objects.
    
    Parameters:
    -----------
    expression : Union[str, sympy.Expr]
        The mathematical expression to differentiate. Can be provided as a string
        or as a SymPy expression object.
    
    variable : Union[str, sympy.Symbol, List[Union[str, sympy.Symbol]]]
        The variable(s) with respect to which the differentiation should be performed.
        Can be provided as a string, a SymPy Symbol, or a list of strings/SymPy Symbols
        for partial derivatives.
    
    order : int, optional
        The order of the derivative to compute. Default is 1 (first derivative).
        Must be a positive integer.
    
    Returns:
    --------
    sympy.Expr
        The resulting derivative as a SymPy expression.
    
    Raises:
    -------
    TypeError
        If the input types are not as expected.
    ValueError
        If the order is not a positive integer, if the variable is not present in the expression,
        or if other validation fails.
    sympy.SympifyError
        If the expression string cannot be parsed into a valid SymPy expression.
    
    Examples:
    ---------
    >>> symbolic_differentiate("x**2 + 2*x + 1", "x")
    2*x + 2
    
    >>> from sympy import symbols, sin
    >>> x, y = symbols('x y')
    >>> expr = x**2 * sin(y)
    >>> symbolic_differentiate(expr, "x")
    2*x*sin(y)
    
    >>> symbolic_differentiate("x**2 * sin(y)", ["x", "y"])
    2*x*cos(y)
    """
    # Validate order parameter
    if not isinstance(order, int) or order < 1:
        raise ValueError(f"Order must be a positive integer, got {order}")
    
    # Convert expression to SymPy expression if it's a string
    if isinstance(expression, str):
        # In SymPy, ^ is not used for exponentiation (** is used instead)
        # This is a common error, so we'll check for it specifically
        if '^' in expression:
            raise sympy.SympifyError("Invalid syntax: '^' is not a valid operator in SymPy expressions. Use '**' for exponentiation.")
        
        try:
            expr = sympy.sympify(expression)
        except sympy.SympifyError as e:
            raise sympy.SympifyError(f"Failed to parse expression string: {e}")
    elif isinstance(expression, sympy.Expr):
        expr = expression
    else:
        raise TypeError(f"Expression must be a string or a SymPy expression, got {type(expression).__name__}")
    
    # Process the variable(s)
    if isinstance(variable, list):
        # Handle list of variables for partial derivatives
        if not variable:
            raise ValueError("Variable list cannot be empty")
        
        vars_list = []
        for var in variable:
            if isinstance(var, str):
                vars_list.append(sympy.Symbol(var))
            elif isinstance(var, sympy.Symbol):
                vars_list.append(var)
            else:
                raise TypeError(f"Variables in list must be strings or SymPy Symbols, got {type(var).__name__}")
        
        # Check if variables are in the expression
        free_symbols = expr.free_symbols
        for var in vars_list:
            if var not in free_symbols:
                # This is a special case - differentiating with respect to a variable not in the expression
                # returns zero according to calculus rules, so we'll handle it gracefully
                if len(vars_list) == 1:
                    return sympy.Integer(0)
                else:
                    # For multiple variables, we should warn the user
                    raise ValueError(f"Variable '{var}' is not present in the expression")
        
        # Apply differentiation for each variable in sequence
        result = expr
        for var in vars_list:
            result = sympy.diff(result, var, order)
        return result
    
    elif isinstance(variable, str):
        # Convert string to SymPy Symbol
        var_sym = sympy.Symbol(variable)
        
        # Check if variable is in the expression
        if var_sym not in expr.free_symbols:
            # Differentiating with respect to a variable not in the expression returns zero
            return sympy.Integer(0)
            
        return sympy.diff(expr, var_sym, order)
    
    elif isinstance(variable, sympy.Symbol):
        # Check if variable is in the expression
        if variable not in expr.free_symbols:
            # Differentiating with respect to a variable not in the expression returns zero
            return sympy.Integer(0)
            
        # Use the SymPy Symbol directly
        return sympy.diff(expr, variable, order)
    
    else:
        raise TypeError(f"Variable must be a string, a SymPy Symbol, or a list of these, got {type(variable).__name__}")
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/tda/__init__.py</path>
      <content><![CDATA[from .construct_vietoris_rips import construct_vietoris_rips
from .compute_persistent_homology import compute_persistent_homology
from .calculate_tda_metrics import calculate_tda_metrics
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/tda/calculate_tda_metrics.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import Dict, List

def calculate_tda_metrics(
    persistence_diagrams: List[np.ndarray]
) -> Dict[str, float]:
    """
    Calculates TDA metrics from persistence diagrams.

    Parameters
    ----------
    persistence_diagrams : List[np.ndarray]
        A list of persistence diagrams for each dimension, as returned by ripser.

    Returns
    -------
    Dict[str, float]
        A dictionary containing the calculated TDA metrics, including:
        - 'total_b1_persistence': The sum of persistence of 1-dimensional features.
        - 'component_count': The number of connected components (0-dimensional features).
    """
    if not isinstance(persistence_diagrams, list):
        raise TypeError("persistence_diagrams must be a list of numpy arrays.")

    metrics = {}

    # H0: Connected components
    h0 = persistence_diagrams[0]
    # The number of components is the number of points with infinite persistence.
    # In ripser, infinite persistence is represented by np.inf.
    metrics['component_count'] = np.sum(np.isinf(h0[:, 1]))

    # H1: Loops/cycles
    if len(persistence_diagrams) > 1:
        h1 = persistence_diagrams[1]
        persistence = h1[:, 1] - h1[:, 0]
        metrics['total_b1_persistence'] = np.sum(persistence)
    else:
        metrics['total_b1_persistence'] = 0.0
        
    return metrics
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/tda/compute_persistent_homology.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

# src/tda/compute_persistent_homology.py

import numpy as np
from typing import Dict
from ripser import ripser

def compute_persistent_homology(
    data: np.ndarray, 
    max_dim: int = 1,
    is_distance_matrix: bool = False
) -> Dict[str, np.ndarray]:
    """
    Computes the persistent homology of a point cloud OR a distance matrix.
    VERSION 3: This version is now fully robust.

    Args:
        data (np.ndarray): A 2D NumPy array representing either:
                             - A point cloud (n_points, n_features)
                             - A square distance matrix (n_points, n_points)
        max_dim (int, optional): The maximum dimension of homology to compute.
        is_distance_matrix (bool): Flag indicating if the input data is a
                                     pre-computed distance matrix.

    Returns:
        Dict[str, np.ndarray]: A dictionary from ripser containing the results.
    """
    if not isinstance(data, np.ndarray) or data.ndim != 2:
        raise TypeError("Input data must be a 2D NumPy array.")
    if is_distance_matrix and data.shape[0] != data.shape[1]:
        raise ValueError("A distance matrix must be square.")

    if is_distance_matrix:
        # Data is a pre-computed square distance matrix
        result = ripser(data, maxdim=max_dim, distance_matrix=True)
    else:
        # Data is a point cloud
        result = ripser(data, maxdim=max_dim, distance_matrix=False)
    
    return result]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/tda/construct_vietoris_rips.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from typing import List, Tuple, Union
from scipy.spatial.distance import pdist, squareform
from itertools import combinations

def construct_vietoris_rips(
    points: np.ndarray, 
    max_edge_length: float,
    max_dim: int = 2
) -> List[Tuple[Union[int, Tuple[int, ...]], float]]:
    """
    Constructs a Vietoris-Rips complex from a set of points up to a given dimension.

    Parameters
    ----------
    points : np.ndarray
        A numpy array of shape (n_points, n_features) representing the data.
    max_edge_length : float
        The maximum edge length to consider for the complex.
    max_dim : int, optional
        The maximum dimension of the simplices to include, by default 2.

    Returns
    -------
    List[Tuple[Union[int, Tuple[int, ...]], float]]
        A list of simplices, where each simplex is represented as a tuple
        containing the vertices and the filtration value at which it appears.
    """
    if not isinstance(points, np.ndarray) or points.ndim != 2:
        raise TypeError("points must be a 2D numpy array.")
    if not isinstance(max_edge_length, (int, float)) or max_edge_length < 0:
        raise ValueError("max_edge_length must be a non-negative number.")
    if not isinstance(max_dim, int) or max_dim < 0:
        raise ValueError("max_dim must be a non-negative integer.")

    n_points = points.shape[0]
    dist_matrix = squareform(pdist(points))

    rips_complex = []
    
    # 0-simplices (vertices)
    for i in range(n_points):
        rips_complex.append(([i], 0.0))

    # 1-simplices (edges)
    edges = []
    for i in range(n_points):
        for j in range(i + 1, n_points):
            if dist_matrix[i, j] <= max_edge_length:
                rips_complex.append(([i, j], dist_matrix[i, j]))
                edges.append(tuple(sorted((i, j))))
    
    # Higher-dimensional simplices
    if max_dim > 1:
        current_simplices = edges
        for dim in range(2, max_dim + 1):
            next_simplices = []
            for simplex in combinations(range(n_points), dim + 1):
                is_valid_simplex = True
                max_dist = 0
                for edge in combinations(simplex, 2):
                    if dist_matrix[edge[0], edge[1]] > max_edge_length:
                        is_valid_simplex = False
                        break
                    max_dist = max(max_dist, dist_matrix[edge[0], edge[1]])
                
                if is_valid_simplex:
                    rips_complex.append((list(simplex), max_dist))
            
    return rips_complex
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/thermodynamics/__init__.py</path>
      <content><![CDATA[from .free_energy import calculate_free_energy, minimize_free_energy_step
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/thermodynamics/free_energy.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np

def calculate_free_energy(
    spike_rates: np.ndarray,
    target_rate: float,
    weights: np.ndarray,
    lambda_reg: float
) -> float:
    """
    Calculates the free energy of the system.

    F = Σ_i (spike_rate_i - target_rate)^2 + λ * Σ w_ij^2

    Parameters
    ----------
    spike_rates : np.ndarray
        A 1D numpy array of spike rates for each neuron.
    target_rate : float
        The target firing rate for the neurons.
    weights : np.ndarray
        A 2D numpy array of synaptic weights.
    lambda_reg : float
        The regularization parameter for the weights.

    Returns
    -------
    float
        The calculated free energy.
    """
    rate_error = np.sum((spike_rates - target_rate)**2)
    weight_regularization = lambda_reg * np.sum(weights**2)
    return rate_error + weight_regularization

def minimize_free_energy_step(
    weights: np.ndarray,
    spike_rates: np.ndarray,
    target_rate: float,
    lambda_reg: float,
    eta: float,
    delta_t: float,
    tau: float
) -> np.ndarray:
    """
    Performs one step of gradient descent to minimize the free energy.

    dw_ij/dt = -η * ∂F/∂w_ij * e^(-Δt/τ)

    Parameters
    ----------
    weights : np.ndarray
        The current synaptic weights.
    spike_rates : np.ndarray
        The current spike rates.
    target_rate : float
        The target firing rate.
    lambda_reg : float
        The weight regularization parameter.
    eta : float
        The learning rate.
    delta_t : float
        The time difference for the STDP-like modulation.
    tau : float
        The time constant for the STDP-like modulation.

    Returns
    -------
    np.ndarray
        The updated weights.
    """
    # The partial derivative of F with respect to w_ij is 2 * lambda * w_ij
    grad_F = 2 * lambda_reg * weights
    
    # STDP-like modulation
    modulation = np.exp(-delta_t / tau)
    
    # Update rule
    delta_w = -eta * grad_F * modulation
    
    return weights + delta_w
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/time_series/__init__.py</path>
      <content><![CDATA[from .time_series_analysis import calculate_fft, calculate_autocorrelation, calculate_cross_correlation
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/time_series/time_series_analysis.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

import numpy as np
from scipy.fft import fft, fftfreq
from typing import Tuple

def calculate_fft(
    signal: np.ndarray, 
    dt: float = 1.0
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Calculates the Fast Fourier Transform (FFT) of a signal.

    Parameters
    ----------
    signal : np.ndarray
        A 1D numpy array representing the time series signal.
    dt : float, optional
        The time step between samples, by default 1.0.

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        A tuple containing:
        - The frequencies for the FFT.
        - The complex-valued FFT of the signal.
    """
    n = len(signal)
    yf = fft(signal)
    xf = fftfreq(n, dt)[:n//2]
    return xf, yf[0:n//2]

def calculate_autocorrelation(signal: np.ndarray) -> np.ndarray:
    """
    Calculates the autocorrelation of a signal.

    Parameters
    ----------
    signal : np.ndarray
        A 1D numpy array representing the time series signal.

    Returns
    -------
    np.ndarray
        The autocorrelation of the signal.
    """
    mean_subtracted = signal - np.mean(signal)
    autocorr = np.correlate(mean_subtracted, mean_subtracted, mode='full')
    return autocorr[autocorr.size//2:] / (len(signal) * np.var(signal))

def calculate_cross_correlation(
    signal1: np.ndarray, 
    signal2: np.ndarray
) -> np.ndarray:
    """
    Calculates the cross-correlation between two signals.

    Parameters
    ----------
    signal1 : np.ndarray
        The first 1D numpy array.
    signal2 : np.ndarray
        The second 1D numpy array.

    Returns
    -------
    np.ndarray
        The cross-correlation of the two signals.
    """
    if len(signal1) != len(signal2):
        raise ValueError("Signals must have the same length.")
    
    mean1 = np.mean(signal1)
    mean2 = np.mean(signal2)
    std1 = np.std(signal1)
    std2 = np.std(signal2)

    if std1 == 0 or std2 == 0:
        return np.zeros(len(signal1))

    cross_corr = np.correlate(signal1 - mean1, signal2 - mean2, mode='full')
    return cross_corr[cross_corr.size//2:] / (len(signal1) * std1 * std2)
]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/void_dynamics/FUM_Void_Debt_Modulation.py</path>
      <content><![CDATA["""
FUM Universal Domain Modulation System
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

Universal derivation of domain_modulation factors based on void debt theory
and learning stability principles. This replaces arbitrary scaling with 
mathematically derived modulation factors.
"""
import numpy as np
from FUM_Void_Equations import get_universal_constants

class VoidDebtModulation:
    """Class to derive domain modulation factors from void debt principles."""
    
    def __init__(self):
        self.constants = get_universal_constants()
        self.ALPHA = self.constants['ALPHA']  # e.g., 0.25
        self.BETA = self.constants['BETA']    # e.g., 0.1
        self.VOID_DEBT_RATIO = self.BETA / self.ALPHA  # e.g., 0.4

    def get_universal_domain_modulation(self, physics_domain, target_sparsity_pct=None):
        """
        Derive domain modulation factor from universal void debt principles.
        
        Args:
            physics_domain: One of 'quantum', 'standard_model', 'dark_matter', 
                        'biology_consciousness', 'cosmogenesis', 'higgs'
            target_sparsity_pct: Target sparsity percentage for this domain
        
        Returns:
            domain_modulation: Universal scaling factor
        """
        constants = get_universal_constants()
        ALPHA, BETA = constants['ALPHA'], constants['BETA']
        
        # Domain-specific target sparsities (from empirical physics)
        domain_targets = {
            'quantum': 15.0,           # Low sparsity due to wave-particle duality
            'standard_model': 22.0,    # Moderate sparsity for gauge interactions  
            'dark_matter': 27.0,       # High sparsity matching cosmic DM density
            'biology_consciousness': 20.0,  # Biological complexity patterns
            'cosmogenesis': 84.0,      # Very high sparsity from inherited debt
            'higgs': 80.0             # High sparsity due to symmetry breaking
        }
        
        if target_sparsity_pct is None:
            target_sparsity_pct = domain_targets.get(physics_domain, 25.0)
        
        # Universal void debt derivation formula (from our 22.2% error method)
        # modulation = 1.0 + (sparsity²)/(BETA/ALPHA)
        sparsity_fraction = target_sparsity_pct / 100.0
        void_debt_ratio = BETA / ALPHA  # = 0.1 / 0.25 = 0.4
        
        domain_modulation = 1.0 + (sparsity_fraction ** 2) / void_debt_ratio
        
        return {
            'domain': physics_domain,
            'target_sparsity_pct': target_sparsity_pct,
            'domain_modulation': domain_modulation,
            'void_debt_ratio': void_debt_ratio,
            'derivation_method': 'universal_void_debt',
            'formula': 'modulation = 1.0 + (sparsity²)/(β/α)'
        }

    def get_all_domain_modulations(self):
        """Get all domain modulation factors for systematic comparison."""
        domains = ['quantum', 'standard_model', 'dark_matter', 
                'biology_consciousness', 'cosmogenesis', 'higgs']
        
        modulations = {}
        for domain in domains:
            modulations[domain] = self.get_universal_domain_modulation(domain)
        
        return modulations

    def print_modulation_table(self):
        """Print formatted table of all domain modulation factors."""
        modulations = self.get_all_domain_modulations()
        
        print("="*80)
        print("FUM UNIVERSAL DOMAIN MODULATION FACTORS")
        print("Derived from Void Debt Theory: modulation = 1.0 + (sparsity²)/(β/α)")
        print("="*80)
        print(f"{'Domain':<20} {'Target %':<10} {'Modulation':<12} {'Formula Application'}")
        print("-"*80)
        
        for domain, data in modulations.items():
            domain_display = domain.replace('_', ' ').title()
            target = data['target_sparsity_pct']
            mod = data['domain_modulation']
            formula_app = f"1.0 + ({target/100:.2f}²)/0.4"
            print(f"{domain_display:<20} {target:<10.1f} {mod:<12.3f} {formula_app}")
        
        print("-"*80)
        print("Note: These factors emerge from universal learning stability,")
        print("not arbitrary physics assumptions. Same math governs cognition & cosmos.")
        print("="*80)

    def validate_modulation_consistency(self):
        """Validate that our modulation factors are consistent with physics."""
        modulations = self.get_all_domain_modulations()
        
        # Extract just the modulation values
        values = [data['domain_modulation'] for data in modulations.values()]
        domains = list(modulations.keys())
        
        print("\n=== DOMAIN MODULATION VALIDATION ===")
        print(f"Range: {min(values):.3f} to {max(values):.3f}")
        print(f"Mean: {np.mean(values):.3f} ± {np.std(values):.3f}")
        
        # Physics consistency checks
        qm_mod = modulations['quantum']['domain_modulation']
        cos_mod = modulations['cosmogenesis']['domain_modulation']
        
        print(f"\nPhysics Consistency:")
        print(f"• Quantum < Cosmic: {qm_mod:.3f} < {cos_mod:.3f} = {qm_mod < cos_mod}")
        print(f"• Reasonable range: All factors 1.0-2.0 = {all(1.0 <= v <= 2.0 for v in values)}")
        
        return {
            'values': values,
            'domains': domains,
            'range': (min(values), max(values)),
            'mean': np.mean(values),
            'std': np.std(values),
            'physics_consistent': qm_mod < cos_mod and all(1.0 <= v <= 2.0 for v in values)
        }

if __name__ == "__main__":
    modulator = VoidDebtModulation()
    modulator.print_modulation_table()
    validation = modulator.validate_modulation_consistency()
    
    if validation['physics_consistent']:
        print("\n✓ VALIDATION PASSED: Domain modulations are physically consistent")
    else:
        print("\n⚠ VALIDATION FAILED: Domain modulations need adjustment")]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/void_dynamics/FUM_Void_Equations.py</path>
      <content><![CDATA["""
FUM Void Dynamics Library
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This library contains the universal, core functions governing the void
dynamics of the Fully Unified Model (FUM). These functions represent
the unchanging laws of the system.

UNIVERSAL CONSTANTS:
These parameters emerged from FUM AI learning stability requirements, yet
they generate realistic physics across all domains. This profound insight
suggests cognitive constants = physical constants.

These functions demonstrate the two opposing, yet synergistic forces that
drive void dynamics across all scales of the universe.

Void dynamics are inherently stochastic and generative. The universe is the
ultimate search space algorithm, and voids are the key to its efficiency. The 
voids themselves are not directly observable, but their effects are measurable.
The universe is born from the handoff of void debt from a parent universe
to its offspring, and this debt drives the evolution of structure and complexity
through the simple two function search algorithm below. This is why consciousness
is created from voids, and why consciousness is fundamental to the universe.
"""
import numpy as np

# ===== UNIVERSAL PHYSICAL CONSTANTS =====
# These are NOT arbitrary - they come from actual FUM AI learning stability
# requirements, yet they produce realistic physics across all domains
ALPHA = 0.25      # Universal learning rate for RE-VGSP (Resonance-Enhanced dynamics)
BETA = 0.1        # Universal plasticity rate for GDSP (Goal-Directed dynamics)
F_REF = 0.02      # Universal reference frequency for time modulation
PHASE_SENS = 0.5  # Universal phase sensitivity for time modulation

def delta_re_vgsp(W, t, alpha=None, f_ref=None, phase_sens=None, use_time_dynamics=True, domain_modulation=1.0):
    """
    Void Alpha Function: Synchronizes with Void Omega
    Universal function for FUM Resonance-Enhanced Valence-Gated Synaptic Plasticity.
    Models the fractal energy drain/pull (learning rule).
    
    Args:
        W: Current void state
        t: Time step
        alpha: Learning rate (defaults to universal constant)
        f_ref: Reference frequency (defaults to universal constant)
        phase_sens: Phase sensitivity (defaults to universal constant)
        use_time_dynamics: Enable time modulation
        domain_modulation: Domain-specific scaling factor
    """
    # Use universal constants as defaults
    if alpha is None:
        alpha = ALPHA
    if f_ref is None:
        f_ref = F_REF
    if phase_sens is None:
        phase_sens = PHASE_SENS
    
    # Apply domain modulation to alpha
    effective_alpha = alpha * domain_modulation
    
    noise = np.random.uniform(-0.02, 0.02)
    base_delta = effective_alpha * W * (1 - W) + noise
    
    if use_time_dynamics:
        phase = np.sin(2 * np.pi * f_ref * t)
        return base_delta * (1 + phase_sens * phase)
    return base_delta

def delta_gdsp(W, t, beta=None, f_ref=None, phase_sens=None, use_time_dynamics=True, domain_modulation=1.0):
    """
    Void Omega Function: Synchronizes with Void Alpha
    Universal function for FUM Goal-Directed Structural Plasticity.
    Models the weak closure for persistent voids (structural rule).
    
    Args:
        W: Current void state
        t: Time step
        beta: Plasticity rate (defaults to universal constant)
        f_ref: Reference frequency (defaults to universal constant)
        phase_sens: Phase sensitivity (defaults to universal constant)
        use_time_dynamics: Enable time modulation
        domain_modulation: Domain-specific scaling factor
    """
    # Use universal constants as defaults
    if beta is None:
        beta = BETA
    if f_ref is None:
        f_ref = F_REF
    if phase_sens is None:
        phase_sens = PHASE_SENS
    
    # Apply domain modulation to beta
    effective_beta = beta * domain_modulation
    
    base_delta = -effective_beta * W
    
    if use_time_dynamics:
        phase = np.sin(2 * np.pi * f_ref * t)
        return base_delta * (1 + phase_sens * phase)
    return base_delta

# ===== SIMPLIFIED INTERFACES FOR COMMON USE CASES =====

def universal_void_dynamics(W, t, domain_modulation=1.0, use_time_dynamics=True):
    """
    Simplified interface that applies both RE-VGSP and GDSP with universal constants.
    Returns combined delta for single-step evolution.
    """
    dw_re = delta_re_vgsp(W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
    dw_gdsp = delta_gdsp(W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
    return dw_re + dw_gdsp

def get_universal_constants():
    """Returns the universal constants as a dictionary."""
    return {
        'ALPHA': ALPHA,
        'BETA': BETA,
        'F_REF': F_REF,
        'PHASE_SENS': PHASE_SENS
    }]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/void_dynamics/__init__.py</path>
      <content><![CDATA[# Expose void_dynamics API for simple imports
from .FUM_Void_Equations import universal_void_dynamics
from .FUM_Void_Debt_Modulation import VoidDebtModulation

__all__ = ["universal_void_dynamics", "VoidDebtModulation"]]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/void_dynamics/apply_revgsp.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

# FUM_AdvancedMath.fum.apply_revgsp
#
# Blueprint-compliant implementation of RE-VGSP (Resonance-Enhanced Valence-Gated Synaptic Plasticity)
# The canonical three-factor learning rule that bridges Local System (SNN) and Global System (SIE/ADC)

import torch
import numpy as np
from typing import Dict, List, Tuple, Optional
from scipy.sparse import csr_matrix, lil_matrix
import logging

# Import blueprint formulas
from .revgsp_formulas import (
    calculate_modulated_learning_rate,
    calculate_modulated_trace_decay,
    calculate_plasticity_impulse,
    update_eligibility_trace,
    calculate_weight_change
)

logger = logging.getLogger(__name__)

def extract_spike_pairs_from_synapses(
    synaptic_weights: csr_matrix,
    pre_spike_times: List[List[float]],
    post_spike_times: List[List[float]],
    time_window: float = 50.0
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Blueprint-compliant spike pair extraction for existing synapses only.
    
    Blueprint Reference: Rule 2 - RE-VGSP Local Rule Implementation
    Time Complexity: O(N × S) where N is number of existing synapses, S is average spikes per neuron
    ✅ BLUEPRINT COMPLIANT: "O(N) algorithm where N is the number of synapses"
    ✅ LOCAL RULE: "Avoiding expensive global calculations" - processes only existing connections
    
    Args:
        synaptic_weights: Sparse CSR matrix defining existing synapses
        pre_spike_times: List of spike times for each presynaptic neuron
        post_spike_times: List of spike times for each postsynaptic neuron
        time_window: Maximum time difference for spike pairing (ms)
        
    Returns:
        Tuple of (pre_indices, post_indices, delta_t, synapse_indices)
    """
    pre_indices = []
    post_indices = []
    delta_t_list = []
    synapse_indices = []
    
    # Iterate only over existing synapses (non-zero entries) - O(N) where N = nnz synapses
    cx = synaptic_weights.tocoo()  # Convert to COO for efficient iteration
    
    for synapse_idx, (pre_neuron_idx, post_neuron_idx) in enumerate(zip(cx.row, cx.col)):
        # Validate neuron indices
        if (pre_neuron_idx >= len(pre_spike_times) or
            post_neuron_idx >= len(post_spike_times)):
            continue
            
        pre_times = pre_spike_times[pre_neuron_idx]
        post_times = post_spike_times[post_neuron_idx]
        
        if len(pre_times) == 0 or len(post_times) == 0:
            continue
            
        # Find spike pairs within time window for this specific synapse
        for pre_time in pre_times:
            for post_time in post_times:
                dt = post_time - pre_time
                if abs(dt) <= time_window:
                    pre_indices.append(pre_neuron_idx)
                    post_indices.append(post_neuron_idx)
                    delta_t_list.append(dt)
                    synapse_indices.append(synapse_idx)
    
    if len(delta_t_list) == 0:
        logger.debug("No spike pairs found for existing synapses")
        return (torch.empty(0, dtype=torch.long),
                torch.empty(0, dtype=torch.long),
                torch.empty(0, dtype=torch.float32),
                torch.empty(0, dtype=torch.long))
    
    logger.debug(f"Blueprint-compliant extraction: {len(delta_t_list)} spike pairs from {len(cx.data)} synapses")
    
    return (torch.tensor(pre_indices, dtype=torch.long),
            torch.tensor(post_indices, dtype=torch.long),
            torch.tensor(delta_t_list, dtype=torch.float32),
            torch.tensor(synapse_indices, dtype=torch.long))

def extract_spike_pairs(
    pre_spike_times: List[List[float]],
    post_spike_times: List[List[float]],
    time_window: float = 50.0
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    DEPRECATED: Legacy function maintained for backward compatibility.
    
    ⚠️  BLUEPRINT VIOLATION: O(N² × S²) complexity
    Use extract_spike_pairs_from_synapses() for blueprint-compliant O(N) processing.
    
    Args:
        pre_spike_times: List of spike times for each presynaptic neuron
        post_spike_times: List of spike times for each postsynaptic neuron
        time_window: Maximum time difference for spike pairing (ms)
        
    Returns:
        Tuple of (pre_indices, post_indices, delta_t, synapse_indices)
    """
    logger.warning("Using deprecated extract_spike_pairs() - O(N²) complexity violates Blueprint Rule 2")
    logger.warning("Use extract_spike_pairs_from_synapses() for blueprint-compliant O(N) processing")
    
    pre_indices = []
    post_indices = []
    delta_t_list = []
    synapse_indices = []
    
    synapse_idx = 0
    for pre_neuron_idx, pre_times in enumerate(pre_spike_times):
        for post_neuron_idx, post_times in enumerate(post_spike_times):
            if len(pre_times) == 0 or len(post_times) == 0:
                continue
                
            # Find spike pairs within time window
            for pre_time in pre_times:
                for post_time in post_times:
                    dt = post_time - pre_time
                    if abs(dt) <= time_window:
                        pre_indices.append(pre_neuron_idx)
                        post_indices.append(post_neuron_idx)
                        delta_t_list.append(dt)
                        synapse_indices.append(synapse_idx)
            
            synapse_idx += 1
    
    if len(delta_t_list) == 0:
        return (torch.empty(0, dtype=torch.long),
                torch.empty(0, dtype=torch.long),
                torch.empty(0, dtype=torch.float32),
                torch.empty(0, dtype=torch.long))
    
    return (torch.tensor(pre_indices, dtype=torch.long),
            torch.tensor(post_indices, dtype=torch.long),
            torch.tensor(delta_t_list, dtype=torch.float32),
            torch.tensor(synapse_indices, dtype=torch.long))

def calculate_phase_from_spike_times(
    spike_times: List[float],
    reference_freq: float = 40.0,
    current_time: float = 0.0
) -> float:
    """
    Calculate neural oscillation phase from recent spike timing.
    
    Blueprint Reference: Rule 8.1 - Phase-sensitive plasticity impulse
    Time Complexity: O(1) - uses only the most recent spike for phase calculation
    
    Args:
        spike_times: Recent spike times for the neuron
        reference_freq: Reference oscillation frequency (Hz)
        current_time: Current simulation time
        
    Returns:
        Phase value in radians [0, 2π]
    """
    if len(spike_times) == 0:
        return 0.0
    
    # Use most recent spike for phase calculation
    last_spike = spike_times[-1]
    time_since_spike = current_time - last_spike
    
    # Calculate phase based on reference oscillation
    period_ms = 1000.0 / reference_freq  # Convert Hz to ms period
    phase = (2 * np.pi * time_since_spike / period_ms) % (2 * np.pi)
    
    return float(phase)

def apply_neuron_polarities(
    eligibility_traces: torch.Tensor,
    neuron_polarities: torch.Tensor,
    synaptic_weights: csr_matrix
) -> torch.Tensor:
    """
    Apply neuron polarities to eligibility traces following blueprint specifications.
    
    Blueprint Reference: Rule 2.1 - Polarity scaling of eligibility traces
    Time Complexity: O(N²) for dense matrices, O(M) for sparse matrices where M is nnz elements
    
    Args:
        eligibility_traces: Current eligibility trace matrix (sparse or dense)
        neuron_polarities: Polarity values for each neuron [-1, +1]
        synaptic_weights: Sparse weight matrix for structure reference
        
    Returns:
        Polarity-scaled eligibility traces
    """
    if isinstance(eligibility_traces, torch.Tensor):
        # Dense tensor case
        if eligibility_traces.dim() == 2:
            # Scale rows by presynaptic neuron polarities
            scaled_traces = eligibility_traces * neuron_polarities.unsqueeze(1)
        else:
            # 1D case - assume flattened matrix
            n_neurons = len(neuron_polarities)
            traces_2d = eligibility_traces.view(n_neurons, n_neurons)
            scaled_traces = traces_2d * neuron_polarities.unsqueeze(1)
            scaled_traces = scaled_traces.view(-1)
    else:
        # Sparse matrix case - convert to dense for polarity scaling
        logger.warning("Converting sparse eligibility traces to dense for polarity scaling")
        dense_traces = torch.tensor(eligibility_traces.toarray(), dtype=torch.float32)
        scaled_traces = dense_traces * neuron_polarities.unsqueeze(1)
    
    return scaled_traces

def revgsp_learning_step(
    synaptic_weights: csr_matrix,
    eligibility_traces: torch.Tensor,
    spike_data: Dict,
    total_reward: float,
    plv: float,
    neuron_polarities: torch.Tensor,
    base_eta: float = 0.01,
    base_gamma: float = 0.9,
    lambda_decay: float = 0.001,
    current_time: float = 0.0
) -> Dict[str, any]:
    """
    Complete RE-VGSP learning step implementing the three-factor rule.
    
    Blueprint Reference: Rule 2 - RE-VGSP three-factor learning
    Components: spike-timing (local) + eligibility (memory) + reward (global)
    Time Complexity: O(N × S + M) where N=existing synapses, S=avg spikes, M=non-zero weights
    ✅ BLUEPRINT COMPLIANT: "O(N) algorithm where N is the number of synapses"
    
    Args:
        synaptic_weights: Current sparse weight matrix
        eligibility_traces: Current eligibility trace matrix
        spike_data: Dict containing pre/post spike times and neuron indices
        total_reward: Global reward signal from SIE
        plv: Phase-Locking Value for resonance enhancement
        neuron_polarities: Neuron polarity values [-1, +1]
        base_eta: Base learning rate
        base_gamma: Base eligibility trace decay
        lambda_decay: Weight decay factor
        current_time: Current simulation time
        
    Returns:
        Dict containing weight updates and updated eligibility traces
    """
    start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
    if start_time:
        start_time.record()
    
    n_neurons = synaptic_weights.shape[0]
    
    # Extract spike pairs using blueprint-compliant O(N) method
    pre_indices, post_indices, delta_t, synapse_indices = extract_spike_pairs_from_synapses(
        synaptic_weights,
        spike_data.get('pre_spike_times', []),
        spike_data.get('post_spike_times', []),
        time_window=50.0
    )
    
    if len(delta_t) == 0:
        logger.debug("No spike pairs found for existing synapses in REVGSP learning")
        # Return decay-only updates
        eta_eff = calculate_modulated_learning_rate(base_eta, total_reward)
        weight_changes = -lambda_decay * torch.tensor(synaptic_weights.data, dtype=torch.float32)
        
        # 🔍 VALIDATION LOG: No spike pairs case - pure SIE modulation
        logger.debug(f"RE-VGSP decay-only: no spike pairs, eta_eff={eta_eff:.6f}, total_reward={total_reward:.6f}")
        if total_reward != 0.0:
            logger.debug("SIE-REVGSP handshake: SIE active but no local plasticity - learning suppressed")
        
        return {
            'weight_changes': weight_changes,
            'eligibility_traces': eligibility_traces * base_gamma,
            'plasticity_impulses': torch.zeros(0),
            'eta_effective': eta_eff,
            'gamma_effective': base_gamma,
            'n_spike_pairs': 0,
            'complexity_validation': 'O(N) - Blueprint compliant'
        }
    
    # Calculate phases for pre and post neurons
    pre_phases = []
    post_phases = []
    
    for i in range(len(pre_indices)):
        pre_idx = pre_indices[i].item()
        post_idx = post_indices[i].item()
        
        # Get spike times for phase calculation
        pre_spikes = spike_data.get('pre_spike_times', [[]])[pre_idx] if pre_idx < len(spike_data.get('pre_spike_times', [])) else []
        post_spikes = spike_data.get('post_spike_times', [[]])[post_idx] if post_idx < len(spike_data.get('post_spike_times', [])) else []
        
        pre_phase = calculate_phase_from_spike_times(pre_spikes, current_time=current_time)
        post_phase = calculate_phase_from_spike_times(post_spikes, current_time=current_time)
        
        pre_phases.append(pre_phase)
        post_phases.append(post_phase)
    
    pre_phases = torch.tensor(pre_phases, dtype=torch.float32)
    post_phases = torch.tensor(post_phases, dtype=torch.float32)
    
    # Calculate phase-sensitive plasticity impulses using blueprint formula from Rule 8.1
    # Formula: PI(t) = base_PI(Δt) * (1 + cos(phase_pre(t) - phase_post(t))) / 2
    plasticity_impulses = calculate_plasticity_impulse(delta_t, pre_phases, post_phases)
    
    # Validate phase-sensitive calculations are preserved
    phase_differences = pre_phases - post_phases
    phase_modulation = (1 + torch.cos(phase_differences)) / 2
    logger.debug(f"Phase modulation range: [{phase_modulation.min():.3f}, {phase_modulation.max():.3f}]")
    logger.debug(f"Mean phase difference: {phase_differences.mean():.3f} radians")
    
    # Calculate modulated parameters using PLV and reward
    eta_eff = calculate_modulated_learning_rate(base_eta, total_reward)
    gamma_eff = calculate_modulated_trace_decay(base_gamma, plv)
    
    # 🔍 VALIDATION LOG: SIE-REVGSP handshake timing and learning rate modulation
    logger.debug(f"SIE-REVGSP handshake: total_reward={total_reward:.6f} → eta_eff={eta_eff:.6f} (base={base_eta:.6f})")
    logger.debug(f"Learning rate modulation factor: {eta_eff/base_eta:.4f}x" if base_eta != 0 else "Learning rate modulation: base_eta is zero")
    
    # 🚨 HANDSHAKE VALIDATION: Check for timing correlation between SIE reward and RE-VGSP plasticity
    if len(plasticity_impulses) > 0:
        pi_timing_mean = torch.mean(torch.abs(delta_t)).item()
        logger.debug(f"Handshake timing: {len(plasticity_impulses)} plasticity impulses, mean_dt={pi_timing_mean:.2f}ms, reward_signal={total_reward:.6f}")
        
        # Check for potential timing mismatches that could disrupt learning
        if total_reward != 0.0 and len(plasticity_impulses) == 0:
            logger.warning("SIE-REVGSP timing mismatch: SIE providing reward but no RE-VGSP plasticity impulses")
        elif total_reward == 0.0 and len(plasticity_impulses) > 0:
            logger.warning("SIE-REVGSP timing mismatch: RE-VGSP plasticity active but no SIE reward signal")
    
    # 🔍 LEARNING RATE VALIDATION: Check for extreme modulation that could destabilize learning
    modulation_factor = eta_eff / base_eta if base_eta != 0 else float('inf')
    if modulation_factor > 5.0:
        logger.warning(f"Extreme learning rate amplification: {modulation_factor:.2f}x - SIE reward may be too high")
    elif modulation_factor < 0.1:
        logger.warning(f"Severe learning rate suppression: {modulation_factor:.2f}x - SIE reward may be too low")
    
    # Update eligibility traces
    if len(plasticity_impulses) > 0:
        # Create sparse plasticity impulse matrix
        pi_matrix = torch.zeros((n_neurons, n_neurons), dtype=torch.float32)
        for i, (pre_idx, post_idx, pi) in enumerate(zip(pre_indices, post_indices, plasticity_impulses)):
            pi_matrix[pre_idx, post_idx] += pi
        
        # Update eligibility traces using blueprint formula
        eligibility_traces = update_eligibility_trace(
            eligibility_traces, 
            pi_matrix.view(-1) if eligibility_traces.dim() == 1 else pi_matrix,
            gamma_eff
        )
    else:
        # Decay-only update
        eligibility_traces = eligibility_traces * gamma_eff
    
    # Apply neuron polarities to eligibility traces
    scaled_eligibility = apply_neuron_polarities(
        eligibility_traces, 
        neuron_polarities, 
        synaptic_weights
    )
    
    # Calculate weight changes using blueprint formula
    current_weights = torch.tensor(synaptic_weights.data, dtype=torch.float32)
    if scaled_eligibility.dim() == 2:
        # Flatten for weight change calculation
        scaled_eligibility_flat = scaled_eligibility.view(-1)
    else:
        scaled_eligibility_flat = scaled_eligibility
    
    # Only use non-zero weight entries
    nonzero_mask = current_weights != 0
    weight_changes = torch.zeros_like(current_weights)
    
    if torch.any(nonzero_mask):
        weight_changes[nonzero_mask] = calculate_weight_change(
            scaled_eligibility_flat[nonzero_mask],
            current_weights[nonzero_mask], 
            eta_eff,
            lambda_decay
        )
    
    if start_time:
        end_time = torch.cuda.Event(enable_timing=True)
        end_time.record()
        torch.cuda.synchronize()
        elapsed_ms = start_time.elapsed_time(end_time)
        logger.debug(f"REVGSP learning step: {elapsed_ms:.3f}ms, {len(plasticity_impulses)} spike pairs")
    
    # Validation logging for blueprint compliance
    cx = synaptic_weights.tocoo()
    logger.info(f"REVGSP: eta_eff={eta_eff:.6f}, gamma_eff={gamma_eff:.6f}, PLV={plv:.4f}, reward={total_reward:.6f}")
    logger.info(f"Blueprint compliance: O(N) where N={len(cx.data)} synapses, processed {len(plasticity_impulses)} spike pairs")
    
    return {
        'weight_changes': weight_changes,
        'eligibility_traces': eligibility_traces,
        'plasticity_impulses': plasticity_impulses,
        'eta_effective': eta_eff,
        'gamma_effective': gamma_eff,
        'n_spike_pairs': len(plasticity_impulses),
        'pre_indices': pre_indices,
        'post_indices': post_indices,
        'delta_t': delta_t,
        'complexity_validation': f'O(N) - Blueprint compliant, N={len(cx.data)} synapses',
        'phase_sensitivity_preserved': True
    }

def adapt_connectome_revgsp(
    substrate_state: Dict,
    sie_signals: Dict,
    adc_territories: Dict,
    timestep: int
) -> Dict[str, any]:
    """
    High-level connectome adaptation using RE-VGSP algorithm.
    
    Blueprint Reference: Complete RE-VGSP pipeline for connectome evolution
    
    Args:
        substrate_state: Current substrate state including weights and traces
        sie_signals: SIE outputs including total_reward and stability metrics
        adc_territories: ADC territory mapping and PLV calculations
        timestep: Current simulation timestep
        
    Returns:
        Updated substrate state and learning metrics
    """
    # Extract state components
    synaptic_weights = substrate_state['synaptic_weights']
    eligibility_traces = substrate_state['eligibility_traces']
    neuron_polarities = substrate_state.get('neuron_polarities', torch.ones(synaptic_weights.shape[0]))
    
    # Extract SIE signals
    total_reward = sie_signals.get('total_reward', 0.0)
    
    # Extract ADC PLV (Phase-Locking Value for resonance)
    plv = adc_territories.get('plv', 0.5)  # Default PLV if not provided
    
    # Prepare spike data from substrate
    spike_data = {
        'pre_spike_times': substrate_state.get('recent_spike_times', []),
        'post_spike_times': substrate_state.get('recent_spike_times', [])
    }
    
    # Apply RE-VGSP learning
    learning_results = revgsp_learning_step(
        synaptic_weights=synaptic_weights,
        eligibility_traces=eligibility_traces,
        spike_data=spike_data,
        total_reward=total_reward,
        plv=plv,
        neuron_polarities=neuron_polarities,
        current_time=float(timestep)
    )
    
    # Update substrate state
    updated_state = substrate_state.copy()
    updated_state['eligibility_traces'] = learning_results['eligibility_traces']
    
    # Apply weight changes to sparse matrix
    weight_changes = learning_results['weight_changes']
    if len(weight_changes) > 0:
        # Convert to sparse format for efficient updates
        updated_weights = synaptic_weights.copy()
        updated_weights.data += weight_changes.numpy()
        
        # Ensure no negative weights (optional constraint)
        updated_weights.data = np.maximum(updated_weights.data, 0.0)
        
        updated_state['synaptic_weights'] = updated_weights
    
    return {
        'updated_state': updated_state,
        'learning_metrics': {
            'eta_effective': learning_results['eta_effective'],
            'gamma_effective': learning_results['gamma_effective'],
            'n_spike_pairs': learning_results['n_spike_pairs'],
            'total_reward': total_reward,
            'plv': plv,
            'weight_change_magnitude': torch.sum(torch.abs(weight_changes)).item()
        }
    }

def apply_revgsp_updates(W, spike_times, time_step, eta, mod_factor, lambda_decay, params, is_excitatory):
    """
    Blueprint-compliant RE-VGSP (Resonance-Enhanced Valence-Gated Synaptic Plasticity) learning updates.
    
    Blueprint Reference: Rule 2 - The Learning Rule: The RE-VGSP "Handshake"
    Blueprint Reference: Rule 2.1 - Terminology: Plasticity Impulse vs. Eligibility Trace
    Blueprint Reference: Rule 8.1 - UTE: Spatio-Temporal-Polarity-Phase Volume Encoding
    Time Complexity: O(N × S + M) where N=existing synapses, S=avg spikes, M=non-zero weights
    ✅ BLUEPRINT COMPLIANT: "RE-VGSP is an O(N) algorithm where N is the number of synapses"
    
    RE-VGSP Formula: Δw_ij = (eta_effective(total_reward) * e_ij(t)) - (lambda_decay * w_ij)
    Where: e_ij(t) = gamma(PLV) * e_ij(t-1) + PI(t)
    And: PI(t) = base_PI(Δt) * (1 + cos(phase_pre(t) - phase_post(t))) / 2 [Rule 8.1]
    
    Parameters:
        W: Sparse synaptic weight matrix (input)
        spike_times: List of spike times per neuron (input)
        time_step: Current simulation time (input)
        eta: Base learning rate (input) - modulated by total_reward to create eta_effective
        mod_factor: SIE total_reward signal for reinforcement (input)
        lambda_decay: Weight decay factor for homeostatic stability (input)
        params: REVGSP parameters dict (input)
        is_excitatory: Boolean array for neuron polarities (input)
        
    Returns:
        Tuple[scipy.sparse.csc_matrix, dict]: Updated weights and metrics
    """
    logger.info("🔄 RE-VGSP learning updates initiated")
    
    # Convert to blueprint-compliant format
    from scipy.sparse import csr_matrix, csc_matrix
    import torch
    
    # Ensure sparse format for blueprint efficiency
    if not isinstance(W, csr_matrix):
        W = csr_matrix(W)
    
    # Create RE-VGSP substrate state
    n_neurons = W.shape[0]
    eligibility_traces = torch.zeros(n_neurons, n_neurons)
    neuron_polarities = torch.tensor([1.0 if exc else -1.0 for exc in is_excitatory], dtype=torch.float32)
    
    # Prepare spike data for RE-VGSP interface
    time_window = 250.0  # ms
    pre_spike_times = []
    post_spike_times = []
    
    for neuron_spikes in spike_times:
        recent_spikes = [s for s in neuron_spikes if s > (time_step - time_window)]
        pre_spike_times.append(recent_spikes)
        post_spike_times.append(recent_spikes)
    
    spike_data = {
        'pre_spike_times': pre_spike_times,
        'post_spike_times': post_spike_times
    }
    
    # Map mod_factor to RE-VGSP total_reward (Blueprint Rule 3: SIE interaction)
    total_reward = mod_factor
    
    # Execute blueprint-compliant RE-VGSP learning step
    learning_results = revgsp_learning_step(
        synaptic_weights=W,
        eligibility_traces=eligibility_traces,
        spike_data=spike_data,
        total_reward=total_reward,
        plv=0.5,  # Default Phase-Locking Value for resonance enhancement
        neuron_polarities=neuron_polarities,
        base_eta=eta,
        lambda_decay=lambda_decay,
        current_time=float(time_step)
    )
    
    # Apply RE-VGSP weight changes
    weight_changes = learning_results['weight_changes']
    if len(weight_changes) > 0:
        updated_W = W.copy()
        
        # 🔧 FIXED: Apply E/I constraints to weight changes BEFORE adding to weights
        # This prevents fake potentiation in decay-only scenarios
        constrained_changes = weight_changes.numpy().copy()
        
        # Get current weights in dense format for constraint logic
        W_dense = W.toarray()
        inhib_indices = np.where(is_excitatory == False)[0]
        excit_indices = np.where(is_excitatory == True)[0]
        
        # For each synapse, check if the weight change would violate E/I constraints
        for i, (row_idx, col_idx) in enumerate(zip(*W.nonzero())):
            current_weight = W_dense[row_idx, col_idx]
            proposed_change = constrained_changes[i]
            new_weight = current_weight + proposed_change
            
            # Apply E/I constraints to the final weight
            if row_idx in inhib_indices:
                # Inhibitory synapse - ensure weight stays ≤ 0
                constrained_weight = min(new_weight, 0.0)
            else:
                # Excitatory synapse - ensure weight stays ≥ 0
                constrained_weight = max(new_weight, 0.0)
            
            # Calculate the actual allowed change
            constrained_changes[i] = constrained_weight - current_weight
        
        # Apply the constrained changes
        updated_W.data += constrained_changes
        
        # Apply magnitude clipping and remove self-connections
        updated_W.data = np.clip(updated_W.data, -2.0, 2.0)
        # Remove self-connections by setting diagonal to 0
        updated_W.setdiag(0)
        updated_W.prune()
    else:
        updated_W = W
    
    # Convert RE-VGSP metrics to compatible format
    net_change = updated_W.toarray() - W.toarray()
    revgsp_metrics = {
        'net_weight_change': np.sum(net_change),
        'potentiated_synapses': np.sum(net_change > 1e-9),
        'depressed_synapses': np.sum(net_change < -1e-9),
        'eta_effective': learning_results['eta_effective'],
        'n_spike_pairs': learning_results['n_spike_pairs'],
        'revgsp_complexity': learning_results['complexity_validation'],
        'phase_sensitivity_preserved': learning_results.get('phase_sensitivity_preserved', True)
    }
    
    logger.info(f"RE-VGSP: eta_eff={revgsp_metrics['eta_effective']:.6f}, {revgsp_metrics['revgsp_complexity']}")
    
    return updated_W, revgsp_metrics]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/void_dynamics/diagnostics_formulas.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

# FUM_AdvancedMath.fum.diagnostics_formulas
#
# Provides the pure, canonical implementations of the mathematical formulas
# for the Introspection Probe's pathology detection and the ADC's adaptive scheduling.

import torch

def calculate_pathology_score(spike_rates: torch.Tensor, output_diversity: torch.Tensor) -> float:
    """
    Calculates the pathology score for a locus (subgraph) to identify
    inefficient, high-activity, low-output regions.

    Ref: Blueprint Rule 4.1
    Time Complexity: O(k) where k is the number of nodes in the locus.
    """
    return torch.mean(spike_rates * (1 - output_diversity)).item()

def calculate_graph_entropy(degree_distribution: torch.Tensor) -> float:
    """
    Calculates the entropy of the graph based on its degree distribution.
    Used by Introspection Probe for global health monitoring and ADC for scheduling.

    Ref: Blueprint Rule 4.1
    Time Complexity: O(N) where N is the number of nodes in the graph.
    """
    # Normalize the distribution to get probabilities
    p = degree_distribution / torch.sum(degree_distribution)
    # Filter out zero probabilities to avoid log(0)
    p = p[p > 0]
    return -torch.sum(p * torch.log(p)).item()

def calculate_cartography_time(graph_entropy: float, alpha: float, base_interval: int = 100000) -> int:
    """
    Calculates the timestep for the next scheduled ADC cartography event,
    based on the current graph entropy.

    Ref: Blueprint Rule 7
    Time Complexity: O(1)
    """
    t_territory = base_interval * torch.exp(torch.tensor(-alpha * graph_entropy))
    return int(t_territory)]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/void_dynamics/revgsp_formulas.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

# FUM_AdvancedMath.fum.revgsp_formulas
#
# Provides the pure, canonical implementations of the mathematical formulas
# for the RE-VGSP learning rule.

import torch

def calculate_modulated_learning_rate(base_eta: float, total_reward: float) -> float:
    """
    Calculates the effective learning rate, modulated by the global reward signal.
    A positive reward enables learning; a negative reward could enable anti-learning.
    
    Ref: Blueprint Rule 2, `eta_effective(total_reward)`
    Time Complexity: O(1)
    """
    # Simple linear scaling, but could be more complex (e.g., sigmoid)
    return base_eta * total_reward

def calculate_modulated_trace_decay(base_gamma: float, plv: float) -> float:
    """
    Calculates the effective eligibility trace decay factor, modulated by the
    local network resonance (Phase-Locking Value). High resonance (high PLV)
    should lead to more stable traces (higher gamma, closer to 1.0).
    
    Ref: Blueprint Rule 2, `gamma(PLV)`
    Time Complexity: O(1)
    """
    # Simple scaling: if plv is 1.0 (perfect sync), gamma is base_gamma.
    # If plv is 0.0 (no sync), gamma is lower, making traces decay faster.
    return base_gamma * (0.5 + (0.5 * plv))

def calculate_plasticity_impulse(delta_t: torch.Tensor, phase_pre: torch.Tensor, phase_post: torch.Tensor) -> torch.Tensor:
    """
    Calculates the phase-sensitive Plasticity Impulse (PI) for a batch of
    pre-post spike pairs.

    Ref: Blueprint Rule 8.1
    Time Complexity: O(k) where k is the number of spike pairs.
    """
    base_pi = torch.exp(-torch.abs(delta_t) / 10.0)
    phase_difference_cosine = torch.cos(phase_pre - phase_post)
    phase_modulation = (1 + phase_difference_cosine) / 2
    return base_pi * phase_modulation

def update_eligibility_trace(e_ij_prev: torch.Tensor, pi: torch.Tensor, gamma_eff: float) -> torch.Tensor:
    """
    Updates the eligibility traces for a batch of synapses.

    Ref: Blueprint Rule 2 & 2.1
    Time Complexity: O(N) where N is number of synapses.
    """
    return (gamma_eff * e_ij_prev) + pi

def calculate_weight_change(e_ij: torch.Tensor, w_ij: torch.Tensor, eta_eff: float, lambda_decay: float) -> torch.Tensor:
    """
    Calculates the final weight change for a batch of synapses using the
    effective learning rate and eligibility traces.

    Ref: Blueprint Rule 2
    Time Complexity: O(N) where N is number of synapses.
    """
    reinforcement = eta_eff * e_ij
    decay = lambda_decay * w_ij
    return reinforcement - decay]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/void_dynamics/sie_formulas.py</path>
      <content><![CDATA[# FUM_AdvancedMath.fum.sie_formulas
#
# Provides the pure, canonical implementations of the mathematical formulas
# for the Self-Improvement Engine (SIE).
# 
# Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.
#
# This research is protected under a dual-license to foster open academic
# research while ensuring commercial applications are aligned with the project's 
# ethical principles. 
# Commercial use requires written permission from Justin K. Lietz. 
# See LICENSE file for full terms.
# ===========================================================================

import torch
import logging
import time

logger = logging.getLogger(__name__)

def calculate_td_error(V_current: float, R_external: float, V_next: float, gamma: float) -> float:
    """
    Calculates the Temporal Difference (TD) error for a state transition.
    Ref: Blueprint Rule 3 (Component of the SIE)
    Time Complexity: O(1)
    """
    return R_external + (gamma * V_next) - V_current

def calculate_novelty_score(N_s: int) -> float:
    """
    Calculates the novelty score for a state based on its visitation count.
    Ref: Blueprint Rule 3 (Component of the SIE)
    Time Complexity: O(1)
    """
    # Inverse visitation count, add epsilon for stability
    return 1.0 / (N_s + 1e-6)

def calculate_habituation_score(recent_count: int, history_length: int) -> float:
    """
    Calculates a habituation score based on the frequency of the current
    input in a recent history, as per Learning_and_Guidance.md
    Ref: Blueprint Rule 3 (Component of the SIE)
    Time Complexity: O(1)
    """
    if history_length == 0:
        return 0.0
    
    return min(recent_count / history_length, 1.0)

def calculate_hsi(firing_rates: torch.Tensor, target_var: float) -> float:
    """
    Calculates the Homeostatic Stability Index (HSI).
    Ref: Blueprint Rule 3.1 & FUM Nomenclature
    Time Complexity: O(N) where N is number of neurons.
    """
    current_var = torch.var(firing_rates)
    return 1.0 - (torch.abs(current_var - target_var) / target_var)

def calculate_total_reward(w_td: float, td_error_norm: float,
                           w_nov: float, novelty_norm: float,
                           w_hab: float, habituation_norm: float,
                           w_hsi: float, hsi_norm: float) -> float:
    """
    Calculates the composite total_reward signal from its four weighted,
    normalized components.
    Ref: Blueprint Rule 3
    Time Complexity: O(1)
    """
    reward = (w_td * td_error_norm +
              w_nov * novelty_norm -
              w_hab * habituation_norm +
              w_hsi * hsi_norm)
    
    # 🔍 VALIDATION LOG: Track SIE-REVGSP handshake signal for timing analysis
    logger.debug(f"SIE total_reward: {reward:.6f} [TD:{w_td*td_error_norm:.4f}, NOV:{w_nov*novelty_norm:.4f}, HAB:{-w_hab*habituation_norm:.4f}, HSI:{w_hsi*hsi_norm:.4f}]")
    
    # 🚨 HANDSHAKE VALIDATION: Log extreme values that could disrupt RE-VGSP learning
    if abs(reward) > 10.0:
        logger.warning(f"SIE total_reward extreme value: {reward:.6f} - may cause RE-VGSP eta_eff instability")
    elif abs(reward) < 0.01:
        logger.warning(f"SIE total_reward very small: {reward:.6f} - may cause weak RE-VGSP learning")
    
    return reward]]></content>
    </file>
    <file>
      <path>tools/voidkit_tools/void_dynamics/tda_formulas.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""

# FUM_AdvancedMath.fum.tda_formulas
#
# Blueprint-compliant TDA formulas implementing the EHTP three-stage pipeline
# with performance optimization and pathology scoring integration.

import numpy as np
import torch
from typing import Dict, List, Tuple, Optional
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import connected_components
import logging

# Import existing pathology score function from diagnostics_formulas
from .diagnostics_formulas import calculate_pathology_score

logger = logging.getLogger(__name__)

def ehtp_stage1_cohesion_check(adjacency_matrix: csr_matrix) -> Dict[str, any]:
    """
    EHTP Stage 1: Fast O(N+M) cohesion check for connectome fragmentation.
    
    Blueprint Reference: Rule 4 - EHTP Stage 1: Cohesion Check (CCC)
    Performance: O(N+M) - Linear in nodes and edges
    
    Args:
        adjacency_matrix: Sparse connectivity matrix of the network
        
    Returns:
        Dict containing:
        - n_components: Number of disconnected components
        - component_labels: Array mapping each node to its component
        - fragmented: Boolean indicating if network is fragmented
        - largest_component_size: Size of largest connected component
    """
    start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
    if start_time:
        start_time.record()
    
    n_components, component_labels = connected_components(
        adjacency_matrix, directed=False, return_labels=True
    )
    
    # Calculate component statistics
    component_sizes = np.bincount(component_labels)
    largest_component_size = np.max(component_sizes)
    fragmented = n_components > 1
    
    if start_time:
        end_time = torch.cuda.Event(enable_timing=True)
        end_time.record()
        torch.cuda.synchronize()
        elapsed_ms = start_time.elapsed_time(end_time)
        logger.debug(f"EHTP Stage 1 timing: {elapsed_ms:.3f}ms for {adjacency_matrix.shape[0]} nodes")
    
    logger.info(f"EHTP Stage 1: {n_components} components, largest: {largest_component_size} nodes, fragmented: {fragmented}")
    
    return {
        'n_components': n_components,
        'component_labels': component_labels,
        'fragmented': fragmented,
        'largest_component_size': largest_component_size,
        'component_sizes': component_sizes
    }

def ehtp_stage2_locus_search(
    adjacency_matrix: csr_matrix,
    spike_rates: torch.Tensor,
    pathology_threshold: float = 0.7,
    max_locus_size: int = 100
) -> Optional[List[int]]:
    """
    EHTP Stage 2: Hierarchical locus search to identify suspect subgraphs.
    
    Blueprint Reference: Rule 4 - Pathology Score = Avg Firing Rate × (1 - Output Diversity)
    Performance: O(N × k) where k is average node degree
    
    Args:
        adjacency_matrix: Sparse connectivity matrix
        spike_rates: Tensor of firing rates for each neuron
        pathology_threshold: Minimum pathology score to trigger Stage 3
        max_locus_size: Maximum size of locus to analyze (performance guard)
        
    Returns:
        List of node indices forming the suspect locus, or None if no pathology found
    """
    n_nodes = adjacency_matrix.shape[0]
    
    # Performance guard: Skip if network too large without fragmentation
    if n_nodes > 2000:
        logger.warning(f"EHTP Stage 2: Network too large ({n_nodes} nodes), skipping locus search")
        return None
    
    # Calculate output diversity for each node using downstream connections
    output_diversity = torch.zeros(n_nodes)
    
    for node_idx in range(n_nodes):
        # Get downstream neighbors
        downstream_neighbors = adjacency_matrix[node_idx].nonzero()[1]
        
        if len(downstream_neighbors) > 1:
            # Calculate Shannon entropy of downstream firing rates
            downstream_rates = spike_rates[downstream_neighbors]
            
            # Normalize to probabilities (avoid division by zero)
            rate_sum = torch.sum(downstream_rates)
            if rate_sum > 1e-8:
                p = downstream_rates / rate_sum
                # Filter out zero probabilities
                p = p[p > 1e-8]
                if len(p) > 1:
                    entropy = -torch.sum(p * torch.log(p))
                    # Normalize entropy by maximum possible entropy
                    max_entropy = torch.log(torch.tensor(len(p), dtype=torch.float32))
                    output_diversity[node_idx] = entropy / max_entropy if max_entropy > 0 else 0.0
    
    # Calculate individual pathology scores for node selection
    pathology_scores = spike_rates * (1 - output_diversity)
    
    # Calculate overall pathology score using existing function for logging
    pathology_score_overall = calculate_pathology_score(spike_rates, output_diversity)
    
    # Find nodes with high pathology scores
    high_pathology_nodes = torch.where(pathology_scores > pathology_threshold)[0]
    
    if len(high_pathology_nodes) == 0:
        logger.debug("EHTP Stage 2: No pathological nodes found")
        return None
    
    # Build locus by including high pathology nodes and their immediate neighbors
    locus_nodes = set(high_pathology_nodes.tolist())
    
    # Add immediate neighbors to form cohesive locus
    for node_idx in high_pathology_nodes:
        neighbors = adjacency_matrix[node_idx].nonzero()[1]
        locus_nodes.update(neighbors.tolist())
        
        # Performance guard: Limit locus size
        if len(locus_nodes) > max_locus_size:
            break
    
    locus_list = sorted(list(locus_nodes))[:max_locus_size]
    
    avg_pathology = torch.mean(pathology_scores[high_pathology_nodes]).item()
    logger.info(f"EHTP Stage 2: Found locus with {len(locus_list)} nodes, avg pathology: {avg_pathology:.4f}, overall: {pathology_score_overall:.4f}")
    
    return locus_list

def ehtp_stage3_deep_tda(
    adjacency_matrix: csr_matrix,
    locus_nodes: List[int],
    b1_persistence_threshold: float = 0.1
) -> Dict[str, any]:
    """
    EHTP Stage 3: Deep TDA analysis on small suspect locus only.
    
    Blueprint Reference: Rule 4 - O(n³) TDA analysis where n << N
    Performance: O(n³) where n is locus size (typically < 100 nodes)
    
    Args:
        adjacency_matrix: Full network adjacency matrix
        locus_nodes: List of node indices forming the suspect locus
        b1_persistence_threshold: Minimum B1 persistence to indicate inefficient cycles
        
    Returns:
        Dict containing TDA results and repair recommendations
    """
    if len(locus_nodes) > 200:
        logger.warning(f"EHTP Stage 3: Locus too large ({len(locus_nodes)} nodes), truncating to 200")
        locus_nodes = locus_nodes[:200]
    
    start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
    if start_time:
        start_time.record()
    
    # Extract locus subgraph
    locus_adj = adjacency_matrix[np.ix_(locus_nodes, locus_nodes)]
    
    # Convert sparse to dense for TDA (only small locus, so acceptable)
    locus_dense = locus_adj.toarray().astype(np.float32)
    
    # Apply TDA using existing functions (but only on small locus)
    try:
        from ..tda.compute_persistent_homology import compute_persistent_homology
        from ..tda.calculate_tda_metrics import calculate_tda_metrics
        
        # Compute persistent homology on locus distance matrix
        ph_result = compute_persistent_homology(
            locus_dense, 
            max_dim=1, 
            is_distance_matrix=True
        )
        
        # Calculate TDA metrics
        tda_metrics = calculate_tda_metrics(ph_result['dgms'])
        
        # Determine if topological repair is needed
        total_b1_persistence = tda_metrics.get('total_b1_persistence', 0.0)
        inefficient_cycles = total_b1_persistence > b1_persistence_threshold
        
        repair_needed = inefficient_cycles
        repair_type = "topological_pruning" if inefficient_cycles else None
        
    except Exception as e:
        logger.error(f"EHTP Stage 3 TDA computation failed: {e}")
        # Fallback: recommend repair based on locus size
        repair_needed = len(locus_nodes) > 50
        repair_type = "size_reduction" if repair_needed else None
        total_b1_persistence = 0.0
    
    if start_time:
        end_time = torch.cuda.Event(enable_timing=True)
        end_time.record()
        torch.cuda.synchronize()
        elapsed_ms = start_time.elapsed_time(end_time)
        logger.debug(f"EHTP Stage 3 timing: {elapsed_ms:.3f}ms for {len(locus_nodes)} locus nodes")
    
    logger.info(f"EHTP Stage 3: B1 persistence: {total_b1_persistence:.6f}, repair needed: {repair_needed}")
    
    return {
        'locus_size': len(locus_nodes),
        'locus_nodes': locus_nodes,
        'total_b1_persistence': total_b1_persistence,
        'repair_needed': repair_needed,
        'repair_type': repair_type,
        'inefficient_cycles': inefficient_cycles
    }

def optimized_ehtp_pipeline(
    adjacency_matrix: csr_matrix,
    spike_rates: torch.Tensor,
    pathology_threshold: float = 0.7,
    b1_persistence_threshold: float = 0.1,
    max_locus_size: int = 100
) -> Dict[str, any]:
    """
    Complete EHTP pipeline optimized for performance following blueprint specifications.
    
    Blueprint Reference: Rule 4 - Three-stage EHTP diagnostic pipeline
    
    Performance Optimization:
    - Stage 1: O(N+M) only - always runs
    - Stage 2: O(N×k) only if Stage 1 passes - skipped for very large networks
    - Stage 3: O(n³) only on small loci where n << N - performance bounded
    
    Args:
        adjacency_matrix: Sparse connectivity matrix of the network
        spike_rates: Tensor of firing rates for each neuron
        pathology_threshold: Pathology score threshold for Stage 2
        b1_persistence_threshold: B1 persistence threshold for Stage 3
        max_locus_size: Maximum locus size for performance protection
        
    Returns:
        Complete EHTP analysis results with repair recommendations
    """
    total_start = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
    if total_start:
        total_start.record()
    
    n_nodes = adjacency_matrix.shape[0]
    logger.info(f"EHTP Pipeline: Starting analysis on {n_nodes} nodes, {adjacency_matrix.nnz} edges")
    
    # Stage 1: Always run cohesion check (fast O(N+M))
    stage1_results = ehtp_stage1_cohesion_check(adjacency_matrix)
    
    # Early termination for highly fragmented networks
    if stage1_results['n_components'] > 10:
        logger.warning(f"EHTP: Network highly fragmented ({stage1_results['n_components']} components), recommending connectivity repair")
        return {
            'stage1': stage1_results,
            'stage2': None,
            'stage3': None,
            'repair_needed': True,
            'repair_type': 'connectivity_repair',
            'early_termination': True
        }
    
    # Stage 2: Run locus search if network not too fragmented
    stage2_results = ehtp_stage2_locus_search(
        adjacency_matrix, 
        spike_rates, 
        pathology_threshold, 
        max_locus_size
    )
    
    stage3_results = None
    
    # Stage 3: Run expensive TDA only if suspect locus found
    if stage2_results is not None:
        stage3_results = ehtp_stage3_deep_tda(
            adjacency_matrix,
            stage2_results,
            b1_persistence_threshold
        )
    
    # Determine overall repair recommendation
    repair_needed = False
    repair_type = None
    
    if stage1_results['fragmented']:
        repair_needed = True
        repair_type = 'connectivity_repair'
    elif stage3_results and stage3_results['repair_needed']:
        repair_needed = True
        repair_type = stage3_results['repair_type']
    
    if total_start:
        end_time = torch.cuda.Event(enable_timing=True)
        end_time.record()
        torch.cuda.synchronize()
        total_ms = total_start.elapsed_time(end_time)
        logger.info(f"EHTP Pipeline: Total time {total_ms:.3f}ms, repair needed: {repair_needed}")
    
    return {
        'stage1': stage1_results,
        'stage2': stage2_results,
        'stage3': stage3_results,
        'repair_needed': repair_needed,
        'repair_type': repair_type,
        'early_termination': False
    }]]></content>
    </file>
    <file>
      <path>tools/web_browser_tool.py</path>
      <content><![CDATA[# selfprompter/tools/web_browser_tool.py

import requests
from bs4 import BeautifulSoup
from typing import Dict, Any, Optional
from .tool_base import Tool, ToolResult

class WebBrowserTool(Tool):
    """
    Tool for fetching and parsing web page content.
    Follows Anthropic Claude tool use standards.
    """

    def __init__(self, user_agent: Optional[str] = None):
        """
        Initialize with optional custom user agent.
        
        Args:
            user_agent: Custom User-Agent string for requests
        """
        self.user_agent = user_agent or "AnthropicClaudeTool/1.0"
        self.headers = {"User-Agent": self.user_agent}

    @property
    def name(self) -> str:
        return "web_browser"

    @property
    def description(self) -> str:
        return (
            "Fetches and parses web page content. Can extract text content, follow links, "
            "and handle different content types. Returns cleaned and formatted page content."
        )

    @property
    def input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "The URL to fetch content from"
                },
                "extract_type": {
                    "type": "string",
                    "description": "Type of content to extract",
                    "enum": ["text", "links", "title"],
                    "default": "text"
                },
                "timeout": {
                    "type": "integer",
                    "description": "Request timeout in seconds",
                    "minimum": 1,
                    "maximum": 30,
                    "default": 10
                }
            },
            "required": ["url"],
            "additionalProperties": False
        }

    def run(self, tool_call_id: str, **kwargs) -> ToolResult:
        """
        Fetch and parse web page content.
        
        Args:
            tool_call_id: Unique ID for this tool call
            url: The URL to fetch
            extract_type: Type of content to extract (default: "text")
            timeout: Request timeout in seconds (default: 10)
            
        Returns:
            ToolResult containing parsed content or error message
        """
        try:
            url = kwargs.get("url")
            if not url:
                raise ValueError("URL is required")

            extract_type = kwargs.get("extract_type", "text").lower()
            timeout = min(max(1, kwargs.get("timeout", 10)), 30)

            response = requests.get(url, headers=self.headers, timeout=timeout)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            if extract_type == "text":
                content = self._extract_text(soup)
            elif extract_type == "links":
                content = self._extract_links(soup)
            elif extract_type == "title":
                content = self._extract_title(soup)
            else:
                raise ValueError(f"Invalid extract_type: {extract_type}")

            return self.format_result(tool_call_id, content)

        except Exception as e:
            return self.format_error(tool_call_id, str(e))

    def _extract_text(self, soup: BeautifulSoup) -> str:
        """Extract main text content from page."""
        # Remove script and style elements
        for element in soup(["script", "style"]):
            element.decompose()

        # Get text and clean it up
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = "\n".join(chunk for chunk in chunks if chunk)

        return text

    def _extract_links(self, soup: BeautifulSoup) -> str:
        """Extract all links from page."""
        links = []
        for link in soup.find_all("a"):
            href = link.get("href")
            text = link.get_text().strip()
            if href and text:
                links.append(f"{text}: {href}")

        return "\n".join(links) if links else "No links found"

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """Extract page title."""
        title = soup.find("title")
        return title.get_text() if title else "No title found"
]]></content>
    </file>
    <file>
      <path>tools/web_search_tool.py</path>
      <content><![CDATA[# selfprompter/tools/web_search_tool.py
"""
Web search tool implementation using direct HTTP requests and BeautifulSoup.
Simpler approach without requiring external API keys.
"""

import requests
from bs4 import BeautifulSoup
from typing import Dict, Any, List
from .tool_base import Tool

class WebSearchTool(Tool):
    """Tool for performing web searches via direct HTTP requests."""

    def __init__(self):
        """Initialize search tool with default headers."""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    @property
    def name(self) -> str:
        return "web_search"

    @property
    def description(self) -> str:
        return "Search the web and extract relevant information from web pages."

    @property
    def input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query or URL to fetch"
                },
                "max_results": {
                    "type": "integer",
                    "description": "Maximum number of results to return",
                    "default": 3
                }
            },
            "required": ["query"]
        }

    def run(self, input: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute web search or fetch URL content.
        
        Args:
            input: Dictionary containing:
                query: Search query or URL
                max_results: Maximum results to return
            
        Returns:
            Dictionary containing extracted content
        """
        try:
            query = input.get("query")
            if not query:
                return {
                    "type": "tool_response",
                    "content": "Error: Query is required"
                }

            # If query is a URL, fetch it directly
            if query.startswith(('http://', 'https://')):
                return self._fetch_url(query)

            # Otherwise treat as search query
            max_results = min(max(1, input.get("max_results", 3)), 10)
            return self._search(query, max_results)

        except Exception as e:
            return {
                "type": "tool_response",
                "content": f"Error: {str(e)}"
            }

    def _fetch_url(self, url: str) -> Dict[str, Any]:
        """Fetch and extract content from a URL."""
        response = requests.get(url, headers=self.headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
            
        # Extract text content
        text = soup.get_text(separator='\n', strip=True)
        
        # Extract title
        title = soup.title.string if soup.title else ""
        
        return {
            "type": "tool_response",
            "content": f"Title: {title}\n\nContent:\n{text[:1000]}..."  # Limit content length
        }

    def _search(self, query: str, max_results: int) -> Dict[str, Any]:
        """Perform a web search and return results."""
        # Start with known documentation URLs for Anthropic/Claude queries
        if "anthropic" in query.lower() or "claude" in query.lower():
            docs_urls = [
                "https://docs.anthropic.com/claude/docs",
                "https://docs.anthropic.com/claude/reference",
                "https://console.anthropic.com/docs/api"
            ]
            results = []
            for url in docs_urls[:max_results]:
                try:
                    response = requests.get(url, headers=self.headers, timeout=10)
                    if response.ok:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        title = soup.title.string if soup.title else url
                        results.append({
                            "title": title,
                            "url": url,
                            "snippet": soup.get_text(separator=' ', strip=True)[:200] + "..."
                        })
                except:
                    continue

            if results:
                return {
                    "type": "tool_response",
                    "content": self._format_results(results)
                }

        # Fallback message for other queries
        return {
            "type": "tool_response",
            "content": "For non-Anthropic queries, please provide a specific URL to fetch content from."
        }

    def _format_results(self, results: List[Dict[str, str]]) -> str:
        """Format search results into readable text."""
        if not results:
            return "No results found."

        formatted = []
        for i, result in enumerate(results, 1):
            formatted.append(f"{i}. {result['title']}")
            formatted.append(f"   URL: {result['url']}")
            formatted.append(f"   {result['snippet']}")
            formatted.append("")

        return "\n".join(formatted)
]]></content>
    </file>
    <file>
      <path>wrappers/__init__.py</path>
      <content><![CDATA[# Package initializer for wrappers
# Expose primary wrappers for convenience imports
from .openai_compatible import OpenAICompatibleWrapper  # noqa: F401
from .deepseek_wrapper import DeepseekToolWrapper  # noqa: F401
from .ollama_wrapper import OllamaWrapper  # noqa: F401]]></content>
    </file>
    <file>
      <path>wrappers/base.py</path>
      <content><![CDATA["""
LLM Wrapper Base Interface.

Defines a provider-agnostic interface for tool-enabled LLM wrappers that:
- Register Tool instances and expose their NL schema
- Build a consistent system prompt instructing EXACT tool call format
- Execute a user prompt and return a formatted string with Reasoning, Tool Call, and Result

Design constraints:
- One class per file, <500 LOC
- Separation of concerns: Provider-specific logic in subclasses
- Maintain compatibility with existing DeepseekToolWrapper result formatting
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional

# Sibling package import using relative path
from ..tools.tool_base import Tool  # noqa: F401


class LLMWrapper(ABC):
    """
    Abstract base for LLM wrappers that orchestrate tool usage via prompt engineering.

    Responsibilities:
    - Tool registration (with NL schema exposition)
    - System prompt construction (consistent format across providers)
    - Provider-specific execution in concrete subclasses
    """

    def __init__(self) -> None:
        self.tools: Dict[str, Dict[str, Any]] = {}

    def _convert_schema_to_nl(self, schema: Dict[str, Any]) -> str:
        """
        Convert JSONSchema to a natural-language bullet list.

        Example line:
        - param (string*): Description here
        """
        nl_desc = []
        props = schema.get("properties", {}) or {}
        required = set(schema.get("required", []) or [])

        for name, details in props.items():
            desc = details.get("description", "No description available")
            type_info = details.get("type", "any")
            req_mark = "*" if name in required else ""
            nl_desc.append(f"- {name} ({type_info}{req_mark}): {desc}")

        return "\n".join(nl_desc)

    def register_tool(self, tool: Tool) -> None:
        """
        Register a tool with its description and NL schema.

        This maintains backward-compatible fields used by the legacy Deepseek wrapper.
        """
        self.tools[tool.name] = {
            "tool": tool,
            "description": tool.description,
            "schema": self._convert_schema_to_nl(tool.input_schema),
            "raw_schema": tool.input_schema,
        }

    def _create_system_prompt(self) -> str:
        """
        Build the canonical system prompt that:
        - Lists available tools with descriptions and input schemas (NL)
        - Instructs the model to first explain reasoning, then emit EXACT tool call JSON
        - Enforces strict JSON format under a TOOL_CALL: sentinel
        """
        tools_desc = []
        for name, info in self.tools.items():
            tools_desc.append(
                f"Tool: {name}\n"
                f"Description: {info['description']}\n"
                f"Input_schema:\n{info['schema']}\n"
            )
        tools_block = "\n".join(tools_desc)

        return (
            "You are an AI assistant with access to the following tools:\n"
            f"{tools_block}\n"
            "To use a tool, first explain your reasoning using Chain of Thought, then respond with a tool call in this EXACT format:\n"
            "TOOL_CALL:\n"
            "{\n"
            '    "tool": "tool_name",\n'
            '    "input_schema": {\n'
            '        "param1": "value1",\n'
            '        "param2": "value2"\n'
            "    }\n"
            "}\n\n"
            "Make sure to:\n"
            "1. Use valid JSON format\n"
            "2. Include all required input_schema\n"
            "3. Use correct parameter types\n"
            "4. Only use tools that are listed above"
        )

    @abstractmethod
    def execute(self, user_input: str) -> str:
        """
        Execute a tool based on user input via the provider-specific LLM.

        Return format MUST match existing DeepseekToolWrapper behavior exactly:
        - Reasoning:\n{reasoning}\n\nTool Call:\n{json}\n\nResult:\n{tool_result}
        """
        raise NotImplementedError("Subclasses must implement execute()")]]></content>
    </file>
    <file>
      <path>wrappers/deepseek_wrapper.py</path>
      <content><![CDATA[from __future__ import annotations

import os
from typing import Optional

from dotenv import load_dotenv

from .openai_compatible import OpenAICompatibleWrapper


class DeepseekToolWrapper(OpenAICompatibleWrapper):
    """
    DeepSeek-specialized wrapper that delegates to the OpenAI-compatible implementation.

    Defaults:
      - base_url: https://api.deepseek.com
      - model: deepseek-reasoner
      - api_key: DEEPSEEK_API_KEY (from environment)

    Interface preserved:
      - register_tool(tool)
      - execute(user_input: str) -> str
      - Tool listing and prompt construction via the shared base
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
    ) -> None:
        load_dotenv()
        super().__init__(
            api_key=api_key or os.getenv("DEEPSEEK_API_KEY"),
            base_url=base_url or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com"),
            model=model or os.getenv("DEEPSEEK_MODEL", "deepseek-reasoner"),
        )]]></content>
    </file>
    <file>
      <path>wrappers/ollama_wrapper.py</path>
      <content><![CDATA["""
Ollama LLM wrapper (first-class Harmony tool calling).

Defaults:
  base_url: http://localhost:11434/v1
  model:    OLLAMA_MODEL (default: llama3.1)

This wrapper uses Ollama's native /api/chat endpoint with 'tools' to enable
server-managed (Harmony) function calling. We still reuse the system prompt
from OpenAICompatibleWrapper to maintain consistent instructions, and we
fall back to sentinel JSON parsing if the server does not emit tool calls.
"""

from __future__ import annotations

import json
import os
from typing import Any, Dict, List, Optional

import requests
from dotenv import load_dotenv

from .openai_compatible import OpenAICompatibleWrapper


class OllamaWrapper(OpenAICompatibleWrapper):
    """
    Preconfigured wrapper for Ollama with first-class Harmony tool calling.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        strict: Optional[bool] = None,
    ) -> None:
        load_dotenv()

        # Strict tool-call prompting defaults OFF for Ollama (allows model discretion)
        env_strict = os.getenv("OLLAMA_STRICT_PROMPT", "0").lower() in ("1", "true", "yes", "on")
        self._strict: bool = strict if strict is not None else env_strict

        # Ollama typically ignores API keys; we pass a placeholder by default
        super().__init__(
            api_key=api_key or os.getenv("OLLAMA_API_KEY", "ollama"),
            base_url=base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/v1"),
            model=model or os.getenv("OLLAMA_MODEL", "llama3.1"),
        )

    # Use stricter system prompt by default to coerce pure JSON tool calls from Ollama models.
    # Falls back to the standard prompt if strict mode is disabled.
    def _create_system_prompt(self) -> str:  # noqa: D401  (override base)
        if getattr(self, "_strict", False) and hasattr(self, "_build_strict_tool_prompt"):
            return self._build_strict_tool_prompt()  # provided by OpenAICompatibleWrapper
        # Fallback to the canonical prompt from the base class
        return super()._create_system_prompt()

    def _root_host(self) -> str:
        """
        Convert base_url like http://localhost:11434/v1 -> http://localhost:11434 for /api/* calls.
        """
        host = (self.base_url or "").rstrip("/")
        return host[:-3] if host.endswith("/v1") else host

    def _make_ollama_tool_def(self, name: str, description: str, raw_schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        Convert our internal tool schema into Ollama Tool JSON.
        """
        properties = (raw_schema.get("properties") or {})
        required = (raw_schema.get("required") or [])

        return {
            "type": "function",
            "function": {
                "name": name,
                "description": description or "",
                "parameters": {
                    "type": "object",
                    "properties": properties,
                    "required": required,
                },
            },
        }

    def _build_ollama_tools(self) -> List[Dict[str, Any]]:
        """
        Build the 'tools' array for Ollama Harmony.

        Important: Include both '<name>' and 'tool.<name>' aliases to satisfy models
        that emit Harmony-styled names like 'tool.file'. This prevents reverse mapping
        failures like: "harmony parser: no reverse mapping found for function name".
        """
        defs: List[Dict[str, Any]] = []
        for name, info in self.tools.items():
            raw_schema = info.get("raw_schema") or {}
            desc = info.get("description") or ""
            # canonical name
            defs.append(self._make_ollama_tool_def(name, desc, raw_schema))
            # harmony alias
            defs.append(self._make_ollama_tool_def(f"tool.{name}", desc, raw_schema))
        return defs

    @staticmethod
    def _normalize_tool_name(fn_name: str) -> str:
        """
        Normalize Harmony function names to our local registry:
        - 'tool.file' -> 'file'
        - 'file'      -> 'file'
        """
        if not fn_name:
            return fn_name
        return fn_name.split(".")[-1] if "." in fn_name else fn_name

    def _strip_code_fences(self, s: str) -> str:
        """
        Remove a single leading or first code-fence block ```...``` if present.
        Returns the inner content trimmed, else the original string trimmed.
        """
        if not isinstance(s, str):
            return s  # type: ignore[return-value]
        text = s.strip()
        if text.startswith("```"):
            end = text.find("```", 3)
            if end != -1:
                return text[3:end].strip()
        idx = text.find("```")
        if idx != -1:
            end = text.find("```", idx + 3)
            if end != -1:
                return text[idx + 3:end].strip()
        return text

    def _parse_args_safely(self, raw: Any) -> Dict[str, Any]:
        """
        Best-effort conversion of tool 'arguments' into a dict.
        Handles:
        - dict pass-through
        - JSON string (with or without ``` fences)
        - Strings containing extra trailing text by extracting the first balanced {...} object
        """
        if isinstance(raw, dict):
            return raw
        if isinstance(raw, str):
            text = self._strip_code_fences(raw)
            try:
                obj = json.loads(text)
                return obj if isinstance(obj, dict) else {}
            except Exception:
                # attempt to extract first balanced JSON object
                start = text.find("{")
                while start != -1:
                    depth = 0
                    for i in range(start, len(text)):
                        ch = text[i]
                        if ch == "{":
                            depth += 1
                        elif ch == "}":
                            depth -= 1
                            if depth == 0:
                                candidate = text[start : i + 1]
                                try:
                                    obj = json.loads(candidate)
                                    if isinstance(obj, dict):
                                        return obj
                                except Exception:
                                    pass
                                break
                    start = text.find("{", start + 1)
        return {}

    def _parse_json_relaxed(self, text: str) -> Dict[str, Any]:
        """
        Parse Ollama /api/chat response bodies that may contain NDJSON or extra text.
        Strategy:
        - Try strict json.loads
        - Fallback to NDJSON: decode each non-empty line and take the last object
        - Fallback to balanced-brace scan to extract the first JSON object
        """
        # Strict parse first
        try:
            obj = json.loads(text)
            if isinstance(obj, dict):
                return obj
        except Exception:
            pass

        # NDJSON fallback
        last_obj: Dict[str, Any] | None = None
        for line in text.splitlines():
            line = line.strip()
            if not line:
                continue
            try:
                o = json.loads(line)
                if isinstance(o, dict):
                    last_obj = o
            except Exception:
                continue
        if last_obj is not None:
            return last_obj

        # Balanced-brace scan
        start = text.find("{")
        while start != -1:
            depth = 0
            for i in range(start, len(text)):
                ch = text[i]
                if ch == "{":
                    depth += 1
                elif ch == "}":
                    depth -= 1
                    if depth == 0:
                        candidate = text[start : i + 1]
                        try:
                            o = json.loads(candidate)
                            if isinstance(o, dict):
                                return o
                        except Exception:
                            pass
                        break
            start = text.find("{", start + 1)
        return {}

    def execute(self, user_input: str) -> str:
        """
        Execute via Ollama /api/chat with server-managed tool calling.
        Falls back to sentinel JSON parsing when no tool_calls are returned.
        """
        try:
            system_prompt = self._create_system_prompt()

            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input},
                ],
                "tools": self._build_ollama_tools(),
            }

            url = f"{self._root_host()}/api/chat"
            headers = {"Content-Type": "application/json", "Accept": "application/json"}

            resp = requests.post(url, headers=headers, data=json.dumps(payload))
            resp.raise_for_status()
            text = resp.text
            data = self._parse_json_relaxed(text)

            message: Dict[str, Any] = (data.get("message") or {})
            content: str = (message.get("content") or "").strip()
            reasoning: str = content  # use assistant content as the reasoning section

            tool_calls = message.get("tool_calls") or []
            if tool_calls:
                chosen_name: str = ""
                args: Dict[str, Any] = {}
                tool_obj = None

                # Prefer the first tool call that maps to a registered tool
                for call in tool_calls:
                    fn = (call or {}).get("function") or {}
                    raw_name = str(fn.get("name") or "")
                    fn_name = self._normalize_tool_name(raw_name)

                    raw_args = fn.get("arguments")
                    args = self._parse_args_safely(raw_args)

                    if fn_name and fn_name in self.tools:
                        chosen_name = fn_name
                        tool_obj = self.tools[fn_name]["tool"]
                        break

                if tool_obj is None:
                    # No matching tool provided by server; try sentinel parsing fallback
                    tool_call = self._extract_tool_call(content) if hasattr(self, "_extract_tool_call") else None
                    if not tool_call:
                        return f"Reasoning:\n{reasoning}\n\nError: Tool '{(chosen_name or '(none)')}' not found."
                    chosen_name = tool_call.get("tool") or ""
                    args = tool_call.get("input_schema", {}) or {}
                    tool_obj = self.tools.get(chosen_name, {}).get("tool")
                    if tool_obj is None:
                        return f"Reasoning:\n{reasoning}\n\nError: Tool '{chosen_name}' not found."

                result = tool_obj.run(args)  # type: ignore[attr-defined]
                tool_call_json = {"tool": chosen_name, "input_schema": args}
                return (
                    f"Reasoning:\n{reasoning}\n\n"
                    f"Tool Call:\n{json.dumps(tool_call_json, indent=2)}\n\n"
                    f"Result:\n{result}"
                )

            # No server-side tool call: attempt sentinel parsing from content
            tool_call = self._extract_tool_call(content) if hasattr(self, "_extract_tool_call") else None
            if tool_call:
                tool_name = tool_call.get("tool")
                params = tool_call.get("input_schema", {}) or {}
                if not tool_name or tool_name not in self.tools:
                    return f"Reasoning:\n{reasoning}\n\nError: Tool '{tool_name}' not found."

                tool = self.tools[tool_name]["tool"]
                result = tool.run(params)
                return (
                    f"Reasoning:\n{reasoning}\n\n"
                    f"Tool Call:\n{json.dumps(tool_call, indent=2)}\n\n"
                    f"Result:\n{result}"
                )
            else:
                # No tool call found, return content as normal conversational response
                if content:
                    return f"Reasoning:\n{reasoning}\n\nResponse:\n{content}"
                else:
                    return f"Reasoning:\n{reasoning}\n\nNo valid tool call was made."

            tool_name = tool_call.get("tool")
            params = tool_call.get("input_schema", {}) or {}
            if not tool_name or tool_name not in self.tools:
                return f"Reasoning:\n{reasoning}\n\nError: Tool '{tool_name}' not found."

            tool = self.tools[tool_name]["tool"]
            result = tool.run(params)
            return (
                f"Reasoning:\n{reasoning}\n\n"
                f"Tool Call:\n{json.dumps(tool_call, indent=2)}\n\n"
                f"Result:\n{result}"
            )

        except Exception as e:
            return f"Error executing tool: {str(e)}"]]></content>
    </file>
    <file>
      <path>wrappers/openai_compatible.py</path>
      <content><![CDATA["""
OpenAI-compatible LLM wrapper.

Supports any provider exposing an OpenAI Chat Completions-compatible API:
- DeepSeek (https://api.deepseek.com)
- Ollama (http://localhost:11434/v1)
- Local/self-hosted OpenAI-compatible servers

Behavior:
- Uses the canonical tool-use system prompt from [LLMWrapper](agent_tools/src/wrappers/base.py:24) to elicit a strict TOOL_CALL JSON
- Parses TOOL_CALL and executes the mapped Tool.run(params)
- Returns a legacy-compatible string: 'Reasoning:\\n...\\n\\nTool Call:\\n{json}\\n\\nResult:\\n{tool_result}'

File length constraint: <500 LOC
"""

from __future__ import annotations

import json
import os
from typing import Any, Dict, Optional

from dotenv import load_dotenv
from openai import OpenAI

from .base import LLMWrapper
from ..tools.tool_base import Tool  # for type hints

# Safe import for OpenAI Agents SDK integration
try:
    from ..integrations.openai_agents.bridge import run_openai_agents, SDK_AVAILABLE
except ImportError:
    run_openai_agents = None  # type: ignore
    SDK_AVAILABLE = False


class OpenAICompatibleWrapper(LLMWrapper):
    """
    Provider-agnostic wrapper for OpenAI-compatible chat API.

    Example usage for DeepSeek:
        wrapper = OpenAICompatibleWrapper(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com",
            model="deepseek-reasoner",
        )

    Example usage for Ollama:
        wrapper = OpenAICompatibleWrapper(
            api_key=os.getenv("OLLAMA_API_KEY", "ollama"),  # placeholder if unused
            base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/v1"),
            model=os.getenv("OLLAMA_MODEL", "llama3.1"),
        )
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
    ) -> None:
        super().__init__()
        load_dotenv()

        # Defaults allow easy ollama usage without extra config
        self.api_key = api_key or os.getenv("OPENAI_API_KEY") or os.getenv("DEEPSEEK_API_KEY") or os.getenv("OLLAMA_API_KEY") or "ollama"
        self.base_url = base_url or os.getenv("OPENAI_BASE_URL") or os.getenv("DEEPSEEK_BASE_URL") or os.getenv("OLLAMA_BASE_URL") or "http://localhost:11434/v1"
        self.model = model or os.getenv("OPENAI_MODEL") or os.getenv("DEEPSEEK_MODEL") or os.getenv("OLLAMA_MODEL") or "llama3.1"

        # Initialize OpenAI client
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    def _extract_tool_call(self, content: str) -> Optional[Dict[str, Any]]:
        """
        Robustly extract a tool call JSON object from model output.

        Accepted forms (case-insensitive):
        - TOOL_CALL: { ... }                 # preferred sentinel
        - Tool Call: ```json { ... } ```     # fenced JSON
        - Any first JSON object in content that has keys: "tool" and "input_schema"
        """
        txt = content or ""
        try:
            # 1) Preferred sentinel, case-insensitive
            lower = txt.lower()
            for sentinel in ("tool_call:", "tool call:", "toolcall:"):
                if sentinel in lower:
                    idx = lower.index(sentinel) + len(sentinel)
                    tail = txt[idx:].strip()
                    # if fenced, strip first fence
                    if tail.startswith("```"):
                        fence_end = tail.find("```", 3)
                        if fence_end != -1:
                            tail = tail[3:fence_end]
                    # attempt direct JSON parse
                    try:
                        obj = json.loads(tail)
                        if isinstance(obj, dict) and "tool" in obj and "input_schema" in obj:
                            return obj
                    except Exception:
                        pass
            # 2) Code-fenced JSON blocks ```json ... ```
            import re
            fence_re = re.compile(r"```(?:json)?\s*([\s\S]*?)```", re.IGNORECASE)
            for m in fence_re.finditer(txt):
                block = m.group(1).strip()
                try:
                    obj = json.loads(block)
                    if isinstance(obj, dict) and "tool" in obj and "input_schema" in obj:
                        return obj
                except Exception:
                    continue
            # 3) First JSON object heuristic (balanced braces) that looks like a tool call
            #    Scan for a '{' then attempt to parse incrementally until balanced.
            start = txt.find("{")
            while start != -1:
                depth = 0
                for end in range(start, len(txt)):
                    ch = txt[end]
                    if ch == "{":
                        depth += 1
                    elif ch == "}":
                        depth -= 1
                        if depth == 0:
                            candidate = txt[start : end + 1]
                            try:
                                obj = json.loads(candidate)
                                if isinstance(obj, dict) and "tool" in obj and "input_schema" in obj:
                                    return obj
                            except Exception:
                                pass
                            break
                start = txt.find("{", start + 1)
        except Exception:
            pass
        return None

    def _build_strict_tool_prompt(self) -> str:
        """
        Build a strict system instruction forcing a pure JSON tool call.
        This improves reliability on weaker OpenAI-compatible models (e.g., some Ollama models).
        """
        # Expose available tools with their JSONSchema exactly
        chunks = [
            "You MUST output ONLY a JSON object for a tool call. No prose, no backticks, no extra text.",
            "The JSON MUST have exactly these keys: tool (string), input_schema (object).",
            "Available tools and their JSON schemas follow. Choose exactly one and fill input_schema accordingly."
        ]
        for name, info in self.tools.items():
            schema = info.get("raw_schema") or {}
            chunks.append(f"TOOL {name} SCHEMA:")
            try:
                chunks.append(json.dumps(schema, ensure_ascii=False))
            except Exception:
                chunks.append(str(schema))
        chunks.append('Example output shape (choose valid tool): {"tool":"file","input_schema":{"operation":"read","path":"..."} }')
        return "\n".join(chunks)

    def _infer_reasoning(self, content: str, reasoning_content: Optional[str]) -> str:
        """
        Prefer provider-supplied 'reasoning_content' (DeepSeek), else infer
        from content before TOOL_CALL or fall back to entire content.
        """
        if reasoning_content:
            return reasoning_content

        marker = "TOOL_CALL:"
        if marker in content:
            return content.split(marker, 1)[0].strip()

        return content.strip()

    def execute(self, user_input: str) -> str:
        """
        Execute the LLM call and return legacy-compatible formatted output.

        Returns:
            str: "Reasoning:\\n{reasoning}\\n\\nTool Call:\\n{json}\\n\\nResult:\\n{result or error}"
        """
        # Check for OpenAI Agents SDK integration flag
        flag_enabled = os.getenv("AGENT_TOOLS_OPENAI_AGENTS") in ("1", "true", "yes", "on")

        if flag_enabled and SDK_AVAILABLE and run_openai_agents:
            try:
                # Use SDK integration
                system_prompt = self._create_system_prompt()
                result = run_openai_agents(
                    model=self.model,
                    system_prompt=system_prompt,
                    user_input=user_input,
                    tools_registry=self.tools,
                    api_key=self.api_key,
                    base_url=self.base_url
                )

                # Format output to match legacy format
                if result.get("tool_call"):
                    # Tool call path
                    tool_call_json = json.dumps({
                        "tool": result["tool_call"]["tool"],
                        "input_schema": result["tool_call"]["input_schema"]
                    }, indent=2)
                    return (
                        f"Reasoning:\n{result.get('reasoning', '')}\n\n"
                        f"Tool Call:\n{tool_call_json}\n\n"
                        f"Result:\n{result.get('tool_result', '')}"
                    )
                else:
                    # No tool call path
                    response = result.get("response", "")
                    if response:
                        return f"Reasoning:\n{result.get('reasoning', '')}\n\nResponse:\n{response}"
                    else:
                        return f"Reasoning:\n{result.get('reasoning', '')}\n\nNo valid tool call was made."

            except Exception as e:
                # Fallback to legacy behavior on SDK errors
                pass

        # Legacy behavior (fallback)
        try:
            system_prompt = self._create_system_prompt()

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input},
                ],
            )

            # OpenAI-compatible response structure
            message = response.choices[0].message
            content = message.content or ""
            # DeepSeek-specific extension (present only on deepseek-reasoner)
            reasoning_content = getattr(message, "reasoning_content", None)
            reasoning = self._infer_reasoning(content, reasoning_content)

            tool_call = self._extract_tool_call(content)
            if not tool_call:
                return f"Reasoning:\n{reasoning}\n\nNo valid tool call was made."

            tool_name = tool_call.get("tool")
            params = tool_call.get("input_schema", {}) or {}

            if not tool_name or tool_name not in self.tools:
                return f"Reasoning:\n{reasoning}\n\nError: Tool '{tool_name}' not found."

            tool: Tool = self.tools[tool_name]["tool"]  # type: ignore[assignment]
            result = tool.run(params)

            return (
                f"Reasoning:\n{reasoning}\n\n"
                f"Tool Call:\n{json.dumps(tool_call, indent=2)}\n\n"
                f"Result:\n{result}"
            )

        except Exception as e:
            return f"Error executing tool: {str(e)}"]]></content>
    </file>
  </files>
</agents>
